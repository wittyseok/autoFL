[
    {
        "name": "keras.layers.noise.GaussianNoise.__init__#37",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianNoise",
        "signature": "keras.layers.noise.GaussianNoise.__init__(self, stddev, **kwargs)",
        "snippet": "    def __init__(self, stddev, **kwargs):\n        super(GaussianNoise, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.stddev = stddev",
        "begin_line": 37,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianNoise.call#42",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianNoise",
        "signature": "keras.layers.noise.GaussianNoise.call(self, inputs, training=None)",
        "snippet": "    def call(self, inputs, training=None):\n        def noised():\n            return inputs + K.random_normal(shape=K.shape(inputs),\n                                            mean=0.,\n                                            stddev=self.stddev)\n        return K.in_train_phase(noised, inputs, training=training)",
        "begin_line": 42,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianNoise.noised#43",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianNoise",
        "signature": "keras.layers.noise.GaussianNoise.noised()",
        "snippet": "        def noised():\n            return inputs + K.random_normal(shape=K.shape(inputs),\n                                            mean=0.,\n                                            stddev=self.stddev)",
        "begin_line": 43,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianNoise.get_config#49",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianNoise",
        "signature": "keras.layers.noise.GaussianNoise.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'stddev': self.stddev}\n        base_config = super(GaussianNoise, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 49,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianNoise.compute_output_shape#54",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianNoise",
        "signature": "keras.layers.noise.GaussianNoise.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianDropout.__init__#81",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianDropout",
        "signature": "keras.layers.noise.GaussianDropout.__init__(self, rate, **kwargs)",
        "snippet": "    def __init__(self, rate, **kwargs):\n        super(GaussianDropout, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.rate = rate",
        "begin_line": 81,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianDropout.call#86",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianDropout",
        "signature": "keras.layers.noise.GaussianDropout.call(self, inputs, training=None)",
        "snippet": "    def call(self, inputs, training=None):\n        if 0 < self.rate < 1:\n            def noised():\n                stddev = np.sqrt(self.rate / (1.0 - self.rate))\n                return inputs * K.random_normal(shape=K.shape(inputs),\n                                                mean=1.0,\n                                                stddev=stddev)\n            return K.in_train_phase(noised, inputs, training=training)\n        return inputs",
        "begin_line": 86,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianDropout.noised#88",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianDropout",
        "signature": "keras.layers.noise.GaussianDropout.noised()",
        "snippet": "            def noised():\n                stddev = np.sqrt(self.rate / (1.0 - self.rate))\n                return inputs * K.random_normal(shape=K.shape(inputs),\n                                                mean=1.0,\n                                                stddev=stddev)",
        "begin_line": 88,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianDropout.get_config#96",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianDropout",
        "signature": "keras.layers.noise.GaussianDropout.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'rate': self.rate}\n        base_config = super(GaussianDropout, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 96,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.GaussianDropout.compute_output_shape#101",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.GaussianDropout",
        "signature": "keras.layers.noise.GaussianDropout.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 101,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.AlphaDropout.__init__#131",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.AlphaDropout",
        "signature": "keras.layers.noise.AlphaDropout.__init__(self, rate, noise_shape=None, seed=None, **kwargs)",
        "snippet": "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n        super(AlphaDropout, self).__init__(**kwargs)\n        self.rate = rate\n        self.noise_shape = noise_shape\n        self.seed = seed\n        self.supports_masking = True",
        "begin_line": 131,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.AlphaDropout._get_noise_shape#138",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.AlphaDropout",
        "signature": "keras.layers.noise.AlphaDropout._get_noise_shape(self, inputs)",
        "snippet": "    def _get_noise_shape(self, inputs):\n        return self.noise_shape if self.noise_shape else K.shape(inputs)",
        "begin_line": 138,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.AlphaDropout.call#141",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.AlphaDropout",
        "signature": "keras.layers.noise.AlphaDropout.call(self, inputs, training=None)",
        "snippet": "    def call(self, inputs, training=None):\n        if 0. < self.rate < 1.:\n            noise_shape = self._get_noise_shape(inputs)\n\n            def dropped_inputs(inputs=inputs, rate=self.rate, seed=self.seed):\n                alpha = 1.6732632423543772848170429916717\n                scale = 1.0507009873554804934193349852946\n                alpha_p = -alpha * scale\n\n                kept_idx = K.greater_equal(K.random_uniform(noise_shape,\n                                                            seed=seed), rate)\n                kept_idx = K.cast(kept_idx, K.floatx())\n\n                # Get affine transformation params\n                a = ((1 - rate) * (1 + rate * alpha_p ** 2)) ** -0.5\n                b = -a * alpha_p * rate\n\n                # Apply mask\n                x = inputs * kept_idx + alpha_p * (1 - kept_idx)\n\n                # Do affine transformation\n                return a * x + b\n\n            return K.in_train_phase(dropped_inputs, inputs, training=training)\n        return inputs",
        "begin_line": 141,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.AlphaDropout.dropped_inputs#145",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.AlphaDropout",
        "signature": "keras.layers.noise.AlphaDropout.dropped_inputs(inputs=inputs, rate=self.rate, seed=self.seed)",
        "snippet": "            def dropped_inputs(inputs=inputs, rate=self.rate, seed=self.seed):\n                alpha = 1.6732632423543772848170429916717\n                scale = 1.0507009873554804934193349852946\n                alpha_p = -alpha * scale\n\n                kept_idx = K.greater_equal(K.random_uniform(noise_shape,\n                                                            seed=seed), rate)\n                kept_idx = K.cast(kept_idx, K.floatx())\n\n                # Get affine transformation params\n                a = ((1 - rate) * (1 + rate * alpha_p ** 2)) ** -0.5\n                b = -a * alpha_p * rate\n\n                # Apply mask\n                x = inputs * kept_idx + alpha_p * (1 - kept_idx)\n\n                # Do affine transformation\n                return a * x + b",
        "begin_line": 145,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.AlphaDropout.get_config#167",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.AlphaDropout",
        "signature": "keras.layers.noise.AlphaDropout.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'rate': self.rate}\n        base_config = super(AlphaDropout, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 167,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.noise.AlphaDropout.compute_output_shape#172",
        "src_path": "keras/layers/noise.py",
        "class_name": "keras.layers.noise.AlphaDropout",
        "signature": "keras.layers.noise.AlphaDropout.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 172,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.models._clone_functional_model#26",
        "src_path": "keras/models.py",
        "class_name": "keras.models",
        "signature": "keras.models._clone_functional_model(model, input_tensors=None)",
        "snippet": "def _clone_functional_model(model, input_tensors=None):\n    \"\"\"Clone a functional `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Model):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Model` instance, got ', model)\n    if isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a functional `Model` instance, '\n                         'got a `Sequential` instance instead:', model)\n\n    layer_map = {}  # Cache for created layers.\n    tensor_map = {}  # Map {reference_tensor: (corresponding_tensor, mask)}\n    if input_tensors is None:\n        # Create placeholders to build the model on top of.\n        input_layers = []\n        input_tensors = []\n        for layer in model._input_layers:\n            input_tensor = Input(batch_shape=layer.batch_input_shape,\n                                 dtype=layer.dtype,\n                                 sparse=layer.sparse,\n                                 name=layer.name)\n            input_tensors.append(input_tensor)\n            # Cache newly created input layer.\n            newly_created_input_layer = input_tensor._keras_history[0]\n            layer_map[layer] = newly_created_input_layer\n        for original_input_layer, cloned_input_layer in zip(model._input_layers, input_layers):\n            layer_map[original_input_layer] = cloned_input_layer\n    else:\n        # Make sure that all input tensors come from a Keras layer.\n        # If tensor comes from an input layer: cache the input layer.\n        input_tensors = to_list(input_tensors)\n        _input_tensors = []\n        for i, x in enumerate(input_tensors):\n            if not K.is_keras_tensor(x):\n                name = model._input_layers[i].name\n                input_tensor = Input(tensor=x,\n                                     name='input_wrapper_for_' + name)\n                _input_tensors.append(input_tensor)\n                # Cache newly created input layer.\n                original_input_layer = x._keras_history[0]\n                newly_created_input_layer = input_tensor._keras_history[0]\n                layer_map[original_input_layer] = newly_created_input_layer\n            else:\n                _input_tensors.append(x)\n        input_tensors = _input_tensors\n\n    for x, y in zip(model.inputs, input_tensors):\n        tensor_map[x] = (y, None)  # tensor, mask\n\n    # Iterated over every node in the reference model, in depth order.\n    depth_keys = list(model._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    for depth in depth_keys:\n        nodes = model._nodes_by_depth[depth]\n        for node in nodes:\n            # Recover the corresponding layer.\n            layer = node.outbound_layer\n\n            # Get or create layer.\n            if layer not in layer_map:\n                # Clone layer.\n                new_layer = layer.__class__.from_config(layer.get_config())\n                layer_map[layer] = new_layer\n                layer = new_layer\n            else:\n                # Reuse previously cloned layer.\n                layer = layer_map[layer]\n                # Don't call InputLayer multiple times.\n                if isinstance(layer, InputLayer):\n                    continue\n\n            # Gather inputs to call the new layer.\n            reference_input_tensors = node.input_tensors\n            reference_output_tensors = node.output_tensors\n\n            # If all previous input tensors are available in tensor_map,\n            # then call node.inbound_layer on them.\n            computed_data = []  # List of tuples (input, mask).\n            for x in reference_input_tensors:\n                if x in tensor_map:\n                    computed_data.append(tensor_map[x])\n\n            if len(computed_data) == len(reference_input_tensors):\n                # Call layer.\n                if node.arguments:\n                    kwargs = node.arguments\n                else:\n                    kwargs = {}\n                if len(computed_data) == 1:\n                    computed_tensor, computed_mask = computed_data[0]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_mask\n                    output_tensors = to_list(\n                        layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                    computed_tensors = [computed_tensor]\n                    computed_masks = [computed_mask]\n                else:\n                    computed_tensors = [x[0] for x in computed_data]\n                    computed_masks = [x[1] for x in computed_data]\n                    if has_arg(layer.call, 'mask'):\n                        if 'mask' not in kwargs:\n                            kwargs['mask'] = computed_masks\n                    output_tensors = to_list(\n                        layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                # Update tensor_map.\n                for x, y, mask in zip(reference_output_tensors,\n                                      output_tensors,\n                                      output_masks):\n                    tensor_map[x] = (y, mask)\n\n    # Check that we did compute the model outputs,\n    # then instantiate a new model from inputs and outputs.\n    output_tensors = []\n    for x in model.outputs:\n        assert x in tensor_map, 'Could not compute output ' + str(x)\n        tensor, _ = tensor_map[x]\n        output_tensors.append(tensor)\n    return Model(input_tensors, output_tensors, name=model.name)",
        "begin_line": 26,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.models._clone_sequential_model#172",
        "src_path": "keras/models.py",
        "class_name": "keras.models",
        "signature": "keras.models._clone_sequential_model(model, input_tensors=None)",
        "snippet": "def _clone_sequential_model(model, input_tensors=None):\n    \"\"\"Clone a `Sequential` model instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Sequential`.\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Sequential` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if not isinstance(model, Sequential):\n        raise ValueError('Expected `model` argument '\n                         'to be a `Sequential` model instance, '\n                         'but got:', model)\n\n    def clone(layer):\n        return layer.__class__.from_config(layer.get_config())\n\n    layers = [clone(layer) for layer in model.layers]\n    if input_tensors is None:\n        return Sequential(layers=layers, name=model.name)\n    else:\n        if len(to_list(input_tensors)) != 1:\n            raise ValueError('To clone a `Sequential` model, we expect '\n                             ' at most one tensor '\n                             'as part of `input_tensors`.')\n        x = to_list(input_tensors)[0]\n        if K.is_keras_tensor(x):\n            origin_layer = x._keras_history[0]\n            if isinstance(origin_layer, InputLayer):\n                return Sequential(layers=[origin_layer] + layers,\n                                  name=model.name)\n            else:\n                raise ValueError('Cannot clone a `Sequential` model on top '\n                                 'of a tensor that comes from a Keras layer '\n                                 'other than an `InputLayer`. '\n                                 'Use the functional API instead.')\n        input_tensor = Input(tensor=x,\n                             name='input_wrapper_for_' + str(x.name))\n        input_layer = input_tensor._keras_history[0]\n        return Sequential(layers=[input_layer] + layers, name=model.name)",
        "begin_line": 172,
        "end_line": 223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.models.clone#198",
        "src_path": "keras/models.py",
        "class_name": "keras.models",
        "signature": "keras.models.clone(layer)",
        "snippet": "    def clone(layer):\n        return layer.__class__.from_config(layer.get_config())",
        "begin_line": 198,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.models.clone_model#226",
        "src_path": "keras/models.py",
        "class_name": "keras.models",
        "signature": "keras.models.clone_model(model, input_tensors=None)",
        "snippet": "def clone_model(model, input_tensors=None):\n    \"\"\"Clone any `Model` instance.\n\n    Model cloning is similar to calling a model on new inputs,\n    except that it creates new layers (and thus new weights) instead\n    of sharing the weights of the existing layers.\n\n    # Arguments\n        model: Instance of `Model`\n            (could be a functional model or a Sequential model).\n        input_tensors: optional list of input tensors\n            to build the model upon. If not provided,\n            placeholders will be created.\n\n    # Returns\n        An instance of `Model` reproducing the behavior\n        of the original model, on top of new inputs tensors,\n        using newly instantiated weights.\n\n    # Raises\n        ValueError: in case of invalid `model` argument value.\n    \"\"\"\n    if isinstance(model, Sequential):\n        return _clone_sequential_model(model, input_tensors=input_tensors)\n    else:\n        return _clone_functional_model(model, input_tensors=input_tensors)",
        "begin_line": 226,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.__init__#53",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.__init__(self, datapath, dataset, start=0, end=None, normalizer=None)",
        "snippet": "    def __init__(self, datapath, dataset, start=0, end=None, normalizer=None):\n        if h5py is None:\n            raise ImportError('The use of HDF5Matrix requires '\n                              'HDF5 and h5py installed.')\n\n        if datapath not in list(self.refs.keys()):\n            f = h5py.File(datapath)\n            self.refs[datapath] = f\n        else:\n            f = self.refs[datapath]\n        self.data = f[dataset]\n        self.start = start\n        if end is None:\n            self.end = self.data.shape[0]\n        else:\n            self.end = end\n        self.normalizer = normalizer\n        if self.normalizer is not None:\n            first_val = self.normalizer(self.data[0:1])\n        else:\n            first_val = self.data[0:1]\n        self._base_shape = first_val.shape[1:]\n        self._base_dtype = first_val.dtype",
        "begin_line": 53,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.__len__#77",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.__len__(self)",
        "snippet": "    def __len__(self):\n        return self.end - self.start",
        "begin_line": 77,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.__getitem__#80",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if isinstance(key, slice):\n            start, stop = key.start, key.stop\n            if start is None:\n                start = 0\n            if stop is None:\n                stop = self.shape[0]\n            if stop + self.start <= self.end:\n                idx = slice(start + self.start, stop + self.start)\n            else:\n                raise IndexError\n        elif isinstance(key, (int, np.integer)):\n            if key + self.start < self.end:\n                idx = key + self.start\n            else:\n                raise IndexError\n        elif isinstance(key, np.ndarray):\n            if np.max(key) + self.start < self.end:\n                idx = (self.start + key).tolist()\n            else:\n                raise IndexError\n        else:\n            # Assume list/iterable\n            if max(key) + self.start < self.end:\n                idx = [x + self.start for x in key]\n            else:\n                raise IndexError\n        if self.normalizer is not None:\n            return self.normalizer(self.data[idx])\n        else:\n            return self.data[idx]",
        "begin_line": 80,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.shape#113",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.shape(self)",
        "snippet": "    def shape(self):\n        \"\"\"Gets a numpy-style shape tuple giving the dataset dimensions.\n\n        # Returns\n            A numpy-style shape tuple.\n        \"\"\"\n        return (self.end - self.start,) + self._base_shape",
        "begin_line": 113,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.dtype#122",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.dtype(self)",
        "snippet": "    def dtype(self):\n        \"\"\"Gets the datatype of the dataset.\n\n        # Returns\n            A numpy dtype string.\n        \"\"\"\n        return self._base_dtype",
        "begin_line": 122,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.ndim#131",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.ndim(self)",
        "snippet": "    def ndim(self):\n        \"\"\"Gets the number of dimensions (rank) of the dataset.\n\n        # Returns\n            An integer denoting the number of dimensions (rank) of the dataset.\n        \"\"\"\n        return self.data.ndim",
        "begin_line": 131,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.size#140",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.size(self)",
        "snippet": "    def size(self):\n        \"\"\"Gets the total dataset size (number of elements).\n\n        # Returns\n            An integer denoting the number of elements in the dataset.\n        \"\"\"\n        return np.prod(self.shape)",
        "begin_line": 140,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.ask_to_proceed_with_overwrite#149",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils",
        "signature": "keras.utils.io_utils.ask_to_proceed_with_overwrite(filepath)",
        "snippet": "def ask_to_proceed_with_overwrite(filepath):\n    \"\"\"Produces a prompt asking about overwriting a file.\n\n    # Arguments\n        filepath: the path to the file to be overwritten.\n\n    # Returns\n        True if we can proceed with overwrite, False otherwise.\n    \"\"\"\n    overwrite = six.moves.input('[WARNING] %s already exists - overwrite? '\n                                '[y/n]' % (filepath)).strip().lower()\n    while overwrite not in ('y', 'n'):\n        overwrite = six.moves.input('Enter \"y\" (overwrite) or \"n\" '\n                                    '(cancel).').strip().lower()\n    if overwrite == 'n':\n        return False\n    print('[TIP] Next time specify overwrite=True!')\n    return True",
        "begin_line": 149,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.H5Dict.__init__#181",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.H5Dict",
        "signature": "keras.utils.io_utils.H5Dict.__init__(self, path, mode='a')",
        "snippet": "    def __init__(self, path, mode='a'):\n        if isinstance(path, h5py.Group):\n            self.data = path\n            self._is_file = False\n        elif isinstance(path, str):\n            self.data = h5py.File(path,)\n            self._is_file = True\n        elif isinstance(path, dict):\n            self.data = path\n            self._is_file = False\n            # Flag to check if a dict is user defined data or a sub group:\n            self.data['_is_group'] = True\n        else:\n            raise TypeError('Required Group, str or dict. '\n                            'Received: {}.'.format(type(path)))\n        self.read_only = mode == 'r'",
        "begin_line": 181,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.H5Dict.__setitem__#198",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.H5Dict",
        "signature": "keras.utils.io_utils.H5Dict.__setitem__(self, attr, val)",
        "snippet": "    def __setitem__(self, attr, val):\n        if self.read_only:\n            raise ValueError('Cannot set item in read only mode.')\n        is_np = type(val).__module__ == np.__name__\n        if isinstance(self.data, dict):\n            if isinstance(attr, bytes):\n                attr = attr.decode('utf-8')\n            if is_np:\n                self.data[attr] = pickle.dumps(val)\n                # We have to remember to unpickle in __getitem__\n                self.data['_{}_pickled'.format(attr)] = True\n            else:\n                self.data[attr] = val\n            return\n        if attr in self:\n            raise KeyError('Cannot set attribute. '\n                           'Group with name \"{}\" exists.'.format(attr))\n        if is_np:\n            dataset = self.data.create_dataset(attr, val.shape, dtype=val.dtype)\n            if not val.shape:\n                # scalar\n                dataset[()] = val\n            else:\n                dataset[:] = val\n        if isinstance(val, list):\n            # Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\n            # because in that case even chunking the array would not make the saving\n            # possible.\n            bad_attributes = [x for x in val if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n\n            # Expecting this to never be true.\n            if len(bad_attributes) > 0:\n                raise RuntimeError('The following attributes cannot be saved to '\n                                   'HDF5 file because they are larger than '\n                                   '%d bytes: %s' % (HDF5_OBJECT_HEADER_LIMIT,\n                                                     ', '.join(bad_attributes)))\n\n            if val and sys.version_info[0] == 3 and isinstance(val[0], str):\n                # convert to bytes\n                val = [x.encode('utf-8') for x in val]\n\n            data_npy = np.asarray(val)\n\n            num_chunks = 1\n            chunked_data = np.array_split(data_npy, num_chunks)\n\n            # This will never loop forever thanks to the test above.\n            is_too_big = lambda x: x.nbytes > HDF5_OBJECT_HEADER_LIMIT\n            while any(map(is_too_big, chunked_data)):\n                num_chunks += 1\n                chunked_data = np.array_split(data_npy, num_chunks)\n\n            if num_chunks > 1:\n                for chunk_id, chunk_data in enumerate(chunked_data):\n                    self.data.attrs['%s%d' % (attr, chunk_id)] = chunk_data\n            else:\n                self.data.attrs[attr] = val\n        else:\n            self.data.attrs[attr] = val",
        "begin_line": 198,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.H5Dict.__getitem__#258",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.H5Dict",
        "signature": "keras.utils.io_utils.H5Dict.__getitem__(self, attr)",
        "snippet": "    def __getitem__(self, attr):\n        if isinstance(self.data, dict):\n            if isinstance(attr, bytes):\n                attr = attr.decode('utf-8')\n            if attr in self.data:\n                val = self.data[attr]\n                if isinstance(val, dict) and val.get('_is_group'):\n                    val = H5Dict(val)\n                elif '_{}_pickled'.format(attr) in self.data:\n                    val = pickle.loads(val)\n                return val\n            else:\n                if self.read_only:\n                    raise ValueError('Cannot create group in read only mode.')\n                val = {'_is_group': True}\n                self.data[attr] = val\n                return H5Dict(val)\n        if attr in self.data.attrs:\n            val = self.data.attrs[attr]\n            if type(val).__module__ == np.__name__:\n                if val.dtype.type == np.string_:\n                    val = val.tolist()\n        elif attr in self.data:\n            val = self.data[attr]\n            if isinstance(val, h5py.Dataset):\n                val = np.asarray(val)\n            else:\n                val = H5Dict(val)\n        else:\n            # could be chunked\n            chunk_attr = '%s%d' % (attr, 0)\n            is_chunked = chunk_attr in self.data.attrs\n            if is_chunked:\n                val = []\n                chunk_id = 0\n                while chunk_attr in self.data.attrs:\n                    chunk = self.data.attrs[chunk_attr]\n                    val.extend([x.decode('utf8') for x in chunk])\n                    chunk_id += 1\n                    chunk_attr = '%s%d' % (attr, chunk_id)\n            else:\n                if self.read_only:\n                    raise ValueError('Cannot create group in read only mode.')\n                val = H5Dict(self.data.create_group(attr))\n        return val",
        "begin_line": 258,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.H5Dict.close#326",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.H5Dict",
        "signature": "keras.utils.io_utils.H5Dict.close(self)",
        "snippet": "    def close(self):\n        if isinstance(self.data, h5py.Group):\n            self.data.file.flush()\n            if self._is_file:\n                self.data.close()",
        "begin_line": 326,
        "end_line": 330,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.699321047526673e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.H5Dict.__contains__#337",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.H5Dict",
        "signature": "keras.utils.io_utils.H5Dict.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        if isinstance(self.data, dict):\n            return key in self.data\n        else:\n            return (key in self.data) or (key in self.data.attrs)",
        "begin_line": 337,
        "end_line": 341,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.io_utils.H5Dict.get#343",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.H5Dict",
        "signature": "keras.utils.io_utils.H5Dict.get(self, key, default=None)",
        "snippet": "    def get(self, key, default=None):\n        if key in self:\n            return self[key]\n        return default",
        "begin_line": 343,
        "end_line": 346,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.__init__#128",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
        "snippet": "    def __init__(self, cell,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if unroll:\n            raise TypeError('Unrolling isn\\'t possible with '\n                            'convolutional RNNs.')\n        if isinstance(cell, (list, tuple)):\n            # The StackedConvRNN2DCells isn't implemented yet.\n            raise TypeError('It is not possible at the moment to'\n                            'stack convolutional cells.')\n        super(ConvRNN2D, self).__init__(cell,\n                                        return_sequences,\n                                        return_state,\n                                        go_backwards,\n                                        stateful,\n                                        unroll,\n                                        **kwargs)\n        self.input_spec = [InputSpec(ndim=5)]",
        "begin_line": 128,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.compute_output_shape#151",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        cell = self.cell\n        if cell.data_format == 'channels_first':\n            rows = input_shape[3]\n            cols = input_shape[4]\n        elif cell.data_format == 'channels_last':\n            rows = input_shape[2]\n            cols = input_shape[3]\n        rows = conv_utils.conv_output_length(rows,\n                                             cell.kernel_size[0],\n                                             padding=cell.padding,\n                                             stride=cell.strides[0],\n                                             dilation=cell.dilation_rate[0])\n        cols = conv_utils.conv_output_length(cols,\n                                             cell.kernel_size[1],\n                                             padding=cell.padding,\n                                             stride=cell.strides[1],\n                                             dilation=cell.dilation_rate[1])\n\n        output_shape = input_shape[:2] + (rows, cols, cell.filters)\n        output_shape = transpose_shape(output_shape, cell.data_format,\n                                       spatial_axes=(2, 3))\n\n        if not self.return_sequences:\n            output_shape = output_shape[:1] + output_shape[2:]\n\n        if self.return_state:\n            output_shape = [output_shape]\n            base = (input_shape[0], rows, cols, cell.filters)\n            base = transpose_shape(base, cell.data_format, spatial_axes=(1, 2))\n            output_shape += [base[:] for _ in range(2)]\n        return output_shape",
        "begin_line": 151,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.build#187",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        # Note input_shape will be list of shapes of initial states and\n        # constants if these are passed in __call__.\n        if self._num_constants is not None:\n            constants_shape = input_shape[-self._num_constants:]\n        else:\n            constants_shape = None\n\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_spec[0] = InputSpec(shape=(batch_size, None) + input_shape[2:5])\n\n        # allow cell (if layer) to build before we set or validate state_spec\n        if isinstance(self.cell, Layer):\n            step_input_shape = (input_shape[0],) + input_shape[2:]\n            if constants_shape is not None:\n                self.cell.build([step_input_shape] + constants_shape)\n            else:\n                self.cell.build(step_input_shape)\n\n        # set or validate state_spec\n        if hasattr(self.cell.state_size, '__len__'):\n            state_size = list(self.cell.state_size)\n        else:\n            state_size = [self.cell.state_size]\n\n        if self.state_spec is not None:\n            # initial_state was passed in call, check compatibility\n            if self.cell.data_format == 'channels_first':\n                ch_dim = 1\n            elif self.cell.data_format == 'channels_last':\n                ch_dim = 3\n            if not [spec.shape[ch_dim] for spec in self.state_spec] == state_size:\n                raise ValueError(\n                    'An initial_state was passed that is not compatible with '\n                    '`cell.state_size`. Received `state_spec`={}; '\n                    'However `cell.state_size` is '\n                    '{}'.format([spec.shape for spec in self.state_spec], self.cell.state_size))\n        else:\n            if self.cell.data_format == 'channels_first':\n                self.state_spec = [InputSpec(shape=(None, dim, None, None))\n                                   for dim in state_size]\n            elif self.cell.data_format == 'channels_last':\n                self.state_spec = [InputSpec(shape=(None, None, None, dim))\n                                   for dim in state_size]\n        if self.stateful:\n            self.reset_states()\n        self.built = True",
        "begin_line": 187,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.get_initial_state#238",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.get_initial_state(self, inputs)",
        "snippet": "    def get_initial_state(self, inputs):\n        # (samples, timesteps, rows, cols, filters)\n        initial_state = K.zeros_like(inputs)\n        # (samples, rows, cols, filters)\n        initial_state = K.sum(initial_state, axis=1)\n        shape = list(self.cell.kernel_shape)\n        shape[-1] = self.cell.filters\n        initial_state = self.cell.input_conv(initial_state,\n                                             K.zeros(tuple(shape)),\n                                             padding=self.cell.padding)\n        # Fix for Theano because it needs\n        # K.int_shape to work in call() with initial_state.\n        keras_shape = list(K.int_shape(inputs))\n        keras_shape.pop(1)\n        if K.image_data_format() == 'channels_first':\n            indices = 2, 3\n        else:\n            indices = 1, 2\n        for i, j in enumerate(indices):\n            keras_shape[j] = conv_utils.conv_output_length(\n                keras_shape[j],\n                shape[i],\n                padding=self.cell.padding,\n                stride=self.cell.strides[i],\n                dilation=self.cell.dilation_rate[i])\n        initial_state._keras_shape = keras_shape\n\n        if hasattr(self.cell.state_size, '__len__'):\n            return [initial_state for _ in self.cell.state_size]\n        else:\n            return [initial_state]",
        "begin_line": 238,
        "end_line": 268,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.__call__#270",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
        "snippet": "    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n        inputs, initial_state, constants = _standardize_args(\n            inputs, initial_state, constants, self._num_constants)\n\n        if initial_state is None and constants is None:\n            return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n\n        # If any of `initial_state` or `constants` are specified and are Keras\n        # tensors, then add them to the inputs and temporarily modify the\n        # input_spec to include them.\n\n        additional_inputs = []\n        additional_specs = []\n        if initial_state is not None:\n            kwargs['initial_state'] = initial_state\n            additional_inputs += initial_state\n            self.state_spec = []\n            for state in initial_state:\n                try:\n                    shape = K.int_shape(state)\n                # Fix for Theano\n                except TypeError:\n                    shape = tuple(None for _ in range(K.ndim(state)))\n                self.state_spec.append(InputSpec(shape=shape))\n\n            additional_specs += self.state_spec\n        if constants is not None:\n            kwargs['constants'] = constants\n            additional_inputs += constants\n            self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                                   for constant in constants]\n            self._num_constants = len(constants)\n            additional_specs += self.constants_spec\n        # at this point additional_inputs cannot be empty\n        for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != K.is_keras_tensor(additional_inputs[0]):\n                raise ValueError('The initial state or constants of an RNN'\n                                 ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors')\n\n        if K.is_keras_tensor(additional_inputs[0]):\n            # Compute the full input spec, including state and constants\n            full_input = [inputs] + additional_inputs\n            full_input_spec = self.input_spec + additional_specs\n            # Perform the call with temporarily replaced input_spec\n            original_input_spec = self.input_spec\n            self.input_spec = full_input_spec\n            output = super(ConvRNN2D, self).__call__(full_input, **kwargs)\n            self.input_spec = original_input_spec\n            return output\n        else:\n            return super(ConvRNN2D, self).__call__(inputs, **kwargs)",
        "begin_line": 270,
        "end_line": 321,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.call#323",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
        "snippet": "    def call(self,\n             inputs,\n             mask=None,\n             training=None,\n             initial_state=None,\n             constants=None):\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            inputs = inputs[0]\n        if initial_state is not None:\n            pass\n        elif self.stateful:\n            initial_state = self.states\n        else:\n            initial_state = self.get_initial_state(inputs)\n\n        if isinstance(mask, list):\n            mask = mask[0]\n\n        if len(initial_state) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_state)) +\n                             ' initial states.')\n        timesteps = K.int_shape(inputs)[1]\n\n        kwargs = {}\n        if has_arg(self.cell.call, 'training'):\n            kwargs['training'] = training\n\n        if constants:\n            if not has_arg(self.cell.call, 'constants'):\n                raise ValueError('RNN cell does not support constants')\n\n            def step(inputs, states):\n                constants = states[-self._num_constants:]\n                states = states[:-self._num_constants]\n                return self.cell.call(inputs, states, constants=constants,\n                                      **kwargs)\n        else:\n            def step(inputs, states):\n                return self.cell.call(inputs, states, **kwargs)\n\n        last_output, outputs, states = K.rnn(step,\n                                             inputs,\n                                             initial_state,\n                                             constants=constants,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             input_length=timesteps)\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        if self.return_sequences:\n            output = outputs\n        else:\n            output = last_output\n\n        # Properly set learning phase\n        if getattr(last_output, '_uses_learning_phase', False):\n            output._uses_learning_phase = True\n\n        if self.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            return [output] + states\n        else:\n            return output",
        "begin_line": 323,
        "end_line": 396,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.step#364",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.step(inputs, states)",
        "snippet": "            def step(inputs, states):\n                return self.cell.call(inputs, states, **kwargs)",
        "begin_line": 364,
        "end_line": 365,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.reset_states#398",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.reset_states(self, states=None)",
        "snippet": "    def reset_states(self, states=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec[0].shape\n        state_shape = self.compute_output_shape(input_shape)\n        if self.return_state:\n            state_shape = state_shape[0]\n        if self.return_sequences:\n            state_shape = state_shape[:1] + state_shape[2:]\n        if None in state_shape:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.\\n'\n                             'The same thing goes for the number of rows and columns.')\n\n        # helper function\n        def get_tuple_shape(nb_channels):\n            result = list(state_shape)\n            if self.cell.data_format == 'channels_first':\n                result[1] = nb_channels\n            elif self.cell.data_format == 'channels_last':\n                result[3] = nb_channels\n            else:\n                raise KeyError\n            return tuple(result)\n\n        # initialize state if None\n        if self.states[0] is None:\n            if hasattr(self.cell.state_size, '__len__'):\n                self.states = [K.zeros(get_tuple_shape(dim))\n                               for dim in self.cell.state_size]\n            else:\n                self.states = [K.zeros(get_tuple_shape(self.cell.state_size))]\n        elif states is None:\n            if hasattr(self.cell.state_size, '__len__'):\n                for state, dim in zip(self.states, self.cell.state_size):\n                    K.set_value(state, np.zeros(get_tuple_shape(dim)))\n            else:\n                K.set_value(self.states[0],\n                            np.zeros(get_tuple_shape(self.cell.state_size)))\n        else:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            if len(states) != len(self.states):\n                raise ValueError('Layer ' + self.name + ' expects ' +\n                                 str(len(self.states)) + ' states, '\n                                                         'but it received ' + str(len(states)) +\n                                 ' state values. Input received: ' +\n                                 str(states))\n            for index, (value, state) in enumerate(zip(states, self.states)):\n                if hasattr(self.cell.state_size, '__len__'):\n                    dim = self.cell.state_size[index]\n                else:\n                    dim = self.cell.state_size\n                if value.shape != get_tuple_shape(dim):\n                    raise ValueError('State ' + str(index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected shape=' +\n                                     str(get_tuple_shape(dim)) +\n                                     ', found shape=' + str(value.shape))\n                # TODO: consider batch calls to `set_value`.\n                K.set_value(state, value)",
        "begin_line": 398,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvRNN2D.get_tuple_shape#421",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvRNN2D",
        "signature": "keras.layers.convolutional_recurrent.ConvRNN2D.get_tuple_shape(nb_channels)",
        "snippet": "        def get_tuple_shape(nb_channels):\n            result = list(state_shape)\n            if self.cell.data_format == 'channels_first':\n                result[1] = nb_channels\n            elif self.cell.data_format == 'channels_last':\n                result[3] = nb_channels\n            else:\n                raise KeyError\n            return tuple(result)",
        "begin_line": 421,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.__init__#536",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(ConvLSTM2DCell, self).__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2, 'dilation_rate')\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_size = (self.filters, self.filters)\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None",
        "begin_line": 536,
        "end_line": 594,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.build#596",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n        self.kernel_shape = kernel_shape\n        recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=recurrent_kernel_shape,\n            initializer=self.recurrent_initializer,\n            name='recurrent_kernel',\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(_, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.filters,), *args, **kwargs),\n                        initializers.Ones()((self.filters,), *args, **kwargs),\n                        self.bias_initializer((self.filters * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.filters * 4,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :, :, :self.filters]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :, :, :self.filters]\n        self.kernel_f = self.kernel[:, :, :, self.filters: self.filters * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, :, :, self.filters: self.filters * 2]\n        self.kernel_c = self.kernel[:, :, :, self.filters * 2: self.filters * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, :, :, self.filters * 2: self.filters * 3]\n        self.kernel_o = self.kernel[:, :, :, self.filters * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, :, :, self.filters * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.filters]\n            self.bias_f = self.bias[self.filters: self.filters * 2]\n            self.bias_c = self.bias[self.filters * 2: self.filters * 3]\n            self.bias_o = self.bias[self.filters * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True",
        "begin_line": 596,
        "end_line": 658,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.bias_initializer#623",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.bias_initializer(_, *args, **kwargs)",
        "snippet": "                def bias_initializer(_, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.filters,), *args, **kwargs),\n                        initializers.Ones()((self.filters,), *args, **kwargs),\n                        self.bias_initializer((self.filters * 2,), *args, **kwargs),\n                    ])",
        "begin_line": 623,
        "end_line": 628,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.call#660",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.call(self, inputs, states, training=None)",
        "snippet": "    def call(self, inputs, states, training=None):\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                K.ones_like(inputs),\n                self.dropout,\n                training=training,\n                count=4)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_dropout_mask is None):\n            self._recurrent_dropout_mask = _generate_dropout_mask(\n                K.ones_like(states[1]),\n                self.recurrent_dropout,\n                training=training,\n                count=4)\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        h_tm1 = states[0]  # previous memory state\n        c_tm1 = states[1]  # previous carry state\n\n        if 0 < self.dropout < 1.:\n            inputs_i = inputs * dp_mask[0]\n            inputs_f = inputs * dp_mask[1]\n            inputs_c = inputs * dp_mask[2]\n            inputs_o = inputs * dp_mask[3]\n        else:\n            inputs_i = inputs\n            inputs_f = inputs\n            inputs_c = inputs\n            inputs_o = inputs\n\n        if 0 < self.recurrent_dropout < 1.:\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\n        else:\n            h_tm1_i = h_tm1\n            h_tm1_f = h_tm1\n            h_tm1_c = h_tm1\n            h_tm1_o = h_tm1\n\n        x_i = self.input_conv(inputs_i, self.kernel_i, self.bias_i,\n                              padding=self.padding)\n        x_f = self.input_conv(inputs_f, self.kernel_f, self.bias_f,\n                              padding=self.padding)\n        x_c = self.input_conv(inputs_c, self.kernel_c, self.bias_c,\n                              padding=self.padding)\n        x_o = self.input_conv(inputs_o, self.kernel_o, self.bias_o,\n                              padding=self.padding)\n        h_i = self.recurrent_conv(h_tm1_i,\n                                  self.recurrent_kernel_i)\n        h_f = self.recurrent_conv(h_tm1_f,\n                                  self.recurrent_kernel_f)\n        h_c = self.recurrent_conv(h_tm1_c,\n                                  self.recurrent_kernel_c)\n        h_o = self.recurrent_conv(h_tm1_o,\n                                  self.recurrent_kernel_o)\n\n        i = self.recurrent_activation(x_i + h_i)\n        f = self.recurrent_activation(x_f + h_f)\n        c = f * c_tm1 + i * self.activation(x_c + h_c)\n        o = self.recurrent_activation(x_o + h_o)\n        h = o * self.activation(c)\n\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n\n        return h, [h, c]",
        "begin_line": 660,
        "end_line": 732,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.input_conv#734",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.input_conv(self, x, w, b=None, padding='valid')",
        "snippet": "    def input_conv(self, x, w, b=None, padding='valid'):\n        conv_out = K.conv2d(x, w, strides=self.strides,\n                            padding=padding,\n                            data_format=self.data_format,\n                            dilation_rate=self.dilation_rate)\n        if b is not None:\n            conv_out = K.bias_add(conv_out, b,\n                                  data_format=self.data_format)\n        return conv_out",
        "begin_line": 734,
        "end_line": 742,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.recurrent_conv#744",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.recurrent_conv(self, x, w)",
        "snippet": "    def recurrent_conv(self, x, w):\n        conv_out = K.conv2d(x, w, strides=(1, 1),\n                            padding='same',\n                            data_format=self.data_format)\n        return conv_out",
        "begin_line": 744,
        "end_line": 748,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.get_config#750",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2DCell",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2DCell.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'filters': self.filters,\n                  'kernel_size': self.kernel_size,\n                  'strides': self.strides,\n                  'padding': self.padding,\n                  'data_format': self.data_format,\n                  'dilation_rate': self.dilation_rate,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(ConvLSTM2DCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 750,
        "end_line": 773,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.__init__#897",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        cell = ConvLSTM2DCell(filters=filters,\n                              kernel_size=kernel_size,\n                              strides=strides,\n                              padding=padding,\n                              data_format=data_format,\n                              dilation_rate=dilation_rate,\n                              activation=activation,\n                              recurrent_activation=recurrent_activation,\n                              use_bias=use_bias,\n                              kernel_initializer=kernel_initializer,\n                              recurrent_initializer=recurrent_initializer,\n                              bias_initializer=bias_initializer,\n                              unit_forget_bias=unit_forget_bias,\n                              kernel_regularizer=kernel_regularizer,\n                              recurrent_regularizer=recurrent_regularizer,\n                              bias_regularizer=bias_regularizer,\n                              kernel_constraint=kernel_constraint,\n                              recurrent_constraint=recurrent_constraint,\n                              bias_constraint=bias_constraint,\n                              dropout=dropout,\n                              recurrent_dropout=recurrent_dropout)\n        super(ConvLSTM2D, self).__init__(cell,\n                                         return_sequences=return_sequences,\n                                         go_backwards=go_backwards,\n                                         stateful=stateful,\n                                         **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)",
        "begin_line": 897,
        "end_line": 949,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.call#951",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.call(self, inputs, mask=None, training=None, initial_state=None)",
        "snippet": "    def call(self, inputs, mask=None, training=None, initial_state=None):\n        return super(ConvLSTM2D, self).call(inputs,\n                                            mask=mask,\n                                            training=training,\n                                            initial_state=initial_state)",
        "begin_line": 951,
        "end_line": 955,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.filters#958",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.filters(self)",
        "snippet": "    def filters(self):\n        return self.cell.filters",
        "begin_line": 958,
        "end_line": 959,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_size#962",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_size(self)",
        "snippet": "    def kernel_size(self):\n        return self.cell.kernel_size",
        "begin_line": 962,
        "end_line": 963,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.strides#966",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.strides(self)",
        "snippet": "    def strides(self):\n        return self.cell.strides",
        "begin_line": 966,
        "end_line": 967,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.padding#970",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.padding(self)",
        "snippet": "    def padding(self):\n        return self.cell.padding",
        "begin_line": 970,
        "end_line": 971,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.data_format#974",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.data_format(self)",
        "snippet": "    def data_format(self):\n        return self.cell.data_format",
        "begin_line": 974,
        "end_line": 975,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.dilation_rate#978",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.dilation_rate(self)",
        "snippet": "    def dilation_rate(self):\n        return self.cell.dilation_rate",
        "begin_line": 978,
        "end_line": 979,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.activation#982",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.activation(self)",
        "snippet": "    def activation(self):\n        return self.cell.activation",
        "begin_line": 982,
        "end_line": 983,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_activation#986",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_activation(self)",
        "snippet": "    def recurrent_activation(self):\n        return self.cell.recurrent_activation",
        "begin_line": 986,
        "end_line": 987,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.use_bias#990",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.use_bias(self)",
        "snippet": "    def use_bias(self):\n        return self.cell.use_bias",
        "begin_line": 990,
        "end_line": 991,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_initializer#994",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_initializer(self)",
        "snippet": "    def kernel_initializer(self):\n        return self.cell.kernel_initializer",
        "begin_line": 994,
        "end_line": 995,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_initializer#998",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_initializer(self)",
        "snippet": "    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer",
        "begin_line": 998,
        "end_line": 999,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.bias_initializer#1002",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.bias_initializer(self)",
        "snippet": "    def bias_initializer(self):\n        return self.cell.bias_initializer",
        "begin_line": 1002,
        "end_line": 1003,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.unit_forget_bias#1006",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.unit_forget_bias(self)",
        "snippet": "    def unit_forget_bias(self):\n        return self.cell.unit_forget_bias",
        "begin_line": 1006,
        "end_line": 1007,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_regularizer#1010",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_regularizer(self)",
        "snippet": "    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer",
        "begin_line": 1010,
        "end_line": 1011,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_regularizer#1014",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_regularizer(self)",
        "snippet": "    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer",
        "begin_line": 1014,
        "end_line": 1015,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.bias_regularizer#1018",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.bias_regularizer(self)",
        "snippet": "    def bias_regularizer(self):\n        return self.cell.bias_regularizer",
        "begin_line": 1018,
        "end_line": 1019,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_constraint#1022",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.kernel_constraint(self)",
        "snippet": "    def kernel_constraint(self):\n        return self.cell.kernel_constraint",
        "begin_line": 1022,
        "end_line": 1023,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_constraint#1026",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_constraint(self)",
        "snippet": "    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint",
        "begin_line": 1026,
        "end_line": 1027,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.bias_constraint#1030",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.bias_constraint(self)",
        "snippet": "    def bias_constraint(self):\n        return self.cell.bias_constraint",
        "begin_line": 1030,
        "end_line": 1031,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.dropout#1034",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.dropout(self)",
        "snippet": "    def dropout(self):\n        return self.cell.dropout",
        "begin_line": 1034,
        "end_line": 1035,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_dropout#1038",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.recurrent_dropout(self)",
        "snippet": "    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout",
        "begin_line": 1038,
        "end_line": 1039,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.get_config#1041",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'filters': self.filters,\n                  'kernel_size': self.kernel_size,\n                  'strides': self.strides,\n                  'padding': self.padding,\n                  'data_format': self.data_format,\n                  'dilation_rate': self.dilation_rate,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(ConvLSTM2D, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 1041,
        "end_line": 1066,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional_recurrent.ConvLSTM2D.from_config#1069",
        "src_path": "keras/layers/convolutional_recurrent.py",
        "class_name": "keras.layers.convolutional_recurrent.ConvLSTM2D",
        "signature": "keras.layers.convolutional_recurrent.ConvLSTM2D.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        return cls(**config)",
        "begin_line": 1069,
        "end_line": 1070,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.__init__#29",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.__init__(self, layer, **kwargs)",
        "snippet": "    def __init__(self, layer, **kwargs):\n        self.layer = layer\n        # Tracks mapping of Wrapper inputs to inner layer inputs. Useful when\n        # the inner layer has update ops that depend on its inputs (as opposed\n        # to the inputs to the Wrapper layer).\n        self._input_map = {}\n        super(Wrapper, self).__init__(**kwargs)",
        "begin_line": 29,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.208951100469657e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.build#37",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.build(self, input_shape=None)",
        "snippet": "    def build(self, input_shape=None):\n        self.built = True",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.activity_regularizer#41",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.activity_regularizer(self)",
        "snippet": "    def activity_regularizer(self):\n        if hasattr(self.layer, 'activity_regularizer'):\n            return self.layer.activity_regularizer\n        else:\n            return None",
        "begin_line": 41,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.trainable#48",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.trainable(self)",
        "snippet": "    def trainable(self):\n        return self.layer.trainable",
        "begin_line": 48,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.trainable#52",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.trainable(self, value)",
        "snippet": "    def trainable(self, value):\n        self.layer.trainable = value",
        "begin_line": 52,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.trainable_weights#56",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        return self.layer.trainable_weights",
        "begin_line": 56,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010481081647626035,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.non_trainable_weights#60",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.non_trainable_weights(self)",
        "snippet": "    def non_trainable_weights(self):\n        return self.layer.non_trainable_weights",
        "begin_line": 60,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.updates#64",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.updates(self)",
        "snippet": "    def updates(self):\n        if hasattr(self.layer, 'updates'):\n            return self.layer.updates\n        return []",
        "begin_line": 64,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.get_updates_for#69",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.get_updates_for(self, inputs=None)",
        "snippet": "    def get_updates_for(self, inputs=None):\n        # If the wrapper modifies the inputs, use the modified inputs to\n        # get the updates from the inner layer.\n        inner_inputs = inputs\n        if inputs is not None:\n            uid = object_list_uid(inputs)\n            if uid in self._input_map:\n                inner_inputs = self._input_map[uid]\n\n        updates = self.layer.get_updates_for(inner_inputs)\n        updates += super(Wrapper, self).get_updates_for(inputs)\n        return updates",
        "begin_line": 69,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.losses#83",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.losses(self)",
        "snippet": "    def losses(self):\n        if hasattr(self.layer, 'losses'):\n            return self.layer.losses\n        return []",
        "begin_line": 83,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.get_losses_for#88",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.get_losses_for(self, inputs=None)",
        "snippet": "    def get_losses_for(self, inputs=None):\n        if inputs is None:\n            losses = self.layer.get_losses_for(None)\n            return losses + super(Wrapper, self).get_losses_for(None)\n        return super(Wrapper, self).get_losses_for(inputs)",
        "begin_line": 88,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.get_weights#94",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.get_weights(self)",
        "snippet": "    def get_weights(self):\n        return self.layer.get_weights()",
        "begin_line": 94,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.set_weights#97",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.set_weights(self, weights)",
        "snippet": "    def set_weights(self, weights):\n        self.layer.set_weights(weights)",
        "begin_line": 97,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.get_config#100",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'layer': {'class_name': self.layer.__class__.__name__,\n                            'config': self.layer.get_config()}}\n        base_config = super(Wrapper, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 100,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Wrapper.from_config#107",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Wrapper",
        "signature": "keras.layers.wrappers.Wrapper.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        from . import deserialize as deserialize_layer\n        layer = deserialize_layer(config.pop('layer'),\n                                  custom_objects=custom_objects)\n        return cls(layer, **config)",
        "begin_line": 107,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed.__init__#159",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed.__init__(self, layer, **kwargs)",
        "snippet": "    def __init__(self, layer, **kwargs):\n        super(TimeDistributed, self).__init__(layer, **kwargs)\n        self.supports_masking = True",
        "begin_line": 159,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed._get_shape_tuple#163",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed._get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None)",
        "snippet": "    def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n        \"\"\"Finds non-specific dimensions in the static shapes\n        and replaces them by the corresponding dynamic shapes of the tensor.\n\n        # Arguments\n            init_tuple: a tuple, the first part of the output shape\n            tensor: the tensor from which to get the (static and dynamic) shapes\n                as the last part of the output shape\n            start_idx: int, which indicate the first dimension to take from\n                the static shape of the tensor\n            int_shape: an alternative static shape to take as the last part\n                of the output shape\n\n        # Returns\n            The new int_shape with the first part from init_tuple\n            and the last part from either `int_shape` (if provided)\n            or K.int_shape(tensor), where every `None` is replaced by\n            the corresponding dimension from K.shape(tensor)\n        \"\"\"\n        # replace all None in int_shape by K.shape\n        if int_shape is None:\n            int_shape = K.int_shape(tensor)[start_idx:]\n        if not any(not s for s in int_shape):\n            return init_tuple + int_shape\n        tensor_shape = K.shape(tensor)\n        int_shape = list(int_shape)\n        for i, s in enumerate(int_shape):\n            if not s:\n                int_shape[i] = tensor_shape[start_idx + i]\n        return init_tuple + tuple(int_shape)",
        "begin_line": 163,
        "end_line": 192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed.build#194",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        assert len(input_shape) >= 3\n        self.input_spec = InputSpec(shape=input_shape)\n        child_input_shape = (input_shape[0],) + input_shape[2:]\n        if not self.layer.built:\n            self.layer.build(child_input_shape)\n            self.layer.built = True\n        super(TimeDistributed, self).build()",
        "begin_line": 194,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010357327809425168,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed.compute_output_shape#203",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        child_input_shape = (input_shape[0],) + input_shape[2:]\n        child_output_shape = self.layer.compute_output_shape(child_input_shape)\n        timesteps = input_shape[1]\n        return (child_output_shape[0], timesteps) + child_output_shape[1:]",
        "begin_line": 203,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed.call#209",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed.call(self, inputs, training=None, mask=None)",
        "snippet": "    def call(self, inputs, training=None, mask=None):\n        kwargs = {}\n        if has_arg(self.layer.call, 'training'):\n            kwargs['training'] = training\n        uses_learning_phase = False\n\n        input_shape = K.int_shape(inputs)\n        if input_shape[0]:\n            # batch size matters, use rnn-based implementation\n            def step(x, _):\n                global uses_learning_phase\n                output = self.layer.call(x, **kwargs)\n                if hasattr(output, '_uses_learning_phase'):\n                    uses_learning_phase = (output._uses_learning_phase or\n                                           uses_learning_phase)\n                return output, []\n\n            _, outputs, _ = K.rnn(step, inputs,\n                                  initial_states=[],\n                                  input_length=input_shape[1],\n                                  unroll=False)\n            y = outputs\n        else:\n            # No batch size specified, therefore the layer will be able\n            # to process batches of any size.\n            # We can go with reshape-based implementation for performance.\n            input_length = input_shape[1]\n            if not input_length:\n                input_length = K.shape(inputs)[1]\n            inner_input_shape = self._get_shape_tuple((-1,), inputs, 2)\n            # Shape: (num_samples * timesteps, ...). And track the\n            # transformation in self._input_map.\n            input_uid = object_list_uid(inputs)\n            inputs = K.reshape(inputs, inner_input_shape)\n            self._input_map[input_uid] = inputs\n            # (num_samples * timesteps, ...)\n            if has_arg(self.layer.call, 'mask') and mask is not None:\n                inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n                kwargs['mask'] = K.reshape(mask, inner_mask_shape)\n            y = self.layer.call(inputs, **kwargs)\n            if hasattr(y, '_uses_learning_phase'):\n                uses_learning_phase = y._uses_learning_phase\n            # Shape: (num_samples, timesteps, ...)\n            output_shape = self.compute_output_shape(input_shape)\n            output_shape = self._get_shape_tuple(\n                (-1, input_length), y, 1, output_shape[2:])\n            y = K.reshape(y, output_shape)\n\n        # Apply activity regularizer if any:\n        if (hasattr(self.layer, 'activity_regularizer') and\n           self.layer.activity_regularizer is not None):\n            regularization_loss = self.layer.activity_regularizer(y)\n            self.add_loss(regularization_loss, inputs)\n\n        if uses_learning_phase:\n            y._uses_learning_phase = True\n        return y",
        "begin_line": 209,
        "end_line": 265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed.step#218",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed.step(x, _)",
        "snippet": "            def step(x, _):\n                global uses_learning_phase\n                output = self.layer.call(x, **kwargs)\n                if hasattr(output, '_uses_learning_phase'):\n                    uses_learning_phase = (output._uses_learning_phase or\n                                           uses_learning_phase)\n                return output, []",
        "begin_line": 218,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.TimeDistributed.compute_mask#267",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.TimeDistributed",
        "signature": "keras.layers.wrappers.TimeDistributed.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        \"\"\"Computes an output mask tensor for Embedding layer\n        based on the inputs, mask, and the inner layer.\n\n        If batch size is specified:\n        Simply return the input `mask`. (An rnn-based implementation with\n        more than one rnn inputs is required but not supported in Keras yet.)\n\n        Otherwise we call `compute_mask` of the inner layer at each time step.\n        If the output mask at each time step is not `None`:\n        (E.g., inner layer is Masking or RNN)\n        Concatenate all of them and return the concatenation.\n        If the output mask at each time step is `None` and the input mask is not `None`:\n        (E.g., inner layer is Dense)\n        Reduce the input_mask to 2 dimensions and return it.\n        Otherwise (both the output mask and the input mask are `None`):\n        (E.g., `mask` is not used at all)\n        Return `None`.\n\n        # Arguments\n            inputs: Tensor\n            mask: Tensor\n        # Returns\n            None or a tensor\n        \"\"\"\n        # cases need to call the layer.compute_mask when input_mask is None:\n        # Masking layer and Embedding layer with mask_zero\n        input_shape = K.int_shape(inputs)\n        if input_shape[0]:\n            # batch size matters, we currently do not handle mask explicitly\n            return mask\n        inner_mask = mask\n        if inner_mask is not None:\n            inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n            inner_mask = K.reshape(inner_mask, inner_mask_shape)\n        input_uid = object_list_uid(inputs)\n        inner_inputs = self._input_map[input_uid]\n        output_mask = self.layer.compute_mask(inner_inputs, inner_mask)\n        if output_mask is None:\n            if mask is None:\n                return None\n            # input_mask is not None, and output_mask is None:\n            # we should return a not-None mask\n            output_mask = mask\n            for _ in range(2, len(K.int_shape(mask))):\n                output_mask = K.any(output_mask, axis=-1)\n        else:\n            # output_mask is not None. We need to reshape it\n            input_length = input_shape[1]\n            if not input_length:\n                input_length = K.shape(inputs)[1]\n            output_mask_int_shape = K.int_shape(output_mask)\n            if output_mask_int_shape is None:\n                # if the output_mask does not have a static shape,\n                # its shape must be the same as mask's\n                if mask is not None:\n                    output_mask_int_shape = K.int_shape(mask)\n                else:\n                    output_mask_int_shape = K.compute_output_shape(input_shape)[:-1]\n            output_mask_shape = self._get_shape_tuple(\n                (-1, input_length), output_mask, 1, output_mask_int_shape[1:])\n            output_mask = K.reshape(output_mask, output_mask_shape)\n        return output_mask",
        "begin_line": 267,
        "end_line": 329,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.__init__#359",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.__init__(self, layer, merge_mode='concat', weights=None, **kwargs)",
        "snippet": "    def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n        if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n            raise ValueError('Invalid merge mode. '\n                             'Merge mode should be one of '\n                             '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n        self.forward_layer = copy.copy(layer)\n        config = layer.get_config()\n        config['go_backwards'] = not config['go_backwards']\n        self.backward_layer = layer.__class__.from_config(config)\n        self.forward_layer.name = 'forward_' + self.forward_layer.name\n        self.backward_layer.name = 'backward_' + self.backward_layer.name\n        self.merge_mode = merge_mode\n        if weights:\n            nw = len(weights)\n            self.forward_layer.initial_weights = weights[:nw // 2]\n            self.backward_layer.initial_weights = weights[nw // 2:]\n        self.stateful = layer.stateful\n        self.return_sequences = layer.return_sequences\n        self.return_state = layer.return_state\n        self.supports_masking = True\n        self._trainable = True\n        super(Bidirectional, self).__init__(layer, **kwargs)\n        self.input_spec = layer.input_spec\n        self._num_constants = None",
        "begin_line": 359,
        "end_line": 382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.trainable#385",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.trainable(self)",
        "snippet": "    def trainable(self):\n        return self._trainable",
        "begin_line": 385,
        "end_line": 386,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.trainable#389",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.trainable(self, value)",
        "snippet": "    def trainable(self, value):\n        self._trainable = value\n        self.forward_layer.trainable = value\n        self.backward_layer.trainable = value",
        "begin_line": 389,
        "end_line": 392,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.compute_output_shape#402",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        output_shape = self.forward_layer.compute_output_shape(input_shape)\n        if self.return_state:\n            state_shape = output_shape[1:]\n            output_shape = output_shape[0]\n\n        if self.merge_mode == 'concat':\n            output_shape = list(output_shape)\n            output_shape[-1] *= 2\n            output_shape = tuple(output_shape)\n        elif self.merge_mode is None:\n            output_shape = [output_shape, copy.copy(output_shape)]\n\n        if self.return_state:\n            if self.merge_mode is None:\n                return output_shape + state_shape + copy.copy(state_shape)\n            return [output_shape] + state_shape + copy.copy(state_shape)\n        return output_shape",
        "begin_line": 402,
        "end_line": 419,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.__call__#421",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
        "snippet": "    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n        inputs, initial_state, constants = recurrent._standardize_args(\n            inputs, initial_state, constants, self._num_constants)\n\n        if initial_state is None and constants is None:\n            return super(Bidirectional, self).__call__(inputs, **kwargs)\n\n        # Applies the same workaround as in `RNN.__call__`\n        additional_inputs = []\n        additional_specs = []\n        if initial_state is not None:\n            # Check if `initial_state` can be splitted into half\n            num_states = len(initial_state)\n            if num_states % 2 > 0:\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, '\n                    'the state should be a list containing the states of '\n                    'the underlying RNNs. '\n                    'Found: ' + str(initial_state))\n\n            kwargs['initial_state'] = initial_state\n            additional_inputs += initial_state\n            state_specs = [InputSpec(shape=K.int_shape(state))\n                           for state in initial_state]\n            self.forward_layer.state_spec = state_specs[:num_states // 2]\n            self.backward_layer.state_spec = state_specs[num_states // 2:]\n            additional_specs += state_specs\n        if constants is not None:\n            kwargs['constants'] = constants\n            additional_inputs += constants\n            constants_spec = [InputSpec(shape=K.int_shape(constant))\n                              for constant in constants]\n            self.forward_layer.constants_spec = constants_spec\n            self.backward_layer.constants_spec = constants_spec\n            additional_specs += constants_spec\n\n            self._num_constants = len(constants)\n            self.forward_layer._num_constants = self._num_constants\n            self.backward_layer._num_constants = self._num_constants\n\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n        for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                raise ValueError('The initial state of a Bidirectional'\n                                 ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state\n            full_input = [inputs] + additional_inputs\n            full_input_spec = self.input_spec + additional_specs\n\n            # Perform the call with temporarily replaced input_spec\n            original_input_spec = self.input_spec\n            self.input_spec = full_input_spec\n            output = super(Bidirectional, self).__call__(full_input, **kwargs)\n            self.input_spec = original_input_spec\n            return output\n        else:\n            return super(Bidirectional, self).__call__(inputs, **kwargs)",
        "begin_line": 421,
        "end_line": 482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.call#484",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
        "snippet": "    def call(self,\n             inputs,\n             mask=None,\n             training=None,\n             initial_state=None,\n             constants=None):\n        kwargs = {}\n        if has_arg(self.layer.call, 'training'):\n            kwargs['training'] = training\n        if has_arg(self.layer.call, 'mask'):\n            kwargs['mask'] = mask\n        if has_arg(self.layer.call, 'constants'):\n            kwargs['constants'] = constants\n\n        if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n            forward_inputs = [inputs[0]]\n            backward_inputs = [inputs[0]]\n            pivot = len(initial_state) // 2 + 1\n            # add forward initial state\n            forward_state = inputs[1:pivot]\n            forward_inputs += forward_state\n            if self._num_constants is None:\n                # add backward initial state\n                backward_state = inputs[pivot:]\n                backward_inputs += backward_state\n            else:\n                # add backward initial state\n                backward_state = inputs[pivot:-self._num_constants]\n                backward_inputs += backward_state\n                # add constants for forward and backward layers\n                forward_inputs += inputs[-self._num_constants:]\n                backward_inputs += inputs[-self._num_constants:]\n            y = self.forward_layer.call(forward_inputs,\n                                        initial_state=forward_state, **kwargs)\n            y_rev = self.backward_layer.call(backward_inputs,\n                                             initial_state=backward_state, **kwargs)\n        else:\n            y = self.forward_layer.call(inputs, **kwargs)\n            y_rev = self.backward_layer.call(inputs, **kwargs)\n\n        if self.return_state:\n            states = y[1:] + y_rev[1:]\n            y = y[0]\n            y_rev = y_rev[0]\n\n        if self.return_sequences:\n            y_rev = K.reverse(y_rev, 1)\n        if self.merge_mode == 'concat':\n            output = K.concatenate([y, y_rev])\n        elif self.merge_mode == 'sum':\n            output = y + y_rev\n        elif self.merge_mode == 'ave':\n            output = (y + y_rev) / 2\n        elif self.merge_mode == 'mul':\n            output = y * y_rev\n        elif self.merge_mode is None:\n            output = [y, y_rev]\n\n        # Properly set learning phase\n        if (getattr(y, '_uses_learning_phase', False) or\n           getattr(y_rev, '_uses_learning_phase', False)):\n            if self.merge_mode is None:\n                for out in output:\n                    out._uses_learning_phase = True\n            else:\n                output._uses_learning_phase = True\n\n        if self.return_state:\n            if self.merge_mode is None:\n                return output + states\n            return [output] + states\n        return output",
        "begin_line": 484,
        "end_line": 555,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.build#561",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        with K.name_scope(self.forward_layer.name):\n            self.forward_layer.build(input_shape)\n        with K.name_scope(self.backward_layer.name):\n            self.backward_layer.build(input_shape)\n        self.built = True",
        "begin_line": 561,
        "end_line": 566,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.compute_mask#568",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.compute_mask(self, inputs, mask)",
        "snippet": "    def compute_mask(self, inputs, mask):\n        if isinstance(mask, list):\n            mask = mask[0]\n        if self.return_sequences:\n            if not self.merge_mode:\n                output_mask = [mask, mask]\n            else:\n                output_mask = mask\n        else:\n            output_mask = [None, None] if not self.merge_mode else None\n\n        if self.return_state:\n            states = self.forward_layer.states\n            state_mask = [None for _ in states]\n            if isinstance(output_mask, list):\n                return output_mask + state_mask * 2\n            return [output_mask] + state_mask * 2\n\n        return output_mask",
        "begin_line": 568,
        "end_line": 586,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.trainable_weights#589",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        if hasattr(self.forward_layer, 'trainable_weights'):\n            return (self.forward_layer.trainable_weights +\n                    self.backward_layer.trainable_weights)\n        return []",
        "begin_line": 589,
        "end_line": 593,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.non_trainable_weights#596",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.non_trainable_weights(self)",
        "snippet": "    def non_trainable_weights(self):\n        if hasattr(self.forward_layer, 'non_trainable_weights'):\n            return (self.forward_layer.non_trainable_weights +\n                    self.backward_layer.non_trainable_weights)\n        return []",
        "begin_line": 596,
        "end_line": 600,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.updates#603",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.updates(self)",
        "snippet": "    def updates(self):\n        if hasattr(self.forward_layer, 'updates'):\n            return self.forward_layer.updates + self.backward_layer.updates\n        return []",
        "begin_line": 603,
        "end_line": 606,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.get_updates_for#608",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.get_updates_for(self, inputs=None)",
        "snippet": "    def get_updates_for(self, inputs=None):\n        forward_updates = self.forward_layer.get_updates_for(inputs)\n        backward_updates = self.backward_layer.get_updates_for(inputs)\n        return (super(Wrapper, self).get_updates_for(inputs) +\n                forward_updates + backward_updates)",
        "begin_line": 608,
        "end_line": 612,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.losses#615",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.losses(self)",
        "snippet": "    def losses(self):\n        if hasattr(self.forward_layer, 'losses'):\n            return self.forward_layer.losses + self.backward_layer.losses\n        return []",
        "begin_line": 615,
        "end_line": 618,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.get_losses_for#620",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.get_losses_for(self, inputs=None)",
        "snippet": "    def get_losses_for(self, inputs=None):\n        forward_losses = self.forward_layer.get_losses_for(inputs)\n        backward_losses = self.backward_layer.get_losses_for(inputs)\n        return (super(Wrapper, self).get_losses_for(inputs) +\n                forward_losses + backward_losses)",
        "begin_line": 620,
        "end_line": 624,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.get_config#634",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'merge_mode': self.merge_mode}\n        if self._num_constants is not None:\n            config['num_constants'] = self._num_constants\n\n        base_config = super(Bidirectional, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 634,
        "end_line": 640,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.wrappers.Bidirectional.from_config#643",
        "src_path": "keras/layers/wrappers.py",
        "class_name": "keras.layers.wrappers.Bidirectional",
        "signature": "keras.layers.wrappers.Bidirectional.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        from . import deserialize as deserialize_layer\n        rnn_layer = deserialize_layer(config.pop('layer'),\n                                      custom_objects=custom_objects)\n        num_constants = config.pop('num_constants', None)\n        layer = cls(rnn_layer, **config)\n        layer._num_constants = num_constants\n        return layer",
        "begin_line": 643,
        "end_line": 650,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.MaxoutDense.__init__#62",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.MaxoutDense",
        "signature": "keras.legacy.layers.MaxoutDense.__init__(self, output_dim, nb_feature=4, init='glorot_uniform', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, **kwargs)",
        "snippet": "    def __init__(self, output_dim,\n                 nb_feature=4,\n                 init='glorot_uniform',\n                 weights=None,\n                 W_regularizer=None,\n                 b_regularizer=None,\n                 activity_regularizer=None,\n                 W_constraint=None,\n                 b_constraint=None,\n                 bias=True,\n                 input_dim=None,\n                 **kwargs):\n        warnings.warn('The `MaxoutDense` layer is deprecated '\n                      'and will be removed after 06/2017.')\n        self.output_dim = output_dim\n        self.nb_feature = nb_feature\n        self.init = initializers.get(init)\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.initial_weights = weights\n        self.input_spec = InputSpec(ndim=2)\n\n        self.input_dim = input_dim\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_dim,)\n        super(MaxoutDense, self).__init__(**kwargs)",
        "begin_line": 62,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.MaxoutDense.build#96",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.MaxoutDense",
        "signature": "keras.legacy.layers.MaxoutDense.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        input_dim = input_shape[1]\n        self.input_spec = InputSpec(dtype=K.floatx(),\n                                    shape=(None, input_dim))\n\n        self.W = self.add_weight((self.nb_feature, input_dim, self.output_dim),\n                                 initializer=self.init,\n                                 name='W',\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((self.nb_feature, self.output_dim,),\n                                     initializer='zero',\n                                     name='b',\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True",
        "begin_line": 96,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.MaxoutDense.compute_output_shape#120",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.MaxoutDense",
        "signature": "keras.legacy.layers.MaxoutDense.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) == 2\n        return (input_shape[0], self.output_dim)",
        "begin_line": 120,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.MaxoutDense.call#124",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.MaxoutDense",
        "signature": "keras.legacy.layers.MaxoutDense.call(self, x)",
        "snippet": "    def call(self, x):\n        # no activation, this layer is only linear.\n        output = K.dot(x, self.W)\n        if self.bias:\n            output += self.b\n        output = K.max(output, axis=1)\n        return output",
        "begin_line": 124,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.MaxoutDense.get_config#132",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.MaxoutDense",
        "signature": "keras.legacy.layers.MaxoutDense.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'output_dim': self.output_dim,\n                  'init': initializers.serialize(self.init),\n                  'nb_feature': self.nb_feature,\n                  'W_regularizer': regularizers.serialize(self.W_regularizer),\n                  'b_regularizer': regularizers.serialize(self.b_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'W_constraint': constraints.serialize(self.W_constraint),\n                  'b_constraint': constraints.serialize(self.b_constraint),\n                  'bias': self.bias,\n                  'input_dim': self.input_dim}\n        base_config = super(MaxoutDense, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 132,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.Highway.__init__#187",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.Highway",
        "signature": "keras.legacy.layers.Highway.__init__(self, init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, **kwargs)",
        "snippet": "    def __init__(self,\n                 init='glorot_uniform',\n                 activation=None,\n                 weights=None,\n                 W_regularizer=None,\n                 b_regularizer=None,\n                 activity_regularizer=None,\n                 W_constraint=None,\n                 b_constraint=None,\n                 bias=True,\n                 input_dim=None,\n                 **kwargs):\n        warnings.warn('The `Highway` layer is deprecated '\n                      'and will be removed after 06/2017.')\n        if 'transform_bias' in kwargs:\n            kwargs.pop('transform_bias')\n            warnings.warn('`transform_bias` argument is deprecated and '\n                          'has been removed.')\n        self.init = initializers.get(init)\n        self.activation = activations.get(activation)\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.initial_weights = weights\n        self.input_spec = InputSpec(ndim=2)\n\n        self.input_dim = input_dim\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_dim,)\n        super(Highway, self).__init__(**kwargs)",
        "begin_line": 187,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.Highway.build#224",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.Highway",
        "signature": "keras.legacy.layers.Highway.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        input_dim = input_shape[1]\n        self.input_spec = InputSpec(dtype=K.floatx(),\n                                    shape=(None, input_dim))\n\n        self.W = self.add_weight((input_dim, input_dim),\n                                 initializer=self.init,\n                                 name='W',\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.W_carry = self.add_weight((input_dim, input_dim),\n                                       initializer=self.init,\n                                       name='W_carry')\n        if self.bias:\n            self.b = self.add_weight((input_dim,),\n                                     initializer='zero',\n                                     name='b',\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n            self.b_carry = self.add_weight((input_dim,),\n                                           initializer='one',\n                                           name='b_carry')\n        else:\n            self.b_carry = None\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True",
        "begin_line": 224,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.Highway.call#254",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.Highway",
        "signature": "keras.legacy.layers.Highway.call(self, x)",
        "snippet": "    def call(self, x):\n        y = K.dot(x, self.W_carry)\n        if self.bias:\n            y += self.b_carry\n        transform_weight = activations.sigmoid(y)\n        y = K.dot(x, self.W)\n        if self.bias:\n            y += self.b\n        act = self.activation(y)\n        act *= transform_weight\n        output = act + (1 - transform_weight) * x\n        return output",
        "begin_line": 254,
        "end_line": 265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.Highway.get_config#267",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers.Highway",
        "signature": "keras.legacy.layers.Highway.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'init': initializers.serialize(self.init),\n                  'activation': activations.serialize(self.activation),\n                  'W_regularizer': regularizers.serialize(self.W_regularizer),\n                  'b_regularizer': regularizers.serialize(self.b_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'W_constraint': constraints.serialize(self.W_constraint),\n                  'b_constraint': constraints.serialize(self.b_constraint),\n                  'bias': self.bias,\n                  'input_dim': self.input_dim}\n        base_config = super(Highway, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 267,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.AtrousConvolution1D#281",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers",
        "signature": "keras.legacy.layers.AtrousConvolution1D(*args, **kwargs)",
        "snippet": "def AtrousConvolution1D(*args, **kwargs):\n    from ..layers import Conv1D\n    if 'atrous_rate' in kwargs:\n        rate = kwargs.pop('atrous_rate')\n    else:\n        rate = 1\n    kwargs['dilation_rate'] = rate\n    warnings.warn('The `AtrousConvolution1D` layer '\n                  ' has been deprecated. Use instead '\n                  'the `Conv1D` layer with the `dilation_rate` '\n                  'argument.')\n    return Conv1D(*args, **kwargs)",
        "begin_line": 281,
        "end_line": 292,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.layers.AtrousConvolution2D#295",
        "src_path": "keras/legacy/layers.py",
        "class_name": "keras.legacy.layers",
        "signature": "keras.legacy.layers.AtrousConvolution2D(*args, **kwargs)",
        "snippet": "def AtrousConvolution2D(*args, **kwargs):\n    from ..layers import Conv2D\n    if 'atrous_rate' in kwargs:\n        rate = kwargs.pop('atrous_rate')\n    else:\n        rate = 1\n    kwargs['dilation_rate'] = rate\n    warnings.warn('The `AtrousConvolution2D` layer '\n                  ' has been deprecated. Use instead '\n                  'the `Conv2D` layer with the `dilation_rate` '\n                  'argument.')\n    return Conv2D(*args, **kwargs)",
        "begin_line": 295,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.__init__#87",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        # Signature detection\n        if (len(args) == 2 or\n            len(args) == 1 and 'outputs' in kwargs or\n                'inputs' in kwargs and 'outputs' in kwargs):\n            # Graph network\n            self._init_graph_network(*args, **kwargs)\n        else:\n            # Subclassed network\n            self._init_subclassed_network(**kwargs)",
        "begin_line": 87,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017574692442882249,
            "pseudo_dstar_susp": 0.002304147465437788,
            "pseudo_tarantula_susp": 0.0006506180871828237,
            "pseudo_op2_susp": 0.002304147465437788,
            "pseudo_barinel_susp": 0.000649772579597141
        }
    },
    {
        "name": "keras.engine.network.Network._base_init#98",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._base_init(self, name=None)",
        "snippet": "    def _base_init(self, name=None):\n        # The following are implemented as property functions:\n        # self.trainable_weights\n        # self.non_trainable_weights\n        # self.input_spec\n        # self.losses\n        # self.updates\n\n        # Handle `name` argument.\n        if not name:\n            prefix = self.__class__.__name__.lower()\n            name = prefix + '_' + str(K.get_uid(prefix))\n        self.name = name\n\n        # This acts just like the `trainable` attribute of any layer instance.\n        # It does not affect users of the underlying layers, only users of the\n        # Network instance.\n        self.trainable = True\n        self._is_compiled = False\n        self._expects_training_arg = False\n        self._initial_weights = None\n\n        self.supports_masking = False\n        if not hasattr(self, 'optimizer'):\n            # Don't reset optimizer if already set.\n            self.optimizer = None\n\n        # Private attributes to implement compatibility with Layer.\n        self._updates = []\n        self._losses = []\n        self._per_input_losses = {}\n        self._per_input_updates = {}\n\n        # All layers in order of horizontal graph traversal.\n        # Entries are unique. Includes input and output layers.\n        self._layers = []\n\n        # Used only in conjunction with graph-networks\n        self._outbound_nodes = []\n        self._inbound_nodes = []",
        "begin_line": 98,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007936507936507937,
            "pseudo_dstar_susp": 0.002304147465437788,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.002304147465437788,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.engine.network.Network._init_graph_network#139",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._init_graph_network(self, inputs, outputs, name=None)",
        "snippet": "    def _init_graph_network(self, inputs, outputs, name=None):\n        self._uses_inputs_arg = True\n        # Normalize and set self.inputs, self.outputs.\n        if isinstance(inputs, (list, tuple)):\n            self.inputs = list(inputs)  # Tensor or list of tensors.\n        else:\n            self.inputs = [inputs]\n        if isinstance(outputs, (list, tuple)):\n            self.outputs = list(outputs)\n        else:\n            self.outputs = [outputs]\n\n        # User-provided argument validation.\n        # Check for redundancy in inputs.\n        if len(set(self.inputs)) != len(self.inputs):\n            raise ValueError('The list of inputs passed to the model '\n                             'is redundant. '\n                             'All inputs should only appear once.'\n                             ' Found: ' + str(self.inputs))\n        for x in self.inputs:\n            # Check that x has appropriate `_keras_history` metadata.\n            if not hasattr(x, '_keras_history'):\n                cls_name = self.__class__.__name__\n                raise ValueError('Input tensors to a ' + cls_name + ' ' +\n                                 'must come from `keras.layers.Input`. '\n                                 'Received: ' + str(x) +\n                                 ' (missing previous layer metadata).')\n            # Check that x is an input tensor.\n            layer, node_index, tensor_index = x._keras_history\n            if (len(layer._inbound_nodes) > 1 or\n                    (layer._inbound_nodes and\n                     layer._inbound_nodes[0].inbound_layers)):\n                cls_name = self.__class__.__name__\n                warnings.warn(cls_name + ' inputs must come from '\n                              '`keras.layers.Input` '\n                              '(thus holding past layer metadata), '\n                              'they cannot be the output of '\n                              'a previous non-Input layer. '\n                              'Here, a tensor specified as '\n                              'input to your model '\n                              'was not an Input tensor, '\n                              'it was generated by layer ' +\n                              layer.name + '.\\n'\n                              'Note that input tensors are '\n                              'instantiated via '\n                              '`tensor = keras.layers.Input(shape)`.\\n'\n                              'The tensor that caused the issue was: ' +\n                              str(x.name))\n        for x in self.outputs:\n            if not hasattr(x, '_keras_history'):\n                cls_name = self.__class__.__name__\n                raise ValueError('Output tensors to a ' + cls_name +\n                                 ' must be '\n                                 'the output of a Keras `Layer` '\n                                 '(thus holding past layer metadata). '\n                                 'Found: ' + str(x))\n        self._base_init(name=name)\n        self._compute_previous_mask = (\n            has_arg(self.call, 'mask') or\n            hasattr(self, 'compute_mask'))\n        # A Network does not create weights of its own,\n        # thus it is already built.\n        self.built = True\n        self._is_graph_network = True\n\n        self._input_layers = []\n        self._output_layers = []\n        self._input_coordinates = []\n        self._output_coordinates = []\n\n        # This is for performance optimization when calling the Network on new\n        # inputs. Every time the Network is called on a set on input tensors,\n        # we compute the output tensors,\n        # output masks and output shapes in one pass,\n        # then cache them here. When any of these outputs is queried later, we\n        # retrieve it from there instead of recomputing it.\n        self._output_mask_cache = {}\n        self._output_tensor_cache = {}\n        self._output_shape_cache = {}\n\n        # Build self._output_layers:\n        for x in self.outputs:\n            layer, node_index, tensor_index = x._keras_history\n            self._output_layers.append(layer)\n            self._output_coordinates.append((layer, node_index, tensor_index))\n\n        # Build self._input_layers:\n        for x in self.inputs:\n            layer, node_index, tensor_index = x._keras_history\n            # It's supposed to be an input layer, so only one node\n            # and one tensor output.\n            assert node_index == 0\n            assert tensor_index == 0\n            self._input_layers.append(layer)\n            self._input_coordinates.append((layer, node_index, tensor_index))\n\n        # Keep track of the network's nodes and layers.\n        nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n            self.inputs, self.outputs)\n        self._network_nodes = nodes\n        self._nodes_by_depth = nodes_by_depth\n        self._layers = layers\n        self._layers_by_depth = layers_by_depth\n\n        # Create the node linking internal inputs to internal outputs.\n        Node(outbound_layer=self,\n             inbound_layers=[],\n             node_indices=[],\n             tensor_indices=[],\n             input_tensors=self.inputs,\n             output_tensors=self.outputs,\n             # No network-level masking for now.\n             input_masks=[None for _ in self.inputs],\n             output_masks=[None for _ in self.outputs],\n             input_shapes=[x._keras_shape for x in self.inputs],\n             output_shapes=[x._keras_shape for x in self.outputs])\n\n        # Fill in the output mask cache.\n        masks = []\n        for x in self.inputs:\n            layer, node_index, tensor_index = x._keras_history\n            node = layer._inbound_nodes[node_index]\n            mask = node.output_masks[tensor_index]\n            masks.append(mask)\n        mask_cache_key = object_list_uid(inputs)\n        mask_cache_key += '_' + object_list_uid(masks)\n        masks = []\n        for x in self.outputs:\n            layer, node_index, tensor_index = x._keras_history\n            node = layer._inbound_nodes[node_index]\n            mask = node.output_masks[tensor_index]\n            masks.append(mask)\n        mask = unpack_singleton(masks)\n        self._output_mask_cache[mask_cache_key] = mask\n\n        # Build self.input_names and self.output_names.\n        self.input_names = []\n        self.output_names = []\n        self._feed_input_names = []\n        self._feed_inputs = []\n        self._feed_input_shapes = []\n        for i, layer in enumerate(self._input_layers):\n            # Check that layer is an InputLayer.\n            if not isinstance(layer, InputLayer):\n                raise TypeError(\n                    'Input layers to a `Model` must be `InputLayer` objects. '\n                    'Received inputs: {}. '\n                    'Input {} (0-based) originates '\n                    'from layer type `{}`.'.format(inputs,\n                                                   i,\n                                                   layer.__class__.__name__))\n            self.input_names.append(layer.name)\n            if layer.is_placeholder:\n                self._feed_inputs.append(layer.input)\n                self._feed_input_names.append(layer.name)\n                self._feed_input_shapes.append(self.inputs[i]._keras_shape)\n\n        for layer in self._output_layers:\n            self.output_names.append(layer.name)",
        "begin_line": 139,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.2,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.2,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.network.Network._init_subclassed_network#299",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._init_subclassed_network(self, name=None)",
        "snippet": "    def _init_subclassed_network(self, name=None):\n        self._base_init(name=name)\n        self._is_graph_network = False\n        self._expects_training_arg = has_arg(self.call, 'training')\n        self._uses_inputs_arg = has_arg(self.call, 'inputs')\n        self.outputs = None\n        self.inputs = None\n        self.built = False",
        "begin_line": 299,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017574692442882249,
            "pseudo_dstar_susp": 0.00129366106080207,
            "pseudo_tarantula_susp": 0.0006506180871828237,
            "pseudo_op2_susp": 0.00129366106080207,
            "pseudo_barinel_susp": 0.000649772579597141
        }
    },
    {
        "name": "keras.engine.network.Network.__setattr__#308",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        # Automatically track layers set as Model\n        # attributes for subclassed Models.\n        if isinstance(value, (Layer, Network)):\n            try:\n                is_graph_network = self._is_graph_network\n            except AttributeError:\n                raise RuntimeError(\n                    'It looks like you are subclassing `Model` and you '\n                    'forgot to call `super(YourClass, self).__init__()`.'\n                    ' Always start with this line.')\n            if not is_graph_network:\n                if value not in self._layers:\n                    self._layers.append(value)\n        super(Network, self).__setattr__(name, value)",
        "begin_line": 308,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007936507936507937,
            "pseudo_dstar_susp": 0.002304147465437788,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.002304147465437788,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.engine.network.Network.layers#325",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.layers(self)",
        "snippet": "    def layers(self):\n        return self._layers",
        "begin_line": 325,
        "end_line": 326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002608242044861763,
            "pseudo_dstar_susp": 0.0002608242044861763,
            "pseudo_tarantula_susp": 0.00026102845210127906,
            "pseudo_op2_susp": 0.0002608242044861763,
            "pseudo_barinel_susp": 0.00026102845210127906
        }
    },
    {
        "name": "keras.engine.network.Network.get_layer#328",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.get_layer(self, name=None, index=None)",
        "snippet": "    def get_layer(self, name=None, index=None):\n        \"\"\"Retrieves a layer based on either its name (unique) or index.\n\n        If `name` and `index` are both provided, `index` will take precedence.\n\n        Indices are based on order of horizontal graph traversal (bottom-up).\n\n        # Arguments\n            name: String, name of layer.\n            index: Integer, index of layer.\n\n        # Returns\n            A layer instance.\n\n        # Raises\n            ValueError: In case of invalid layer name or index.\n        \"\"\"\n        # It would be unreliable to build a dictionary\n        # based on layer names, because names can potentially\n        # be changed at any point by the user\n        # without the network being notified of it.\n        if index is not None:\n            if len(self.layers) <= index:\n                raise ValueError('Was asked to retrieve layer at index ' +\n                                 str(index) + ' but model only has ' +\n                                 str(len(self.layers)) + ' layers.')\n            else:\n                return self.layers[index]\n        else:\n            if not name:\n                raise ValueError('Provide either a layer name or layer index.')\n\n        for layer in self.layers:\n            if layer.name == name:\n                return layer\n\n        raise ValueError('No such layer: ' + name)",
        "begin_line": 328,
        "end_line": 364,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.updates#367",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.updates(self)",
        "snippet": "    def updates(self):\n        \"\"\"Retrieves the model's updates.\n\n        Will only include updates that are either\n        unconditional, or conditional on inputs to this model\n        (e.g. will not include updates that depend on tensors\n        that aren't inputs to this model).\n\n        # Returns\n            A list of update ops.\n        \"\"\"\n        if not self.trainable and not self.stateful:\n            return []\n        updates = []\n        for layer in self.layers:\n            if hasattr(layer, 'updates'):\n                if self._is_graph_network:\n                    # Collect updates that are dependent on inputs\n                    # that are part of the model.\n                    for node_index, node in enumerate(layer._inbound_nodes):\n                        node_key = self._node_key(layer, node_index)\n                        if node_key in self._network_nodes:\n                            # The model owns this layer node.\n                            inputs = node.input_tensors\n                            updates += layer.get_updates_for(inputs)\n                    # Collect unconditional updates.\n                    updates += layer.get_updates_for(None)\n                else:\n                    updates += layer.updates\n        return updates",
        "begin_line": 367,
        "end_line": 396,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033311125916055963,
            "pseudo_dstar_susp": 0.0005777007510109763,
            "pseudo_tarantula_susp": 0.0003169572107765452,
            "pseudo_op2_susp": 0.0005777007510109763,
            "pseudo_barinel_susp": 0.0003169572107765452
        }
    },
    {
        "name": "keras.engine.network.Network.losses#399",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.losses(self)",
        "snippet": "    def losses(self):\n        \"\"\"Retrieves the model's losses.\n\n        Will only include losses that are either\n        unconditional, or conditional on inputs to this model\n        (e.g. will not include losses that depend on tensors\n        that aren't inputs to this model).\n\n        # Returns\n            A list of loss tensors.\n        \"\"\"\n        losses = []\n        for layer in self.layers:\n            if hasattr(layer, 'losses'):\n                if self._is_graph_network:\n                    # Collect losses that are dependent on inputs\n                    # that are part of the model.\n                    for node_index, node in enumerate(layer._inbound_nodes):\n                        node_key = self._node_key(layer, node_index)\n                        if node_key in self._network_nodes:\n                            # The model owns this layer node.\n                            inputs = node.input_tensors\n                            losses += layer.get_losses_for(inputs)\n                    # Collect unconditional losses.\n                    losses += layer.get_losses_for(None)\n                else:\n                    losses += layer.losses\n\n        # Add any potential unconditional model-level loss.\n        losses += self.get_losses_for(None)\n\n        unique_tensors = list(\n            set(x for x in losses if not isinstance(x, (float, int))))\n        non_tensors = [x for x in losses if isinstance(x, (float, int))]\n        return unique_tensors + non_tensors",
        "begin_line": 399,
        "end_line": 433,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017006802721088435,
            "pseudo_dstar_susp": 0.025,
            "pseudo_tarantula_susp": 0.0004962779156327543,
            "pseudo_op2_susp": 0.025,
            "pseudo_barinel_susp": 0.0004962779156327543
        }
    },
    {
        "name": "keras.engine.network.Network.uses_learning_phase#436",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.uses_learning_phase(self)",
        "snippet": "    def uses_learning_phase(self):\n        if not self.outputs:\n            return False\n        return any([x._uses_learning_phase for x in self.outputs])",
        "begin_line": 436,
        "end_line": 439,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003800836183960471,
            "pseudo_dstar_susp": 0.0012121212121212121,
            "pseudo_tarantula_susp": 0.00028686173264486515,
            "pseudo_op2_susp": 0.0012121212121212121,
            "pseudo_barinel_susp": 0.00028686173264486515
        }
    },
    {
        "name": "keras.engine.network.Network.stateful#442",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.stateful(self)",
        "snippet": "    def stateful(self):\n        return any([(hasattr(layer, 'stateful') and\n                    layer.stateful) for layer in self.layers])",
        "begin_line": 442,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003222687721559781,
            "pseudo_dstar_susp": 0.0008680555555555555,
            "pseudo_tarantula_susp": 0.0002770850651149903,
            "pseudo_op2_susp": 0.0008680555555555555,
            "pseudo_barinel_susp": 0.0002770850651149903
        }
    },
    {
        "name": "keras.engine.network.Network.reset_states#446",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.reset_states(self)",
        "snippet": "    def reset_states(self):\n        for layer in self.layers:\n            if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n                layer.reset_states()",
        "begin_line": 446,
        "end_line": 449,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.state_updates#452",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.state_updates(self)",
        "snippet": "    def state_updates(self):\n        \"\"\"Returns the `updates` from all layers that are stateful.\n\n        This is useful for separating training updates and\n        state updates, e.g. when we need to update a layer's internal state\n        during prediction.\n\n        # Returns\n            A list of update ops.\n        \"\"\"\n        state_updates = []\n        for layer in self.layers:\n            if layer.stateful:\n                state_updates += layer.updates\n        return state_updates",
        "begin_line": 452,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026041666666666666,
            "pseudo_dstar_susp": 0.00026041666666666666,
            "pseudo_tarantula_susp": 0.00026062027625749283,
            "pseudo_op2_susp": 0.00026041666666666666,
            "pseudo_barinel_susp": 0.00026062027625749283
        }
    },
    {
        "name": "keras.engine.network.Network.trainable_weights#469",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        if not self.trainable:\n            return []\n        weights = []\n        for layer in self.layers:\n            weights += layer.trainable_weights\n        return weights",
        "begin_line": 469,
        "end_line": 475,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004830917874396135,
            "pseudo_dstar_susp": 0.0012285012285012285,
            "pseudo_tarantula_susp": 0.00034818941504178273,
            "pseudo_op2_susp": 0.0012285012285012285,
            "pseudo_barinel_susp": 0.00034818941504178273
        }
    },
    {
        "name": "keras.engine.network.Network.non_trainable_weights#478",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.non_trainable_weights(self)",
        "snippet": "    def non_trainable_weights(self):\n        weights = []\n        for layer in self.layers:\n            weights += layer.non_trainable_weights\n        if not self.trainable:\n            trainable_weights = []\n            for layer in self.layers:\n                trainable_weights += layer.trainable_weights\n            return trainable_weights + weights\n        return weights",
        "begin_line": 478,
        "end_line": 487,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003795066413662239,
            "pseudo_dstar_susp": 0.0008006405124099279,
            "pseudo_tarantula_susp": 0.0003278688524590164,
            "pseudo_op2_susp": 0.0008006405124099279,
            "pseudo_barinel_susp": 0.0003278688524590164
        }
    },
    {
        "name": "keras.engine.network.Network.get_weights#489",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.get_weights(self)",
        "snippet": "    def get_weights(self):\n        \"\"\"Retrieves the weights of the model.\n\n        # Returns\n            A flat list of Numpy arrays.\n        \"\"\"\n        weights = []\n        for layer in self.layers:\n            weights += layer.weights\n        return K.batch_get_value(weights)",
        "begin_line": 489,
        "end_line": 498,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.805142203046579e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.set_weights#500",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.set_weights(self, weights)",
        "snippet": "    def set_weights(self, weights):\n        \"\"\"Sets the weights of the model.\n\n        # Arguments\n            weights: A list of Numpy arrays with shapes and types matching\n                the output of `model.get_weights()`.\n        \"\"\"\n        tuples = []\n        for layer in self.layers:\n            num_param = len(layer.weights)\n            layer_weights = weights[:num_param]\n            for sw, w in zip(layer.weights, layer_weights):\n                tuples.append((sw, w))\n            weights = weights[num_param:]\n        K.batch_set_value(tuples)",
        "begin_line": 500,
        "end_line": 514,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.819897689186806e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.input_spec#517",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.input_spec(self)",
        "snippet": "    def input_spec(self):\n        \"\"\"Gets the model's input specs.\n\n        # Returns\n            A list of `InputSpec` instances (one per input to the model)\n                or a single instance if the model has only one input.\n        \"\"\"\n        if not self._is_graph_network:\n            # TODO: support it in subclassed networks after inputs are set.\n            return None\n\n        specs = []\n        for layer in getattr(self, '_input_layers', []):\n            if layer.input_spec is None:\n                specs.append(None)\n            else:\n                if not isinstance(layer.input_spec, list):\n                    raise TypeError('Layer ' + layer.name +\n                                    ' has an input_spec attribute that '\n                                    'is not a list. We expect a list. '\n                                    'Found input_spec = ' +\n                                    str(layer.input_spec))\n                specs += layer.input_spec\n        return unpack_singleton(specs)",
        "begin_line": 517,
        "end_line": 540,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00041841004184100416,
            "pseudo_dstar_susp": 0.0003563791874554526,
            "pseudo_tarantula_susp": 0.0008525149190110827,
            "pseudo_op2_susp": 0.0003563791874554526,
            "pseudo_barinel_susp": 0.0008532423208191126
        }
    },
    {
        "name": "keras.engine.network.Network.call#542",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.call(self, inputs, mask=None)",
        "snippet": "    def call(self, inputs, mask=None):\n        \"\"\"Calls the model on new inputs.\n\n        In this case `call` just reapplies\n        all ops in the graph to the new inputs\n        (e.g. build a new computational graph from the provided inputs).\n\n        A model is callable on non-Keras tensors.\n\n        # Arguments\n            inputs: A tensor or list of tensors.\n            mask: A mask or list of masks. A mask can be\n                either a tensor or None (no mask).\n\n        # Returns\n            A tensor if there is a single output, or\n            a list of tensors if there are more than one outputs.\n        \"\"\"\n        inputs = to_list(inputs)\n        if mask is None:\n            masks = [None for _ in range(len(inputs))]\n        else:\n            masks = to_list(mask)\n        cache_key = object_list_uid(inputs)\n        cache_key += '_' + object_list_uid(masks)\n        if cache_key in self._output_tensor_cache:\n            return self._output_tensor_cache[cache_key]\n        else:\n            output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n            return output_tensors",
        "begin_line": 542,
        "end_line": 571,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0006472491909385113,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0006472491909385113,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.engine.network.Network.compute_mask#573",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.compute_mask(self, inputs, mask)",
        "snippet": "    def compute_mask(self, inputs, mask):\n        if not self._is_graph_network:\n            return None\n\n        inputs = to_list(inputs)\n        if mask is None:\n            masks = [None for _ in range(len(inputs))]\n        else:\n            masks = to_list(mask)\n        cache_key = object_list_uid(inputs)\n        cache_key += '_' + object_list_uid(masks)\n        if cache_key in self._output_mask_cache:\n            return self._output_mask_cache[cache_key]\n        else:\n            _, output_masks, _ = self.run_internal_graph(inputs, masks)\n            return output_masks",
        "begin_line": 573,
        "end_line": 588,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.023255813953488372,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.023255813953488372,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.network.Network.compute_output_shape#590",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if not self._is_graph_network:\n            # Must be implemented by subclasses.\n            raise NotImplementedError\n\n        input_shapes = to_list(input_shape)\n        if len(input_shapes) != len(self._input_layers):\n            raise ValueError('Invalid input_shape argument ' +\n                             str(input_shape) + ': model has ' +\n                             str(len(self._input_layers)) + ' tensor inputs.')\n\n        cache_key = ', '.join([str(x) for x in input_shapes])\n        if cache_key in self._output_shape_cache:\n            output_shapes = self._output_shape_cache[cache_key]\n            if isinstance(output_shapes, list):\n                return unpack_singleton(output_shapes)\n            return output_shapes\n        else:\n            # Bad luck, we have to run the graph manually.\n            layers_to_output_shapes = {}\n            for i in range(len(input_shapes)):\n                layer = self._input_layers[i]\n                input_shape = input_shapes[i]\n                # It's an input layer: compute_output_shape is identity,\n                # and there is only one node and one tensor output.\n                shape_key = layer.name + '_0_0'\n                layers_to_output_shapes[shape_key] = input_shape\n\n            depth_keys = list(self._nodes_by_depth.keys())\n            depth_keys.sort(reverse=True)\n            # Iterate over nodes, by depth level.\n            if len(depth_keys) > 1:\n                for depth in depth_keys:\n                    nodes = self._nodes_by_depth[depth]\n                    for node in nodes:\n                        # This is always a single layer, never a list.\n                        layer = node.outbound_layer\n                        if layer in self._input_layers:\n                            # We've already covered the input layers\n                            # a few lines above.\n                            continue\n                        # Potentially redundant list,\n                        # same size of node.input_tensors.\n                        input_shapes = []\n                        for j in range(len(node.inbound_layers)):\n                            inbound_layer = node.inbound_layers[j]\n                            node_index = node.node_indices[j]\n                            tensor_index = node.tensor_indices[j]\n                            shape_key = inbound_layer.name + '_%s_%s' % (node_index, tensor_index)\n                            input_shape = layers_to_output_shapes[shape_key]\n                            input_shapes.append(input_shape)\n\n                        output_shape = layer.compute_output_shape(unpack_singleton(input_shapes))\n\n                        output_shapes = to_list(output_shape)\n                        node_index = layer._inbound_nodes.index(node)\n                        for j in range(len(output_shapes)):\n                            shape_key = layer.name + '_%s_%s' % (node_index, j)\n                            layers_to_output_shapes[shape_key] = output_shapes[j]\n\n            # Read final output shapes from layers_to_output_shapes.\n            output_shapes = []\n            output_shape_keys = []\n            for i in range(len(self._output_layers)):\n                layer = self._output_layers[i]\n                node_index = self._output_coordinates[i][1]\n                tensor_index = self._output_coordinates[i][2]\n                shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)\n                output_shape_keys.append(shape_key)\n\n            for i, key in enumerate(output_shape_keys):\n                assert key in layers_to_output_shapes\n                output_shapes.append(layers_to_output_shapes[key])\n            # Store in cache.\n            self._output_shape_cache[cache_key] = output_shapes\n            if isinstance(output_shapes, list):\n                return unpack_singleton(output_shapes)\n            return output_shapes",
        "begin_line": 590,
        "end_line": 667,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0006406149903907751,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0006406149903907751,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.network.Network.run_internal_graph#669",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.run_internal_graph(self, inputs, masks=None)",
        "snippet": "    def run_internal_graph(self, inputs, masks=None):\n        \"\"\"Computes output tensors for new inputs.\n\n        # Note:\n            - Expects `inputs` to be a list (potentially with 1 element).\n            - Can be run on non-Keras tensors.\n\n        # Arguments\n            inputs: List of tensors\n            masks: List of masks (tensors or None).\n\n        # Returns\n            Three lists: output_tensors, output_masks, output_shapes\n        \"\"\"\n        if masks is None:\n            masks = [None for _ in range(len(inputs))]\n\n        # Dictionary mapping reference tensors to tuples\n        # (computed tensor, compute mask)\n        # we assume a 1:1 mapping from tensor to mask\n        # TODO: raise exception when a `.compute_mask()` call\n        # does not return a list the same size as `call`\n        tensor_map = {}\n        for x, y, mask in zip(self.inputs, inputs, masks):\n            tensor_map[str(id(x))] = (y, mask)\n\n        depth_keys = list(self._nodes_by_depth.keys())\n        depth_keys.sort(reverse=True)\n        for depth in depth_keys:\n            nodes = self._nodes_by_depth[depth]\n            for node in nodes:\n                # This is always a single layer, never a list.\n                layer = node.outbound_layer\n                reference_input_tensors = node.input_tensors\n                reference_output_tensors = node.output_tensors\n\n                # If all previous input tensors are available in tensor_map,\n                # then call node.inbound_layer on them.\n                computed_data = []  # List of tuples (input, mask).\n                for x in reference_input_tensors:\n                    if str(id(x)) in tensor_map:\n                        computed_data.append(tensor_map[str(id(x))])\n\n                if len(computed_data) == len(reference_input_tensors):\n                    # call layer\n                    with K.name_scope(layer.name):\n                        if node.arguments:\n                            kwargs = node.arguments\n                        else:\n                            kwargs = {}\n                        if len(computed_data) == 1:\n                            computed_tensor, computed_mask = computed_data[0]\n                            if has_arg(layer.call, 'mask'):\n                                if 'mask' not in kwargs:\n                                    kwargs['mask'] = computed_mask\n                            output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n                            output_masks = layer.compute_mask(computed_tensor,\n                                                              computed_mask)\n                            if output_masks is None:\n                                output_masks = [None for _ in output_tensors]\n                            else:\n                                output_masks = to_list(output_masks)\n                            computed_tensors = [computed_tensor]\n\n                            # computed_masks might be used in the future.\n                            computed_masks = [computed_mask]\n                        else:\n                            computed_tensors = [x[0] for x in computed_data]\n                            computed_masks = [x[1] for x in computed_data]\n                            if has_arg(layer.call, 'mask'):\n                                if 'mask' not in kwargs:\n                                    kwargs['mask'] = computed_masks\n                            output_tensors = to_list(\n                                layer.call(computed_tensors, **kwargs))\n                            output_masks = layer.compute_mask(computed_tensors,\n                                                              computed_masks)\n                            if output_masks is None:\n                                output_masks = [None for _ in output_tensors]\n                            else:\n                                output_masks = to_list(output_masks)\n                        # Apply activity regularizer if any:\n                        if hasattr(layer, 'activity_regularizer') and layer.activity_regularizer is not None:\n                            with K.name_scope('activity_regularizer'):\n                                regularization_losses = [\n                                    layer.activity_regularizer(x)\n                                    for x in output_tensors]\n                            layer.add_loss(regularization_losses,\n                                           inputs=computed_tensors)\n\n                        if len(output_masks) != len(output_tensors):\n                            raise Exception(\n                                'Layers should have equal number of output tensors '\n                                'and output masks. Layer ' + str(layer.name) + ' has'\n                                ' ' + str(len(output_tensors)) + ' output tensors and'\n                                ' ' + str(len(output_masks)) + ' output masks.')\n                    # Update model updates and losses:\n                    # Keep track of updates that depend on the inputs\n                    # (e.g. BN updates).\n                    self.add_update(layer.get_updates_for(computed_tensors), inputs)\n                    # Keep track of unconditional updates (e.g. a counter).\n                    self.add_update(layer.get_updates_for(None), None)\n                    # Keep track of losses that depend on the inputs\n                    # (e.g. activity regularizers).\n                    self.add_loss(layer.get_losses_for(computed_tensors), inputs)\n                    # Keep track of unconditional losses\n                    # (e.g. weight regularizers).\n                    self.add_loss(layer.get_losses_for(None), None)\n\n                    # Update _keras_shape.\n                    if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n                        input_shapes = unpack_singleton([x._keras_shape for x in computed_tensors])\n                        shapes = to_list(layer.compute_output_shape(input_shapes))\n                        uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n\n                        for x, s in zip(output_tensors, shapes):\n                            x._keras_shape = s\n                            x._uses_learning_phase = getattr(x, '_uses_learning_phase', False) or uses_learning_phase\n\n                    # Update tensor_map.\n                    for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):\n                        tensor_map[str(id(x))] = (y, mask)\n\n        output_tensors = []\n        output_masks = []\n        output_shapes = []\n        for x in self.outputs:\n            assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\n            tensor, mask = tensor_map[str(id(x))]\n            if hasattr(tensor, '_keras_shape') and output_shapes is not None:\n                shape = tensor._keras_shape\n                output_shapes.append(shape)\n            else:\n                output_shapes = None\n            output_tensors.append(tensor)\n            output_masks.append(mask)\n\n        # Update cache;\n        # keys are based on ids on input tensors and inputs masks.\n        cache_key = object_list_uid(inputs)\n        cache_key += '_' + object_list_uid(masks)\n\n        output_tensors = unpack_singleton(output_tensors)\n        self._output_tensor_cache[cache_key] = output_tensors\n\n        output_masks = unpack_singleton(output_masks)\n        self._output_mask_cache[cache_key] = output_masks\n\n        if output_shapes is not None:\n            input_shapes = [x._keras_shape for x in inputs]\n            cache_key = ', '.join([str(x) for x in input_shapes])\n\n            output_shapes = unpack_singleton(output_shapes)\n            self._output_shape_cache[cache_key] = output_shapes\n        return output_tensors, output_masks, output_shapes",
        "begin_line": 669,
        "end_line": 822,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.network.Network.get_config#824",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.get_config(self)",
        "snippet": "    def get_config(self):\n        if not self._is_graph_network:\n            # Subclassed networks are not serializable\n            # (unless serialization is implemented by\n            # the author of the subclassed network).\n            raise NotImplementedError\n\n        config = {\n            'name': self.name,\n        }\n\n        # Build a map from a layer unique name (self._node_key)\n        # to the index of the nodes that are saved in the config.\n        # Only nodes in network_nodes are saved.\n        node_conversion_map = {}\n        for layer in self.layers:\n            if issubclass(layer.__class__, Network):\n                # Networks start with a pre-existing node\n                # linking their input to output.\n                kept_nodes = 1\n            else:\n                kept_nodes = 0\n            for original_node_index, node in enumerate(layer._inbound_nodes):\n                node_key = self._node_key(layer, original_node_index)\n                if node_key in self._network_nodes:\n                    # i.e. we mark it to be saved\n                    node_conversion_map[node_key] = kept_nodes\n                    kept_nodes += 1\n\n        # serialize and save the layers in layer_configs\n        layer_configs = []\n        for layer in self.layers:  # From the earliest layers on.\n            layer_class_name = layer.__class__.__name__\n            layer_config = layer.get_config()\n            filtered_inbound_nodes = []\n            for original_node_index, node in enumerate(layer._inbound_nodes):\n                node_key = self._node_key(layer, original_node_index)\n                if node_key in self._network_nodes:\n                    # The node is relevant to the model:\n                    # add to filtered_inbound_nodes.\n                    if node.arguments:\n                        try:\n                            json.dumps(node.arguments)\n                            kwargs = node.arguments\n                        except TypeError:\n                            warnings.warn(\n                                'Layer ' + layer.name +\n                                ' was passed non-serializable '\n                                'keyword arguments: ' +\n                                str(node.arguments) +\n                                '. They will not be included '\n                                'in the serialized model '\n                                '(and thus will be missing '\n                                'at deserialization time).')\n                            kwargs = {}\n                    else:\n                        kwargs = {}\n                    if node.inbound_layers:\n                        node_data = []\n                        for i in range(len(node.inbound_layers)):\n                            inbound_layer = node.inbound_layers[i]\n                            node_index = node.node_indices[i]\n                            tensor_index = node.tensor_indices[i]\n\n                            new_node_index = node_conversion_map.get(\n                                self._node_key(inbound_layer, node_index), 0)\n                            node_data.append([inbound_layer.name,\n                                              new_node_index,\n                                              tensor_index,\n                                              kwargs])\n                        filtered_inbound_nodes.append(node_data)\n            layer_configs.append({\n                'name': layer.name,\n                'class_name': layer_class_name,\n                'config': layer_config,\n                'inbound_nodes': filtered_inbound_nodes,\n            })\n        config['layers'] = layer_configs\n\n        # Gather info about inputs and outputs.\n        model_inputs = []\n        for i in range(len(self._input_layers)):\n            layer = self._input_layers[i]\n            node_index = self._input_coordinates[i][1]\n\n            node_key = self._node_key(layer, node_index)\n            if node_key not in self._network_nodes:\n                continue\n            new_node_index = node_conversion_map[node_key]\n            tensor_index = self._input_coordinates[i][2]\n            model_inputs.append([layer.name, new_node_index, tensor_index])\n        config['input_layers'] = model_inputs\n        model_outputs = []\n        for i in range(len(self._output_layers)):\n            layer = self._output_layers[i]\n            node_index = self._output_coordinates[i][1]\n\n            node_key = self._node_key(layer, node_index)\n            if node_key not in self._network_nodes:\n                continue\n            new_node_index = node_conversion_map[node_key]\n            tensor_index = self._output_coordinates[i][2]\n            model_outputs.append([layer.name, new_node_index, tensor_index])\n        config['output_layers'] = model_outputs\n        return copy.deepcopy(config)",
        "begin_line": 824,
        "end_line": 928,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.engine.network.Network.from_config#931",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        \"\"\"Instantiates a Model from its config (output of `get_config()`).\n\n        # Arguments\n            config: Model config dictionary.\n            custom_objects: Optional dictionary mapping names\n                (strings) to custom classes or functions to be\n                considered during deserialization.\n\n        # Returns\n            A model instance.\n\n        # Raises\n            ValueError: In case of improperly formatted config dict.\n        \"\"\"\n        # Layer instances created during\n        # the graph reconstruction process\n        created_layers = {}\n\n        # Dictionary mapping layer instances to\n        # node data that specifies a layer call.\n        # It acts as a queue that maintains any unprocessed\n        # layer call until it becomes possible to process it\n        # (i.e. until the input tensors to the call all exist).\n        unprocessed_nodes = {}\n\n        def add_unprocessed_node(layer, node_data):\n            if layer not in unprocessed_nodes:\n                unprocessed_nodes[layer] = [node_data]\n            else:\n                unprocessed_nodes[layer].append(node_data)\n\n        def process_node(layer, node_data):\n            input_tensors = []\n            for input_data in node_data:\n                inbound_layer_name = input_data[0]\n                inbound_node_index = input_data[1]\n                inbound_tensor_index = input_data[2]\n                if len(input_data) == 3:\n                    kwargs = {}\n                elif len(input_data) == 4:\n                    kwargs = input_data[3]\n                else:\n                    raise ValueError('Improperly formatted model config.')\n                if inbound_layer_name not in created_layers:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                inbound_layer = created_layers[inbound_layer_name]\n                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                input_tensors.append(\n                    inbound_node.output_tensors[inbound_tensor_index])\n            # Call layer on its inputs, thus creating the node\n            # and building the layer if needed.\n            if input_tensors:\n                layer(unpack_singleton(input_tensors), **kwargs)\n\n        def process_layer(layer_data):\n            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n\n            # Arguments\n                layer_data: layer config dict.\n\n            # Raises\n                ValueError: In case of improperly formatted `layer_data` dict.\n            \"\"\"\n            layer_name = layer_data['name']\n\n            # Instantiate layer.\n            from ..layers import deserialize as deserialize_layer\n\n            layer = deserialize_layer(layer_data,\n                                      custom_objects=custom_objects)\n            created_layers[layer_name] = layer\n\n            # Gather layer inputs.\n            inbound_nodes_data = layer_data['inbound_nodes']\n            for node_data in inbound_nodes_data:\n                # We don't process nodes (i.e. make layer calls)\n                # on the fly because the inbound node may not yet exist,\n                # in case of layer shared at different topological depths\n                # (e.g. a model such as A(B(A(B(x)))))\n                add_unprocessed_node(layer, node_data)\n\n        # First, we create all layers and enqueue nodes to be processed\n        for layer_data in config['layers']:\n            process_layer(layer_data)\n        # Then we process nodes in order of layer depth.\n        # Nodes that cannot yet be processed (if the inbound node\n        # does not yet exist) are re-enqueued, and the process\n        # is repeated until all nodes are processed.\n        while unprocessed_nodes:\n            for layer_data in config['layers']:\n                layer = created_layers[layer_data['name']]\n                if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n\n        name = config.get('name')\n        input_tensors = []\n        output_tensors = []\n        for layer_data in config['input_layers']:\n            layer_name, node_index, tensor_index = layer_data\n            assert layer_name in created_layers\n            layer = created_layers[layer_name]\n            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n            input_tensors.append(layer_output_tensors[tensor_index])\n        for layer_data in config['output_layers']:\n            layer_name, node_index, tensor_index = layer_data\n            assert layer_name in created_layers\n            layer = created_layers[layer_name]\n            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n            output_tensors.append(layer_output_tensors[tensor_index])\n        return cls(inputs=input_tensors, outputs=output_tensors, name=name)",
        "begin_line": 931,
        "end_line": 1046,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026968716289104636,
            "pseudo_dstar_susp": 0.00026968716289104636,
            "pseudo_tarantula_susp": 0.0002699784017278618,
            "pseudo_op2_susp": 0.00026968716289104636,
            "pseudo_barinel_susp": 0.0002699784017278618
        }
    },
    {
        "name": "keras.engine.network.Network.add_unprocessed_node#957",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.add_unprocessed_node(layer, node_data)",
        "snippet": "        def add_unprocessed_node(layer, node_data):\n            if layer not in unprocessed_nodes:\n                unprocessed_nodes[layer] = [node_data]\n            else:\n                unprocessed_nodes[layer].append(node_data)",
        "begin_line": 957,
        "end_line": 961,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0005260389268805891,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0005260389268805891,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.network.Network.process_node#963",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.process_node(layer, node_data)",
        "snippet": "        def process_node(layer, node_data):\n            input_tensors = []\n            for input_data in node_data:\n                inbound_layer_name = input_data[0]\n                inbound_node_index = input_data[1]\n                inbound_tensor_index = input_data[2]\n                if len(input_data) == 3:\n                    kwargs = {}\n                elif len(input_data) == 4:\n                    kwargs = input_data[3]\n                else:\n                    raise ValueError('Improperly formatted model config.')\n                if inbound_layer_name not in created_layers:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                inbound_layer = created_layers[inbound_layer_name]\n                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                input_tensors.append(\n                    inbound_node.output_tensors[inbound_tensor_index])\n            # Call layer on its inputs, thus creating the node\n            # and building the layer if needed.\n            if input_tensors:\n                layer(unpack_singleton(input_tensors), **kwargs)",
        "begin_line": 963,
        "end_line": 988,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002843332385555871,
            "pseudo_dstar_susp": 0.0005260389268805891,
            "pseudo_tarantula_susp": 0.0002699784017278618,
            "pseudo_op2_susp": 0.0005260389268805891,
            "pseudo_barinel_susp": 0.0002699784017278618
        }
    },
    {
        "name": "keras.engine.network.Network.process_layer#990",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.process_layer(layer_data)",
        "snippet": "        def process_layer(layer_data):\n            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n\n            # Arguments\n                layer_data: layer config dict.\n\n            # Raises\n                ValueError: In case of improperly formatted `layer_data` dict.\n            \"\"\"\n            layer_name = layer_data['name']\n\n            # Instantiate layer.\n            from ..layers import deserialize as deserialize_layer\n\n            layer = deserialize_layer(layer_data,\n                                      custom_objects=custom_objects)\n            created_layers[layer_name] = layer\n\n            # Gather layer inputs.\n            inbound_nodes_data = layer_data['inbound_nodes']\n            for node_data in inbound_nodes_data:\n                # We don't process nodes (i.e. make layer calls)\n                # on the fly because the inbound node may not yet exist,\n                # in case of layer shared at different topological depths\n                # (e.g. a model such as A(B(A(B(x)))))\n                add_unprocessed_node(layer, node_data)",
        "begin_line": 990,
        "end_line": 1015,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002843332385555871,
            "pseudo_dstar_susp": 0.0005260389268805891,
            "pseudo_tarantula_susp": 0.0002699784017278618,
            "pseudo_op2_susp": 0.0005260389268805891,
            "pseudo_barinel_susp": 0.0002699784017278618
        }
    },
    {
        "name": "keras.engine.network.Network.save#1048",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.save(self, filepath, overwrite=True, include_optimizer=True)",
        "snippet": "    def save(self, filepath, overwrite=True, include_optimizer=True):\n        \"\"\"Saves the model to a single HDF5 file.\n\n        The savefile includes:\n            - The model architecture, allowing to re-instantiate the model.\n            - The model weights.\n            - The state of the optimizer, allowing to resume training\n                exactly where you left off.\n\n        This allows you to save the entirety of the state of a model\n        in a single file.\n\n        Saved models can be reinstantiated via `keras.models.load_model`.\n        The model returned by `load_model`\n        is a compiled model ready to be used (unless the saved model\n        was never compiled in the first place).\n\n        # Arguments\n            filepath: String, path to the file to save the weights to.\n            overwrite: Whether to silently overwrite any existing file at the\n                target location, or provide the user with a manual prompt.\n            include_optimizer: If True, save optimizer's state together.\n\n        # Example\n\n        ```python\n        from keras.models import load_model\n\n        model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n        del model  # deletes the existing model\n\n        # returns a compiled model\n        # identical to the previous one\n        model = load_model('my_model.h5')\n        ```\n        \"\"\"\n        if not self._is_graph_network:\n            raise NotImplementedError\n        from ..models import save_model\n        save_model(self, filepath, overwrite, include_optimizer)",
        "begin_line": 1048,
        "end_line": 1087,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.save_weights#1089",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.save_weights(self, filepath, overwrite=True)",
        "snippet": "    def save_weights(self, filepath, overwrite=True):\n        \"\"\"Dumps all layer weights to a HDF5 file.\n\n        The weight file has:\n            - `layer_names` (attribute), a list of strings\n                (ordered names of model layers).\n            - For every layer, a `group` named `layer.name`\n                - For every such layer group, a group attribute `weight_names`,\n                    a list of strings\n                    (ordered names of weights tensor of the layer).\n                - For every weight in the layer, a dataset\n                    storing the weight value, named after the weight tensor.\n\n        # Arguments\n            filepath: String, path to the file to save the weights to.\n            overwrite: Whether to silently overwrite any existing file at the\n                target location, or provide the user with a manual prompt.\n\n        # Raises\n            ImportError: If h5py is not available.\n        \"\"\"\n        if h5py is None:\n            raise ImportError('`save_weights` requires h5py.')\n        # If file exists and should not be overwritten:\n        if not overwrite and os.path.isfile(filepath):\n            proceed = ask_to_proceed_with_overwrite(filepath)\n            if not proceed:\n                return\n        with h5py.File(filepath, 'w') as f:\n            saving.save_weights_to_hdf5_group(f, self.layers)\n            f.flush()",
        "begin_line": 1089,
        "end_line": 1119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.engine.network.Network.load_weights#1121",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False)",
        "snippet": "    def load_weights(self, filepath, by_name=False,\n                     skip_mismatch=False, reshape=False):\n        \"\"\"Loads all layer weights from a HDF5 save file.\n\n        If `by_name` is False (default) weights are loaded\n        based on the network's topology, meaning the architecture\n        should be the same as when the weights were saved.\n        Note that layers that don't have weights are not taken\n        into account in the topological ordering, so adding or\n        removing layers is fine as long as they don't have weights.\n\n        If `by_name` is True, weights are loaded into layers\n        only if they share the same name. This is useful\n        for fine-tuning or transfer-learning models where\n        some of the layers have changed.\n\n        # Arguments\n            filepath: String, path to the weights file to load.\n            by_name: Boolean, whether to load weights by name\n                or by topological order.\n            skip_mismatch: Boolean, whether to skip loading of layers\n                where there is a mismatch in the number of weights,\n                or a mismatch in the shape of the weight\n                (only valid when `by_name`=True).\n            reshape: Reshape weights to fit the layer when the correct number\n                of weight arrays is present but their shape does not match.\n\n\n        # Raises\n            ImportError: If h5py is not available.\n        \"\"\"\n        if h5py is None:\n            raise ImportError('`load_weights` requires h5py.')\n        with h5py.File(filepath, mode='r') as f:\n            if 'layer_names' not in f.attrs and 'model_weights' in f:\n                f = f['model_weights']\n            if by_name:\n                saving.load_weights_from_hdf5_group_by_name(\n                    f, self.layers, skip_mismatch=skip_mismatch,\n                    reshape=reshape)\n            else:\n                saving.load_weights_from_hdf5_group(\n                    f, self.layers, reshape=reshape)",
        "begin_line": 1121,
        "end_line": 1163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.00047214353163361664,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.00047214353163361664,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.network.Network._updated_config#1165",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._updated_config(self)",
        "snippet": "    def _updated_config(self):\n        \"\"\"Util hared between different serialization methods.\n\n        # Returns\n            Model config with Keras version information added.\n        \"\"\"\n        from .. import __version__ as keras_version\n\n        config = self.get_config()\n        model_config = {\n            'class_name': self.__class__.__name__,\n            'config': config,\n            'keras_version': keras_version,\n            'backend': K.backend()\n        }\n        return model_config",
        "begin_line": 1165,
        "end_line": 1180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.network.Network.to_json#1182",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.to_json(self, **kwargs)",
        "snippet": "    def to_json(self, **kwargs):\n        \"\"\"Returns a JSON string containing the network configuration.\n\n        To load a network from a JSON save file, use\n        `keras.models.model_from_json(json_string, custom_objects={})`.\n\n        # Arguments\n            **kwargs: Additional keyword arguments\n                to be passed to `json.dumps()`.\n\n        # Returns\n            A JSON string.\n        \"\"\"\n        def get_json_type(obj):\n            # If obj is any numpy type\n            if type(obj).__module__ == np.__name__:\n                if isinstance(obj, np.ndarray):\n                    return obj.tolist()\n                else:\n                    return obj.item()\n\n            # If obj is a python 'type'\n            if type(obj).__name__ == type.__name__:\n                return obj.__name__\n\n            raise TypeError('Not JSON Serializable:', obj)\n\n        model_config = self._updated_config()\n        return json.dumps(model_config, default=get_json_type, **kwargs)",
        "begin_line": 1182,
        "end_line": 1210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.network.Network.get_json_type#1195",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.get_json_type(obj)",
        "snippet": "        def get_json_type(obj):\n            # If obj is any numpy type\n            if type(obj).__module__ == np.__name__:\n                if isinstance(obj, np.ndarray):\n                    return obj.tolist()\n                else:\n                    return obj.item()\n\n            # If obj is a python 'type'\n            if type(obj).__name__ == type.__name__:\n                return obj.__name__\n\n            raise TypeError('Not JSON Serializable:', obj)",
        "begin_line": 1195,
        "end_line": 1207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.07692307692307693,
            "pseudo_dstar_susp": 0.0008658008658008658,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0008658008658008658,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.network.Network.to_yaml#1212",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.to_yaml(self, **kwargs)",
        "snippet": "    def to_yaml(self, **kwargs):\n        \"\"\"Returns a yaml string containing the network configuration.\n\n        To load a network from a yaml save file, use\n        `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n        `custom_objects` should be a dictionary mapping\n        the names of custom losses / layers / etc to the corresponding\n        functions / classes.\n\n        # Arguments\n            **kwargs: Additional keyword arguments\n                to be passed to `yaml.dump()`.\n\n        # Returns\n            A YAML string.\n        \"\"\"\n        return yaml.dump(self._updated_config(), **kwargs)",
        "begin_line": 1212,
        "end_line": 1229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.1,
            "pseudo_dstar_susp": 0.0007451564828614009,
            "pseudo_tarantula_susp": 0.010638297872340425,
            "pseudo_op2_susp": 0.0007451564828614009,
            "pseudo_barinel_susp": 0.010638297872340425
        }
    },
    {
        "name": "keras.engine.network.Network.summary#1231",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.summary(self, line_length=None, positions=None, print_fn=None)",
        "snippet": "    def summary(self, line_length=None, positions=None, print_fn=None):\n        \"\"\"Prints a string summary of the network.\n\n        # Arguments\n            line_length: Total length of printed lines\n                (e.g. set this to adapt the display to different\n                terminal window sizes).\n            positions: Relative or absolute positions of log elements\n                in each line. If not provided,\n                defaults to `[.33, .55, .67, 1.]`.\n            print_fn: Print function to use.\n                It will be called on each line of the summary.\n                You can set it to a custom function\n                in order to capture the string summary.\n                It defaults to `print` (prints to stdout).\n        \"\"\"\n        if not self.built:\n            raise ValueError(\n                'This model has never been called, thus its weights '\n                'have not yet been created, so no summary can be displayed. '\n                'Build the model first '\n                '(e.g. by calling it on some test data).')\n        return print_layer_summary(self,\n                                   line_length=line_length,\n                                   positions=positions,\n                                   print_fn=print_fn)",
        "begin_line": 1231,
        "end_line": 1256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002793296089385475,
            "pseudo_dstar_susp": 0.0006968641114982578,
            "pseudo_tarantula_susp": 0.0017605633802816902,
            "pseudo_op2_susp": 0.0006968641114982578,
            "pseudo_barinel_susp": 0.0017605633802816902
        }
    },
    {
        "name": "keras.engine.network.Network.__getstate__#1258",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.__getstate__(self)",
        "snippet": "    def __getstate__(self):\n        return saving.pickle_model(self)",
        "begin_line": 1258,
        "end_line": 1259,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network.Network.__setstate__#1261",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.__setstate__(self, state)",
        "snippet": "    def __setstate__(self, state):\n        model = saving.unpickle_model(state)\n        self.__dict__.update(model.__dict__)",
        "begin_line": 1261,
        "end_line": 1263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.network._make_node_key#1266",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network",
        "signature": "keras.engine.network._make_node_key(layer_name, node_index)",
        "snippet": "def _make_node_key(layer_name, node_index):\n    return layer_name + '_ib-' + str(node_index)",
        "begin_line": 1266,
        "end_line": 1267,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000864304235090752,
            "pseudo_dstar_susp": 0.0030211480362537764,
            "pseudo_tarantula_susp": 0.00041118421052631577,
            "pseudo_op2_susp": 0.0030211480362537764,
            "pseudo_barinel_susp": 0.00041118421052631577
        }
    },
    {
        "name": "keras.engine.network._map_graph_network#1270",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network",
        "signature": "keras.engine.network._map_graph_network(inputs, outputs)",
        "snippet": "def _map_graph_network(inputs, outputs):\n    \"\"\"Validates a network's topology and gather its layers and nodes.\n\n    # Arguments\n        inputs: List of input tensors.\n        outputs: List of outputs tensors.\n\n    # Returns\n        A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.\n        - nodes: list of Node instances.\n        - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n        - layers: list of Layer instances.\n        - layers_by_depth: dict mapping ints (depth)\n            to lists of layer instances.\n\n    # Raises\n        ValueError: In case the network is not valid (e.g. disconnected graph).\n    \"\"\"\n    # Network_nodes: set of nodes included in the graph of layers\n    # (not all nodes included in the layers are relevant to the current graph).\n    network_nodes = set()  # ids of all nodes relevant to the Network\n    nodes_depths = {}  # dict {node: depth value}\n    layers_depths = {}  # dict {layer: depth value}\n    layer_indices = {}  # dict {layer: index in traversal}\n    nodes_in_decreasing_depth = []\n\n    def build_map(tensor,\n                  finished_nodes,\n                  nodes_in_progress,\n                  layer,\n                  node_index,\n                  tensor_index):\n        \"\"\"Builds a map of the graph of layers.\n\n        This recursively updates the map `layer_indices`,\n        the list `nodes_in_decreasing_depth` and the set `network_nodes`.\n\n        # Arguments:\n            tensor: Some tensor in a graph.\n            finished_nodes: Set of nodes whose subgraphs have been traversed\n                completely. Useful to prevent duplicated work.\n            nodes_in_progress: Set of nodes that are currently active on the\n                recursion stack. Useful to detect cycles.\n            layer: Layer from which `tensor` comes from. If not provided,\n                will be obtained from `tensor._keras_history`.\n            node_index: Node index from which `tensor` comes from.\n            tensor_index: Tensor_index from which `tensor` comes from.\n\n        # Raises:\n            ValueError: if a cycle is detected.\n        \"\"\"\n        node = layer._inbound_nodes[node_index]\n\n        # Prevent cycles.\n        if node in nodes_in_progress:\n            raise ValueError('The tensor ' + str(tensor) + ' at layer \"' +\n                             layer.name + '\" is part of a cycle.')\n\n        # Don't repeat work for shared subgraphs\n        if node in finished_nodes:\n            return\n\n        node_key = _make_node_key(layer.name, node_index)\n        # Update network_nodes.\n        network_nodes.add(node_key)\n\n        # Store the traversal order for layer sorting.\n        if layer not in layer_indices:\n            layer_indices[layer] = len(layer_indices)\n\n        nodes_in_progress.add(node)\n\n        # Propagate to all previous tensors connected to this node.\n        for i in range(len(node.inbound_layers)):\n            x = node.input_tensors[i]\n            layer = node.inbound_layers[i]\n            node_index = node.node_indices[i]\n            tensor_index = node.tensor_indices[i]\n            build_map(x, finished_nodes, nodes_in_progress, layer,\n                      node_index, tensor_index)\n\n        finished_nodes.add(node)\n        nodes_in_progress.remove(node)\n        nodes_in_decreasing_depth.append(node)\n\n    finished_nodes = set()\n    nodes_in_progress = set()\n    for x in outputs:\n        layer, node_index, tensor_index = x._keras_history\n        build_map(x, finished_nodes, nodes_in_progress,\n                  layer=layer,\n                  node_index=node_index,\n                  tensor_index=tensor_index)\n\n    for node in reversed(nodes_in_decreasing_depth):\n        # If the depth is not set, the node has no outbound nodes (depth 0).\n        depth = nodes_depths.setdefault(node, 0)\n\n        # Update the depth of the corresponding layer\n        previous_depth = layers_depths.get(node.outbound_layer, 0)\n        # If we've seen this layer before at a higher depth,\n        # we should use that depth instead of the node depth.\n        # This is necessary for shared layers that have inputs at different\n        # depth levels in the graph.\n        depth = max(depth, previous_depth)\n        layers_depths[node.outbound_layer] = depth\n        nodes_depths[node] = depth\n\n        # Update the depth of inbound nodes.\n        # The \"depth\" of a node is the max of the depths\n        # of all layers it is connected to.\n        for i in range(len(node.inbound_layers)):\n            inbound_layer = node.inbound_layers[i]\n            node_index = node.node_indices[i]\n            inbound_node = inbound_layer._inbound_nodes[node_index]\n            previous_depth = nodes_depths.get(inbound_node, 0)\n            nodes_depths[inbound_node] = max(depth + 1, previous_depth)\n\n    # Build a dict {depth: list of nodes with this depth}\n    nodes_by_depth = {}\n    for node, depth in nodes_depths.items():\n        if depth not in nodes_by_depth:\n            nodes_by_depth[depth] = []\n        nodes_by_depth[depth].append(node)\n\n    # Build a dict {depth: list of layers with this depth}\n    layers_by_depth = {}\n    for layer, depth in layers_depths.items():\n        if depth not in layers_by_depth:\n            layers_by_depth[depth] = []\n        layers_by_depth[depth].append(layer)\n\n    # Get sorted list of layer depths.\n    depth_keys = list(layers_by_depth.keys())\n    depth_keys.sort(reverse=True)\n\n    # Set self.layers and self._layers_by_depth.\n    layers = []\n    for depth in depth_keys:\n        layers_for_depth = layers_by_depth[depth]\n        # Network.layers needs to have a deterministic order:\n        # here we order them by traversal order.\n        layers_for_depth.sort(key=lambda x: layer_indices[x])\n        layers.extend(layers_for_depth)\n\n    # Get sorted list of node depths.\n    depth_keys = list(nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n\n    # Check that all tensors required are computable.\n    # computable_tensors: all tensors in the graph\n    # that can be computed from the inputs provided.\n    computable_tensors = []\n    for x in inputs:\n        computable_tensors.append(x)\n\n    layers_with_complete_input = []  # To provide a better error msg.\n    for depth in depth_keys:\n        for node in nodes_by_depth[depth]:\n            layer = node.outbound_layer\n            if layer:\n                for x in node.input_tensors:\n                    if x not in computable_tensors:\n                        raise ValueError('Graph disconnected: '\n                                         'cannot obtain value for tensor ' +\n                                         str(x) + ' at layer \"' +\n                                         layer.name + '\". '\n                                         'The following previous layers '\n                                         'were accessed without issue: ' +\n                                         str(layers_with_complete_input))\n                for x in node.output_tensors:\n                    computable_tensors.append(x)\n                layers_with_complete_input.append(layer.name)\n\n    # Ensure name unicity, which will be crucial for serialization\n    # (since serialized nodes refer to layers by their name).\n    all_names = [layer.name for layer in layers]\n    for name in all_names:\n        if all_names.count(name) != 1:\n            raise ValueError('The name \"' + name + '\" is used ' +\n                             str(all_names.count(name)) +\n                             ' times in the model. '\n                             'All layer names should be unique.')\n    return network_nodes, nodes_by_depth, layers, layers_by_depth",
        "begin_line": 1270,
        "end_line": 1453,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.2,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.2,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.network.build_map#1296",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network",
        "signature": "keras.engine.network.build_map(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)",
        "snippet": "    def build_map(tensor,\n                  finished_nodes,\n                  nodes_in_progress,\n                  layer,\n                  node_index,\n                  tensor_index):\n        \"\"\"Builds a map of the graph of layers.\n\n        This recursively updates the map `layer_indices`,\n        the list `nodes_in_decreasing_depth` and the set `network_nodes`.\n\n        # Arguments:\n            tensor: Some tensor in a graph.\n            finished_nodes: Set of nodes whose subgraphs have been traversed\n                completely. Useful to prevent duplicated work.\n            nodes_in_progress: Set of nodes that are currently active on the\n                recursion stack. Useful to detect cycles.\n            layer: Layer from which `tensor` comes from. If not provided,\n                will be obtained from `tensor._keras_history`.\n            node_index: Node index from which `tensor` comes from.\n            tensor_index: Tensor_index from which `tensor` comes from.\n\n        # Raises:\n            ValueError: if a cycle is detected.\n        \"\"\"\n        node = layer._inbound_nodes[node_index]\n\n        # Prevent cycles.\n        if node in nodes_in_progress:\n            raise ValueError('The tensor ' + str(tensor) + ' at layer \"' +\n                             layer.name + '\" is part of a cycle.')\n\n        # Don't repeat work for shared subgraphs\n        if node in finished_nodes:\n            return\n\n        node_key = _make_node_key(layer.name, node_index)\n        # Update network_nodes.\n        network_nodes.add(node_key)\n\n        # Store the traversal order for layer sorting.\n        if layer not in layer_indices:\n            layer_indices[layer] = len(layer_indices)\n\n        nodes_in_progress.add(node)\n\n        # Propagate to all previous tensors connected to this node.\n        for i in range(len(node.inbound_layers)):\n            x = node.input_tensors[i]\n            layer = node.inbound_layers[i]\n            node_index = node.node_indices[i]\n            tensor_index = node.tensor_indices[i]\n            build_map(x, finished_nodes, nodes_in_progress, layer,\n                      node_index, tensor_index)\n\n        finished_nodes.add(node)\n        nodes_in_progress.remove(node)\n        nodes_in_decreasing_depth.append(node)",
        "begin_line": 1296,
        "end_line": 1353,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019723865877712033,
            "pseudo_dstar_susp": 0.2,
            "pseudo_tarantula_susp": 0.0011111111111111111,
            "pseudo_op2_susp": 0.2,
            "pseudo_barinel_susp": 0.0011111111111111111
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.__init__#86",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.__init__(self, layers=None, name=None)",
        "snippet": "    def __init__(self, layers=None, name=None):\n        super(Sequential, self).__init__(name=name)\n\n        # Add to the model any layers passed to the constructor.\n        if layers:\n            for layer in layers:\n                self.add(layer)",
        "begin_line": 86,
        "end_line": 92,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0017574692442882249,
            "pseudo_dstar_susp": 0.00129366106080207,
            "pseudo_tarantula_susp": 0.0006506180871828237,
            "pseudo_op2_susp": 0.00129366106080207,
            "pseudo_barinel_susp": 0.000649772579597141
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.layers#95",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.layers(self)",
        "snippet": "    def layers(self):\n        # Historically, `sequential.layers` only returns layers that were added\n        # via `add`, and omits the auto-generated `InputLayer`\n        # that comes at the bottom of the stack.\n        if self._layers and isinstance(self._layers[0], InputLayer):\n            return self._layers[1:]\n        return self._layers",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.001199040767386091,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.001199040767386091,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.add#114",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.add(self, layer)",
        "snippet": "    def add(self, layer):\n        \"\"\"Adds a layer instance on top of the layer stack.\n\n        # Arguments\n            layer: layer instance.\n\n        # Raises\n            TypeError: If `layer` is not a layer instance.\n            ValueError: In case the `layer` argument does not\n                know its input shape.\n            ValueError: In case the `layer` argument has\n                multiple output tensors, or is already connected\n                somewhere else (forbidden in `Sequential` models).\n        \"\"\"\n        if not isinstance(layer, Layer):\n            raise TypeError('The added layer must be '\n                            'an instance of class Layer. '\n                            'Found: ' + str(layer))\n        self.built = False\n        if not self._layers:\n            set_inputs = False\n            # First layer in model: check that it is an input layer.\n            if not isinstance(layer, InputLayer):\n                # Create an input tensor and call `layer` on the input tensor.\n                # First, we need to infer the expected input shape and dtype.\n                first_layer = layer\n                if isinstance(layer, (Model, Sequential)):\n                    # We were passed a model as first layer.\n                    # This requires a specific way to figure out the\n                    # input shape and dtype.\n                    if not layer.layers:\n                        raise ValueError('Cannot add an empty model '\n                                         'to a `Sequential` model.')\n                    # In case of nested models: recover the first layer\n                    # of the deepest model to infer input shape and dtype.\n                    first_layer = layer.layers[0]\n                    while isinstance(first_layer, (Model, Sequential)):\n                        first_layer = first_layer.layers[0]\n\n                if hasattr(first_layer, 'batch_input_shape'):\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                    # Instantiate the input layer.\n                    x = Input(\n                        batch_shape=batch_shape,\n                        dtype=dtype,\n                        name=layer.name + '_input')\n                    # This will build the current layer\n                    # and create the node connecting the current layer\n                    # to the input layer we just created.\n                    layer(x)\n                    set_inputs = True\n            else:\n                # Corner case where the user passes an InputLayer via `add`.\n                assert len(layer._inbound_nodes[-1].output_tensors) == 1\n                set_inputs = True\n\n            if set_inputs:\n                if len(layer._inbound_nodes[-1].output_tensors) != 1:\n                    raise ValueError('All layers in a Sequential model '\n                                     'should have a single output tensor. '\n                                     'For multi-output layers, '\n                                     'use the functional API.')\n                self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]\n                self.inputs = network.get_source_inputs(self.outputs[0])\n        elif self.outputs:\n            output_tensor = layer(self.outputs[0])\n            if isinstance(output_tensor, list):\n                raise TypeError('All layers in a Sequential model '\n                                'should have a single output tensor. '\n                                'For multi-output layers, '\n                                'use the functional API.')\n            self.outputs = [output_tensor]\n        if self.inputs:\n            self.build()\n        else:\n            self._layers.append(layer)",
        "begin_line": 114,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033444816053511705,
            "pseudo_dstar_susp": 0.0013280212483399733,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0013280212483399733,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.pop#192",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.pop(self)",
        "snippet": "    def pop(self):\n        \"\"\"Removes the last layer in the model.\n\n        # Raises\n            TypeError: if there are no layers in the model.\n        \"\"\"\n        if not self.layers:\n            raise TypeError('There are no layers in the model.')\n\n        self._layers.pop()\n        self.built = False\n        if not self.layers:\n            self.outputs = None\n            self.inputs = None\n        elif self.outputs:\n            self.layers[-1]._outbound_nodes = []\n            self.outputs = [self.layers[-1].output]\n            self.build()",
        "begin_line": 192,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.build#211",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.build(self, input_shape=None)",
        "snippet": "    def build(self, input_shape=None):\n        if input_shape and not self.inputs:\n            batch_shape = tuple(input_shape)\n            dtype = K.floatx()\n            x = Input(batch_shape=batch_shape,\n                      dtype=dtype,\n                      name=self.name + '_input')\n            self.inputs = [x]\n            for layer in self._layers:\n                x = layer(x)\n            self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n\n        if self.inputs:\n            self._init_graph_network(self.inputs,\n                                     self.outputs,\n                                     name=self.name)\n            self.built = True",
        "begin_line": 211,
        "end_line": 229,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0018484288354898336,
            "pseudo_dstar_susp": 0.0013192612137203166,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0013192612137203166,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.predict_proba#231",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.predict_proba(self, x, batch_size=32, verbose=0)",
        "snippet": "    def predict_proba(self, x, batch_size=32, verbose=0):\n        \"\"\"Generates class probability predictions for the input samples.\n\n        The input samples are processed batch by batch.\n\n        # Arguments\n            x: input data, as a Numpy array or list of Numpy arrays\n                (if the model has multiple inputs).\n            batch_size: integer.\n            verbose: verbosity mode, 0 or 1.\n\n        # Returns\n            A Numpy array of probability predictions.\n        \"\"\"\n        preds = self.predict(x, batch_size, verbose)\n        if preds.min() < 0. or preds.max() > 1.:\n            warnings.warn('Network returning invalid probability values. '\n                          'The last layer might not normalize predictions '\n                          'into probabilities '\n                          '(like softmax or sigmoid would).')\n        return preds",
        "begin_line": 231,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.predict_classes#253",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.predict_classes(self, x, batch_size=32, verbose=0)",
        "snippet": "    def predict_classes(self, x, batch_size=32, verbose=0):\n        \"\"\"Generate class predictions for the input samples.\n\n        The input samples are processed batch by batch.\n\n        # Arguments\n            x: input data, as a Numpy array or list of Numpy arrays\n                (if the model has multiple inputs).\n            batch_size: integer.\n            verbose: verbosity mode, 0 or 1.\n\n        # Returns:\n            A numpy array of class predictions.\n        \"\"\"\n        proba = self.predict(x, batch_size=batch_size, verbose=verbose)\n        if proba.shape[-1] > 1:\n            return proba.argmax(axis=-1)\n        else:\n            return (proba > 0.5).astype('int32')",
        "begin_line": 253,
        "end_line": 271,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.get_config#273",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.get_config(self)",
        "snippet": "    def get_config(self):\n        config = []\n        for layer in self.layers:\n            config.append({\n                'class_name': layer.__class__.__name__,\n                'config': layer.get_config()\n            })\n        return copy.deepcopy(config)",
        "begin_line": 273,
        "end_line": 280,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.003401360544217687,
            "pseudo_dstar_susp": 0.0008620689655172414,
            "pseudo_tarantula_susp": 0.0015128593040847202,
            "pseudo_op2_susp": 0.0008620689655172414,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.from_config#283",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        model = cls()\n        for conf in config:\n            layer = layer_module.deserialize(conf,\n                                             custom_objects=custom_objects)\n            model.add(layer)\n        return model",
        "begin_line": 283,
        "end_line": 289,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003952569169960474,
            "pseudo_dstar_susp": 0.0003417634996582365,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0003417634996582365,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.utils.layer_utils.count_params#12",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.count_params(weights)",
        "snippet": "def count_params(weights):\n    \"\"\"Count the total number of scalars composing the weights.\n\n    # Arguments\n        weights: An iterable containing the weights on which to compute params\n\n    # Returns\n        The total number of scalars composing the weights\n    \"\"\"\n    return int(np.sum([K.count_params(p) for p in set(weights)]))",
        "begin_line": 12,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0009354536950420954,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.0009354536950420954,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.utils.layer_utils.print_summary#24",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.print_summary(model, line_length=None, positions=None, print_fn=None)",
        "snippet": "def print_summary(model, line_length=None, positions=None, print_fn=None):\n    \"\"\"Prints a summary of a model.\n\n    # Arguments\n        model: Keras model instance.\n        line_length: Total length of printed lines\n            (e.g. set this to adapt the display to different\n            terminal window sizes).\n        positions: Relative or absolute positions of log elements in each line.\n            If not provided, defaults to `[.33, .55, .67, 1.]`.\n        print_fn: Print function to use.\n            It will be called on each line of the summary.\n            You can set it to a custom function\n            in order to capture the string summary.\n            It defaults to `print` (prints to stdout).\n    \"\"\"\n    if print_fn is None:\n        print_fn = print\n\n    if model.__class__.__name__ == 'Sequential':\n        sequential_like = True\n    elif not model._is_graph_network:\n        # We treat subclassed models as a simple sequence of layers,\n        # for logging purposes.\n        sequential_like = True\n    else:\n        sequential_like = True\n        nodes_by_depth = model._nodes_by_depth.values()\n        nodes = []\n        for v in nodes_by_depth:\n            if (len(v) > 1) or (len(v) == 1 and len(v[0].inbound_layers) > 1):\n                # if the model has multiple nodes\n                # or if the nodes have multiple inbound_layers\n                # the model is no longer sequential\n                sequential_like = False\n                break\n            nodes += v\n        if sequential_like:\n            # search for shared layers\n            for layer in model.layers:\n                flag = False\n                for node in layer._inbound_nodes:\n                    if node in nodes:\n                        if flag:\n                            sequential_like = False\n                            break\n                        else:\n                            flag = True\n                if not sequential_like:\n                    break\n\n    if sequential_like:\n        line_length = line_length or 65\n        positions = positions or [.45, .85, 1.]\n        if positions[-1] <= 1:\n            positions = [int(line_length * p) for p in positions]\n        # header names for the different log elements\n        to_display = ['Layer (type)', 'Output Shape', 'Param #']\n    else:\n        line_length = line_length or 98\n        positions = positions or [.33, .55, .67, 1.]\n        if positions[-1] <= 1:\n            positions = [int(line_length * p) for p in positions]\n        # header names for the different log elements\n        to_display = ['Layer (type)',\n                      'Output Shape',\n                      'Param #',\n                      'Connected to']\n        relevant_nodes = []\n        for v in model._nodes_by_depth.values():\n            relevant_nodes += v\n\n    def print_row(fields, positions):\n        line = ''\n        for i in range(len(fields)):\n            if i > 0:\n                line = line[:-1] + ' '\n            line += str(fields[i])\n            line = line[:positions[i]]\n            line += ' ' * (positions[i] - len(line))\n        print_fn(line)\n\n    print_fn('_' * line_length)\n    print_row(to_display, positions)\n    print_fn('=' * line_length)\n\n    def print_layer_summary(layer):\n        try:\n            output_shape = layer.output_shape\n        except AttributeError:\n            output_shape = 'multiple'\n        name = layer.name\n        cls_name = layer.__class__.__name__\n        fields = [name + ' (' + cls_name + ')',\n                  output_shape, layer.count_params()]\n        print_row(fields, positions)\n\n    def print_layer_summary_with_connections(layer):\n        \"\"\"Prints a summary for a single layer.\n\n        # Arguments\n            layer: target layer.\n        \"\"\"\n        try:\n            output_shape = layer.output_shape\n        except AttributeError:\n            output_shape = 'multiple'\n        connections = []\n        for node in layer._inbound_nodes:\n            if relevant_nodes and node not in relevant_nodes:\n                # node is not part of the current network\n                continue\n            for i in range(len(node.inbound_layers)):\n                inbound_layer = node.inbound_layers[i].name\n                inbound_node_index = node.node_indices[i]\n                inbound_tensor_index = node.tensor_indices[i]\n                connections.append(inbound_layer +\n                                   '[' + str(inbound_node_index) + '][' +\n                                   str(inbound_tensor_index) + ']')\n\n        name = layer.name\n        cls_name = layer.__class__.__name__\n        if not connections:\n            first_connection = ''\n        else:\n            first_connection = connections[0]\n        fields = [name +\n                  ' (' + cls_name + ')',\n                  output_shape,\n                  layer.count_params(),\n                  first_connection]\n        print_row(fields, positions)\n        if len(connections) > 1:\n            for i in range(1, len(connections)):\n                fields = ['', '', '', connections[i]]\n                print_row(fields, positions)\n\n    layers = model.layers\n    for i in range(len(layers)):\n        if sequential_like:\n            print_layer_summary(layers[i])\n        else:\n            print_layer_summary_with_connections(layers[i])\n        if i == len(layers) - 1:\n            print_fn('=' * line_length)\n        else:\n            print_fn('_' * line_length)\n\n    model._check_trainable_weights_consistency()\n    if hasattr(model, '_collected_trainable_weights'):\n        trainable_count = count_params(model._collected_trainable_weights)\n    else:\n        trainable_count = count_params(model.trainable_weights)\n\n    non_trainable_count = count_params(model.non_trainable_weights)\n\n    print_fn(\n        'Total params: {:,}'.format(trainable_count + non_trainable_count))\n    print_fn('Trainable params: {:,}'.format(trainable_count))\n    print_fn('Non-trainable params: {:,}'.format(non_trainable_count))\n    print_fn('_' * line_length)",
        "begin_line": 24,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0007429420505200594,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0007429420505200594,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.utils.layer_utils.print_row#96",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.print_row(fields, positions)",
        "snippet": "    def print_row(fields, positions):\n        line = ''\n        for i in range(len(fields)):\n            if i > 0:\n                line = line[:-1] + ' '\n            line += str(fields[i])\n            line = line[:positions[i]]\n            line += ' ' * (positions[i] - len(line))\n        print_fn(line)",
        "begin_line": 96,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008695652173913044,
            "pseudo_dstar_susp": 0.0009363295880149813,
            "pseudo_tarantula_susp": 0.0017605633802816902,
            "pseudo_op2_susp": 0.0009363295880149813,
            "pseudo_barinel_susp": 0.0017605633802816902
        }
    },
    {
        "name": "keras.utils.layer_utils.print_layer_summary#110",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.print_layer_summary(layer)",
        "snippet": "    def print_layer_summary(layer):\n        try:\n            output_shape = layer.output_shape\n        except AttributeError:\n            output_shape = 'multiple'\n        name = layer.name\n        cls_name = layer.__class__.__name__\n        fields = [name + ' (' + cls_name + ')',\n                  output_shape, layer.count_params()]\n        print_row(fields, positions)",
        "begin_line": 110,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003401360544217687,
            "pseudo_dstar_susp": 0.0008620689655172414,
            "pseudo_tarantula_susp": 0.0015128593040847202,
            "pseudo_op2_susp": 0.0008620689655172414,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.utils.layer_utils.print_layer_summary_with_connections#121",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.print_layer_summary_with_connections(layer)",
        "snippet": "    def print_layer_summary_with_connections(layer):\n        \"\"\"Prints a summary for a single layer.\n\n        # Arguments\n            layer: target layer.\n        \"\"\"\n        try:\n            output_shape = layer.output_shape\n        except AttributeError:\n            output_shape = 'multiple'\n        connections = []\n        for node in layer._inbound_nodes:\n            if relevant_nodes and node not in relevant_nodes:\n                # node is not part of the current network\n                continue\n            for i in range(len(node.inbound_layers)):\n                inbound_layer = node.inbound_layers[i].name\n                inbound_node_index = node.node_indices[i]\n                inbound_tensor_index = node.tensor_indices[i]\n                connections.append(inbound_layer +\n                                   '[' + str(inbound_node_index) + '][' +\n                                   str(inbound_tensor_index) + ']')\n\n        name = layer.name\n        cls_name = layer.__class__.__name__\n        if not connections:\n            first_connection = ''\n        else:\n            first_connection = connections[0]\n        fields = [name +\n                  ' (' + cls_name + ')',\n                  output_shape,\n                  layer.count_params(),\n                  first_connection]\n        print_row(fields, positions)\n        if len(connections) > 1:\n            for i in range(1, len(connections)):\n                fields = ['', '', '', connections[i]]\n                print_row(fields, positions)",
        "begin_line": 121,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0008650519031141869,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0008650519031141869,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.utils.layer_utils.convert_all_kernels_in_model#187",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.convert_all_kernels_in_model(model)",
        "snippet": "def convert_all_kernels_in_model(model):\n    \"\"\"Converts all convolution kernels in a model from Theano to TensorFlow.\n\n    Also works from TensorFlow to Theano.\n\n    # Arguments\n        model: target model for the conversion.\n    \"\"\"\n    # Note: SeparableConvolution not included\n    # since only supported by TF.\n    conv_classes = {\n        'Conv1D',\n        'Conv2D',\n        'Conv3D',\n        'Conv2DTranspose',\n    }\n    to_assign = []\n    for layer in model.layers:\n        if layer.__class__.__name__ in conv_classes:\n            original_kernel = K.get_value(layer.kernel)\n            converted_kernel = convert_kernel(original_kernel)\n            to_assign.append((layer.kernel, converted_kernel))\n    K.batch_set_value(to_assign)",
        "begin_line": 187,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.layer_utils.convert_dense_weights_data_format#212",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.convert_dense_weights_data_format(dense, previous_feature_map_shape, target_data_format='channels_first')",
        "snippet": "def convert_dense_weights_data_format(dense,\n                                      previous_feature_map_shape,\n                                      target_data_format='channels_first'):\n    \"\"\"Utility useful when changing a convnet's `data_format`.\n\n    When porting the weights of a convnet from one data format to the other,\n    if the convnet includes a `Flatten` layer\n    (applied to the last convolutional feature map)\n    followed by a `Dense` layer, the weights of that `Dense` layer\n    should be updated to reflect the new dimension ordering.\n\n    # Arguments\n        dense: The target `Dense` layer.\n        previous_feature_map_shape: A shape tuple of 3 integers,\n            e.g. `(512, 7, 7)`. The shape of the convolutional\n            feature map right before the `Flatten` layer that\n            came before the target `Dense` layer.\n        target_data_format: One of \"channels_last\", \"channels_first\".\n            Set it \"channels_last\"\n            if converting a \"channels_first\" model to \"channels_last\",\n            or reciprocally.\n    \"\"\"\n    assert target_data_format in {'channels_last', 'channels_first'}\n    kernel, bias = dense.get_weights()\n    for i in range(kernel.shape[1]):\n        if target_data_format == 'channels_first':\n            c, h, w = previous_feature_map_shape\n            original_fm_shape = (h, w, c)\n            ki = kernel[:, i].reshape(original_fm_shape)\n            ki = np.transpose(ki, (2, 0, 1))  # last -> first\n        else:\n            h, w, c = previous_feature_map_shape\n            original_fm_shape = (c, h, w)\n            ki = kernel[:, i].reshape(original_fm_shape)\n            ki = np.transpose(ki, (1, 2, 0))  # first -> last\n        kernel[:, i] = np.reshape(ki, (np.prod(previous_feature_map_shape),))\n    dense.set_weights([kernel, bias])",
        "begin_line": 212,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.layer_utils.get_source_inputs#251",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.get_source_inputs(tensor, layer=None, node_index=None)",
        "snippet": "def get_source_inputs(tensor, layer=None, node_index=None):\n    \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n\n    Output will always be a list of tensors\n    (potentially with 1 element).\n\n    # Arguments\n        tensor: The tensor to start from.\n        layer: Origin layer of the tensor. Will be\n            determined via tensor._keras_history if not provided.\n        node_index: Origin node index of the tensor.\n\n    # Returns\n        List of input tensors.\n    \"\"\"\n    if not hasattr(tensor, '_keras_history'):\n        return tensor\n\n    if layer is None or node_index:\n        layer, node_index, _ = tensor._keras_history\n    if not layer._inbound_nodes:\n        return [tensor]\n    else:\n        node = layer._inbound_nodes[node_index]\n        if not node.inbound_layers:\n            # Reached an Input layer, stop recursion.\n            return node.input_tensors\n        else:\n            source_tensors = []\n            for i in range(len(node.inbound_layers)):\n                x = node.input_tensors[i]\n                layer = node.inbound_layers[i]\n                node_index = node.node_indices[i]\n                previous_sources = get_source_inputs(x,\n                                                     layer,\n                                                     node_index)\n                # Avoid input redundancy.\n                for x in previous_sources:\n                    if x not in source_tensors:\n                        source_tensors.append(x)\n            return source_tensors",
        "begin_line": 251,
        "end_line": 291,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011376564277588168,
            "pseudo_dstar_susp": 0.00117096018735363,
            "pseudo_tarantula_susp": 0.0006138735420503376,
            "pseudo_op2_susp": 0.00117096018735363,
            "pseudo_barinel_susp": 0.0006138735420503376
        }
    },
    {
        "name": "keras.backend.__init__.backend#111",
        "src_path": "keras/backend/__init__.py",
        "class_name": "keras.backend.__init__",
        "signature": "keras.backend.__init__.backend()",
        "snippet": "def backend():\n    \"\"\"Publicly accessible method\n    for determining the current backend.\n\n    # Returns\n        String, the name of the backend Keras is currently using.\n\n    # Example\n    ```python\n        >>> keras.backend.backend()\n        'tensorflow'\n    ```\n    \"\"\"\n    return _BACKEND",
        "begin_line": 111,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001402524544179523,
            "pseudo_dstar_susp": 0.03333333333333333,
            "pseudo_tarantula_susp": 0.00042936882782310007,
            "pseudo_op2_susp": 0.03333333333333333,
            "pseudo_barinel_susp": 0.00042936882782310007
        }
    },
    {
        "name": "docs.autogen.count_leading_spaces#435",
        "src_path": "docs/autogen.py",
        "class_name": "docs.autogen",
        "signature": "docs.autogen.count_leading_spaces(s)",
        "snippet": "def count_leading_spaces(s):\n    ws = re.search(r'\\S', s)\n    if ws:\n        return ws.start()\n    else:\n        return 0",
        "begin_line": 435,
        "end_line": 440,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "docs.autogen.process_list_block#443",
        "src_path": "docs/autogen.py",
        "class_name": "docs.autogen",
        "signature": "docs.autogen.process_list_block(docstring, starting_point, leading_spaces, marker)",
        "snippet": "def process_list_block(docstring, starting_point, leading_spaces, marker):\n    ending_point = docstring.find('\\n\\n', starting_point)\n    block = docstring[starting_point:(None if ending_point == -1 else\n                                      ending_point - 1)]\n    # Place marker for later reinjection.\n    docstring = docstring.replace(block, marker)\n    lines = block.split('\\n')\n    # Remove the computed number of leading white spaces from each line.\n    lines = [re.sub('^' + ' ' * leading_spaces, '', line) for line in lines]\n    # Usually lines have at least 4 additional leading spaces.\n    # These have to be removed, but first the list roots have to be detected.\n    top_level_regex = r'^    ([^\\s\\\\\\(]+):(.*)'\n    top_level_replacement = r'- __\\1__:\\2'\n    lines = [re.sub(top_level_regex, top_level_replacement, line) for line in lines]\n    # All the other lines get simply the 4 leading space (if present) removed\n    lines = [re.sub(r'^    ', '', line) for line in lines]\n    # Fix text lines after lists\n    indent = 0\n    text_block = False\n    for i in range(len(lines)):\n        line = lines[i]\n        spaces = re.search(r'\\S', line)\n        if spaces:\n            # If it is a list element\n            if line[spaces.start()] == '-':\n                indent = spaces.start() + 1\n                if text_block:\n                    text_block = False\n                    lines[i] = '\\n' + line\n            elif spaces.start() < indent:\n                text_block = True\n                indent = spaces.start()\n                lines[i] = '\\n' + line\n        else:\n            text_block = False\n            indent = 0\n    block = '\\n'.join(lines)\n    return docstring, block",
        "begin_line": 443,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "docs.autogen.process_docstring#483",
        "src_path": "docs/autogen.py",
        "class_name": "docs.autogen",
        "signature": "docs.autogen.process_docstring(docstring)",
        "snippet": "def process_docstring(docstring):\n    # First, extract code blocks and process them.\n    code_blocks = []\n    if '```' in docstring:\n        tmp = docstring[:]\n        while '```' in tmp:\n            tmp = tmp[tmp.find('```'):]\n            index = tmp[3:].find('```') + 6\n            snippet = tmp[:index]\n            # Place marker in docstring for later reinjection.\n            docstring = docstring.replace(\n                snippet, '$CODE_BLOCK_%d' % len(code_blocks))\n            snippet_lines = snippet.split('\\n')\n            # Remove leading spaces.\n            num_leading_spaces = snippet_lines[-1].find('`')\n            snippet_lines = ([snippet_lines[0]] +\n                             [line[num_leading_spaces:]\n                             for line in snippet_lines[1:]])\n            # Most code snippets have 3 or 4 more leading spaces\n            # on inner lines, but not all. Remove them.\n            inner_lines = snippet_lines[1:-1]\n            leading_spaces = None\n            for line in inner_lines:\n                if not line or line[0] == '\\n':\n                    continue\n                spaces = count_leading_spaces(line)\n                if leading_spaces is None:\n                    leading_spaces = spaces\n                if spaces < leading_spaces:\n                    leading_spaces = spaces\n            if leading_spaces:\n                snippet_lines = ([snippet_lines[0]] +\n                                 [line[leading_spaces:]\n                                  for line in snippet_lines[1:-1]] +\n                                 [snippet_lines[-1]])\n            snippet = '\\n'.join(snippet_lines)\n            code_blocks.append(snippet)\n            tmp = tmp[index:]\n\n    # Format docstring lists.\n    section_regex = r'\\n( +)# (.*)\\n'\n    section_idx = re.search(section_regex, docstring)\n    shift = 0\n    sections = {}\n    while section_idx and section_idx.group(2):\n        anchor = section_idx.group(2)\n        leading_spaces = len(section_idx.group(1))\n        shift += section_idx.end()\n        marker = '$' + anchor.replace(' ', '_') + '$'\n        docstring, content = process_list_block(docstring,\n                                                shift,\n                                                leading_spaces,\n                                                marker)\n        sections[marker] = content\n        section_idx = re.search(section_regex, docstring[shift:])\n\n    # Format docstring section titles.\n    docstring = re.sub(r'\\n(\\s+)# (.*)\\n',\n                       r'\\n\\1__\\2__\\n\\n',\n                       docstring)\n\n    # Strip all remaining leading spaces.\n    lines = docstring.split('\\n')\n    docstring = '\\n'.join([line.lstrip(' ') for line in lines])\n\n    # Reinject list blocks.\n    for marker, content in sections.items():\n        docstring = docstring.replace(marker, content)\n\n    # Reinject code blocks.\n    for i, code_block in enumerate(code_blocks):\n        docstring = docstring.replace(\n            '$CODE_BLOCK_%d' % i, code_block)\n    return docstring",
        "begin_line": 483,
        "end_line": 556,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.applications.mobilenet_v2.MobileNetV2#10",
        "src_path": "keras/applications/mobilenet_v2.py",
        "class_name": "keras.applications.mobilenet_v2",
        "signature": "keras.applications.mobilenet_v2.MobileNetV2(*args, **kwargs)",
        "snippet": "def MobileNetV2(*args, **kwargs):\n    return mobilenet_v2.MobileNetV2(*args, **kwargs)",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.losses.mean_squared_error#13",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.mean_squared_error(y_true, y_pred)",
        "snippet": "def mean_squared_error(y_true, y_pred):\n    return K.mean(K.square(y_pred - y_true), axis=-1)",
        "begin_line": 13,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003589375448671931,
            "pseudo_dstar_susp": 0.0005977286312014345,
            "pseudo_tarantula_susp": 0.00032562683165092806,
            "pseudo_op2_susp": 0.0005977286312014345,
            "pseudo_barinel_susp": 0.00032562683165092806
        }
    },
    {
        "name": "keras.losses.mean_absolute_error#17",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.mean_absolute_error(y_true, y_pred)",
        "snippet": "def mean_absolute_error(y_true, y_pred):\n    return K.mean(K.abs(y_pred - y_true), axis=-1)",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010357327809425168,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.mean_absolute_percentage_error#21",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.mean_absolute_percentage_error(y_true, y_pred)",
        "snippet": "def mean_absolute_percentage_error(y_true, y_pred):\n    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                            K.epsilon(),\n                                            None))\n    return 100. * K.mean(diff, axis=-1)",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.mean_squared_logarithmic_error#28",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.mean_squared_logarithmic_error(y_true, y_pred)",
        "snippet": "def mean_squared_logarithmic_error(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.mean(K.square(first_log - second_log), axis=-1)",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.squared_hinge#34",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.squared_hinge(y_true, y_pred)",
        "snippet": "def squared_hinge(y_true, y_pred):\n    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.hinge#38",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.hinge(y_true, y_pred)",
        "snippet": "def hinge(y_true, y_pred):\n    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.categorical_hinge#42",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.categorical_hinge(y_true, y_pred)",
        "snippet": "def categorical_hinge(y_true, y_pred):\n    pos = K.sum(y_true * y_pred, axis=-1)\n    neg = K.max((1. - y_true) * y_pred, axis=-1)\n    return K.maximum(0., neg - pos + 1.)",
        "begin_line": 42,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.logcosh#48",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.logcosh(y_true, y_pred)",
        "snippet": "def logcosh(y_true, y_pred):\n    \"\"\"Logarithm of the hyperbolic cosine of the prediction error.\n\n    `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n    like the mean squared error, but will not be so strongly affected by the\n    occasional wildly incorrect prediction.\n\n    # Arguments\n        y_true: tensor of true targets.\n        y_pred: tensor of predicted targets.\n\n    # Returns\n        Tensor with one scalar loss entry per sample.\n    \"\"\"\n    def _logcosh(x):\n        return x + K.softplus(-2. * x) - K.log(2.)\n    return K.mean(_logcosh(y_pred - y_true), axis=-1)",
        "begin_line": 48,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses._logcosh#63",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses._logcosh(x)",
        "snippet": "    def _logcosh(x):\n        return x + K.softplus(-2. * x) - K.log(2.)",
        "begin_line": 63,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.categorical_crossentropy#68",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.categorical_crossentropy(y_true, y_pred)",
        "snippet": "def categorical_crossentropy(y_true, y_pred):\n    return K.categorical_crossentropy(y_true, y_pred)",
        "begin_line": 68,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000723589001447178,
            "pseudo_dstar_susp": 0.0008319467554076539,
            "pseudo_tarantula_susp": 0.000671591672263264,
            "pseudo_op2_susp": 0.0008319467554076539,
            "pseudo_barinel_susp": 0.000671591672263264
        }
    },
    {
        "name": "keras.losses.sparse_categorical_crossentropy#72",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.sparse_categorical_crossentropy(y_true, y_pred)",
        "snippet": "def sparse_categorical_crossentropy(y_true, y_pred):\n    return K.sparse_categorical_crossentropy(y_true, y_pred)",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.binary_crossentropy#76",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.binary_crossentropy(y_true, y_pred)",
        "snippet": "def binary_crossentropy(y_true, y_pred):\n    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.kullback_leibler_divergence#80",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.kullback_leibler_divergence(y_true, y_pred)",
        "snippet": "def kullback_leibler_divergence(y_true, y_pred):\n    y_true = K.clip(y_true, K.epsilon(), 1)\n    y_pred = K.clip(y_pred, K.epsilon(), 1)\n    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)",
        "begin_line": 80,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.poisson#86",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.poisson(y_true, y_pred)",
        "snippet": "def poisson(y_true, y_pred):\n    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.cosine_proximity#90",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.cosine_proximity(y_true, y_pred)",
        "snippet": "def cosine_proximity(y_true, y_pred):\n    y_true = K.l2_normalize(y_true, axis=-1)\n    y_pred = K.l2_normalize(y_pred, axis=-1)\n    return -K.sum(y_true * y_pred, axis=-1)",
        "begin_line": 90,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.serialize#106",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.serialize(loss)",
        "snippet": "def serialize(loss):\n    return serialize_keras_object(loss)",
        "begin_line": 106,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.losses.deserialize#110",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.deserialize(name, custom_objects=None)",
        "snippet": "def deserialize(name, custom_objects=None):\n    return deserialize_keras_object(name,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='loss function')",
        "begin_line": 110,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005991611743559018,
            "pseudo_dstar_susp": 0.0011111111111111111,
            "pseudo_tarantula_susp": 0.0005271481286241434,
            "pseudo_op2_susp": 0.0011111111111111111,
            "pseudo_barinel_susp": 0.0005271481286241434
        }
    },
    {
        "name": "keras.losses.get#117",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.get(identifier)",
        "snippet": "def get(identifier):\n    \"\"\"Get the `identifier` loss function.\n\n    # Arguments\n        identifier: None or str, name of the function.\n\n    # Returns\n        The loss function or None if `identifier` is None.\n\n    # Raises\n        ValueError if unknown identifier.\n    \"\"\"\n    if identifier is None:\n        return None\n    if isinstance(identifier, six.string_types):\n        identifier = str(identifier)\n        return deserialize(identifier)\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret '\n                         'loss function identifier:', identifier)",
        "begin_line": 117,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.001122334455667789,
            "pseudo_tarantula_susp": 0.0005305039787798408,
            "pseudo_op2_susp": 0.001122334455667789,
            "pseudo_barinel_susp": 0.0005305039787798408
        }
    },
    {
        "name": "keras.datasets.imdb.load_data#14",
        "src_path": "keras/datasets/imdb.py",
        "class_name": "keras.datasets.imdb",
        "signature": "keras.datasets.imdb.load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3, **kwargs)",
        "snippet": "def load_data(path='imdb.npz', num_words=None, skip_top=0,\n              maxlen=None, seed=113,\n              start_char=1, oov_char=2, index_from=3, **kwargs):\n    \"\"\"Loads the IMDB dataset.\n\n    # Arguments\n        path: where to cache the data (relative to `~/.keras/dataset`).\n        num_words: max number of words to include. Words are ranked\n            by how often they occur (in the training set) and only\n            the most frequent words are kept\n        skip_top: skip the top N most frequently occurring words\n            (which may not be informative).\n        maxlen: sequences longer than this will be filtered out.\n        seed: random seed for sample shuffling.\n        start_char: The start of a sequence will be marked with this character.\n            Set to 1 because 0 is usually the padding character.\n        oov_char: words that were cut out because of the `num_words`\n            or `skip_top` limit will be replaced with this character.\n        index_from: index actual words with this index and higher.\n\n    # Returns\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n\n    # Raises\n        ValueError: in case `maxlen` is so low\n            that no input sequence could be kept.\n\n    Note that the 'out of vocabulary' character is only used for\n    words that were present in the training set but are not included\n    because they're not making the `num_words` cut here.\n    Words that were not seen in the training set but are in the test set\n    have simply been skipped.\n    \"\"\"\n    # Legacy support\n    if 'nb_words' in kwargs:\n        warnings.warn('The `nb_words` argument in `load_data` '\n                      'has been renamed `num_words`.')\n        num_words = kwargs.pop('nb_words')\n    if kwargs:\n        raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n\n    path = get_file(path,\n                    origin='https://s3.amazonaws.com/text-datasets/imdb.npz',\n                    file_hash='599dadb1135973df5b59232a0e9a887c')\n    with np.load(path) as f:\n        x_train, labels_train = f['x_train'], f['y_train']\n        x_test, labels_test = f['x_test'], f['y_test']\n\n    np.random.seed(seed)\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    x_train = x_train[indices]\n    labels_train = labels_train[indices]\n\n    indices = np.arange(len(x_test))\n    np.random.shuffle(indices)\n    x_test = x_test[indices]\n    labels_test = labels_test[indices]\n\n    xs = np.concatenate([x_train, x_test])\n    labels = np.concatenate([labels_train, labels_test])\n\n    if start_char is not None:\n        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n    elif index_from:\n        xs = [[w + index_from for w in x] for x in xs]\n\n    if maxlen:\n        xs, labels = _remove_long_seq(maxlen, xs, labels)\n        if not xs:\n            raise ValueError('After filtering for sequences shorter than maxlen=' +\n                             str(maxlen) + ', no sequence was kept. '\n                             'Increase maxlen.')\n    if not num_words:\n        num_words = max([max(x) for x in xs])\n\n    # by convention, use 2 as OOV word\n    # reserve 'index_from' (=3 by default) characters:\n    # 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs]\n    else:\n        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n\n    idx = len(x_train)\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n\n    return (x_train, y_train), (x_test, y_test)",
        "begin_line": 14,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.LeakyReLU.__init__#40",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.LeakyReLU",
        "signature": "keras.layers.advanced_activations.LeakyReLU.__init__(self, alpha=0.3, **kwargs)",
        "snippet": "    def __init__(self, alpha=0.3, **kwargs):\n        super(LeakyReLU, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.alpha = K.cast_to_floatx(alpha)",
        "begin_line": 40,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.LeakyReLU.call#45",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.LeakyReLU",
        "signature": "keras.layers.advanced_activations.LeakyReLU.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.relu(inputs, alpha=self.alpha)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.LeakyReLU.get_config#48",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.LeakyReLU",
        "signature": "keras.layers.advanced_activations.LeakyReLU.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'alpha': float(self.alpha)}\n        base_config = super(LeakyReLU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 48,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.LeakyReLU.compute_output_shape#53",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.LeakyReLU",
        "signature": "keras.layers.advanced_activations.LeakyReLU.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.PReLU.__init__#91",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.PReLU",
        "signature": "keras.layers.advanced_activations.PReLU.__init__(self, alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, **kwargs)",
        "snippet": "    def __init__(self, alpha_initializer='zeros',\n                 alpha_regularizer=None,\n                 alpha_constraint=None,\n                 shared_axes=None,\n                 **kwargs):\n        super(PReLU, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.alpha_initializer = initializers.get(alpha_initializer)\n        self.alpha_regularizer = regularizers.get(alpha_regularizer)\n        self.alpha_constraint = constraints.get(alpha_constraint)\n        if shared_axes is None:\n            self.shared_axes = None\n        elif not isinstance(shared_axes, (list, tuple)):\n            self.shared_axes = [shared_axes]\n        else:\n            self.shared_axes = list(shared_axes)",
        "begin_line": 91,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.PReLU.build#108",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.PReLU",
        "signature": "keras.layers.advanced_activations.PReLU.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        param_shape = list(input_shape[1:])\n        self.param_broadcast = [False] * len(param_shape)\n        if self.shared_axes is not None:\n            for i in self.shared_axes:\n                param_shape[i - 1] = 1\n                self.param_broadcast[i - 1] = True\n        self.alpha = self.add_weight(shape=param_shape,\n                                     name='alpha',\n                                     initializer=self.alpha_initializer,\n                                     regularizer=self.alpha_regularizer,\n                                     constraint=self.alpha_constraint)\n        # Set input spec\n        axes = {}\n        if self.shared_axes:\n            for i in range(1, len(input_shape)):\n                if i not in self.shared_axes:\n                    axes[i] = input_shape[i]\n        self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\n        self.built = True",
        "begin_line": 108,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.PReLU.call#129",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.PReLU",
        "signature": "keras.layers.advanced_activations.PReLU.call(self, inputs, mask=None)",
        "snippet": "    def call(self, inputs, mask=None):\n        pos = K.relu(inputs)\n        if K.backend() == 'theano':\n            neg = (K.pattern_broadcast(self.alpha, self.param_broadcast) *\n                   (inputs - K.abs(inputs)) * 0.5)\n        else:\n            neg = -self.alpha * K.relu(-inputs)\n        return pos + neg",
        "begin_line": 129,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.PReLU.get_config#138",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.PReLU",
        "signature": "keras.layers.advanced_activations.PReLU.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'alpha_initializer': initializers.serialize(self.alpha_initializer),\n            'alpha_regularizer': regularizers.serialize(self.alpha_regularizer),\n            'alpha_constraint': constraints.serialize(self.alpha_constraint),\n            'shared_axes': self.shared_axes\n        }\n        base_config = super(PReLU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 138,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.PReLU.compute_output_shape#148",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.PReLU",
        "signature": "keras.layers.advanced_activations.PReLU.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 148,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ELU.__init__#174",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ELU",
        "signature": "keras.layers.advanced_activations.ELU.__init__(self, alpha=1.0, **kwargs)",
        "snippet": "    def __init__(self, alpha=1.0, **kwargs):\n        super(ELU, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.alpha = K.cast_to_floatx(alpha)",
        "begin_line": 174,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ELU.call#179",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ELU",
        "signature": "keras.layers.advanced_activations.ELU.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.elu(inputs, self.alpha)",
        "begin_line": 179,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ELU.get_config#182",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ELU",
        "signature": "keras.layers.advanced_activations.ELU.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'alpha': float(self.alpha)}\n        base_config = super(ELU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 182,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ELU.compute_output_shape#187",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ELU",
        "signature": "keras.layers.advanced_activations.ELU.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 187,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ThresholdedReLU.__init__#213",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ThresholdedReLU",
        "signature": "keras.layers.advanced_activations.ThresholdedReLU.__init__(self, theta=1.0, **kwargs)",
        "snippet": "    def __init__(self, theta=1.0, **kwargs):\n        super(ThresholdedReLU, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.theta = K.cast_to_floatx(theta)",
        "begin_line": 213,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ThresholdedReLU.call#218",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ThresholdedReLU",
        "signature": "keras.layers.advanced_activations.ThresholdedReLU.call(self, inputs, mask=None)",
        "snippet": "    def call(self, inputs, mask=None):\n        return inputs * K.cast(K.greater(inputs, self.theta), K.floatx())",
        "begin_line": 218,
        "end_line": 219,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ThresholdedReLU.get_config#221",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ThresholdedReLU",
        "signature": "keras.layers.advanced_activations.ThresholdedReLU.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'theta': float(self.theta)}\n        base_config = super(ThresholdedReLU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 221,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ThresholdedReLU.compute_output_shape#226",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ThresholdedReLU",
        "signature": "keras.layers.advanced_activations.ThresholdedReLU.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 226,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.Softmax.__init__#245",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.Softmax",
        "signature": "keras.layers.advanced_activations.Softmax.__init__(self, axis=-1, **kwargs)",
        "snippet": "    def __init__(self, axis=-1, **kwargs):\n        super(Softmax, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis",
        "begin_line": 245,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.Softmax.call#250",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.Softmax",
        "signature": "keras.layers.advanced_activations.Softmax.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return activations.softmax(inputs, axis=self.axis)",
        "begin_line": 250,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.Softmax.get_config#253",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.Softmax",
        "signature": "keras.layers.advanced_activations.Softmax.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'axis': self.axis}\n        base_config = super(Softmax, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 253,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.Softmax.compute_output_shape#258",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.Softmax",
        "signature": "keras.layers.advanced_activations.Softmax.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 258,
        "end_line": 259,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ReLU.__init__#277",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ReLU",
        "signature": "keras.layers.advanced_activations.ReLU.__init__(self, max_value=None, **kwargs)",
        "snippet": "    def __init__(self, max_value=None, **kwargs):\n        super(ReLU, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.max_value = max_value",
        "begin_line": 277,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.layers.advanced_activations.ReLU.call#282",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ReLU",
        "signature": "keras.layers.advanced_activations.ReLU.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return activations.relu(inputs, max_value=self.max_value)",
        "begin_line": 282,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.layers.advanced_activations.ReLU.get_config#285",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ReLU",
        "signature": "keras.layers.advanced_activations.ReLU.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'max_value': self.max_value}\n        base_config = super(ReLU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 285,
        "end_line": 288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.advanced_activations.ReLU.compute_output_shape#290",
        "src_path": "keras/layers/advanced_activations.py",
        "class_name": "keras.layers.advanced_activations.ReLU",
        "signature": "keras.layers.advanced_activations.ReLU.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 290,
        "end_line": 291,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.applications.__init__.wrapper#22",
        "src_path": "keras/applications/__init__.py",
        "class_name": "keras.applications.__init__",
        "signature": "keras.applications.__init__.wrapper(*args, **kwargs)",
        "snippet": "    def wrapper(*args, **kwargs):\n        if hasattr(keras_applications, 'get_submodules_from_kwargs'):\n            kwargs['backend'] = backend\n            kwargs['layers'] = layers\n            kwargs['models'] = models\n            kwargs['utils'] = utils\n        return base_fun(*args, **kwargs)",
        "begin_line": 22,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0004510599909788002,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.layers.normalization.BatchNormalization.__init__#60",
        "src_path": "keras/layers/normalization.py",
        "class_name": "keras.layers.normalization.BatchNormalization",
        "signature": "keras.layers.normalization.BatchNormalization.__init__(self, axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, **kwargs)",
        "snippet": "    def __init__(self,\n                 axis=-1,\n                 momentum=0.99,\n                 epsilon=1e-3,\n                 center=True,\n                 scale=True,\n                 beta_initializer='zeros',\n                 gamma_initializer='ones',\n                 moving_mean_initializer='zeros',\n                 moving_variance_initializer='ones',\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        super(BatchNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.momentum = momentum\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n        self.moving_variance_initializer = initializers.get(moving_variance_initializer)\n        self.beta_regularizer = regularizers.get(beta_regularizer)\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n        self.beta_constraint = constraints.get(beta_constraint)\n        self.gamma_constraint = constraints.get(gamma_constraint)",
        "begin_line": 60,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003952569169960474,
            "pseudo_dstar_susp": 0.0003417634996582365,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0003417634996582365,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.layers.normalization.BatchNormalization.build#91",
        "src_path": "keras/layers/normalization.py",
        "class_name": "keras.layers.normalization.BatchNormalization",
        "signature": "keras.layers.normalization.BatchNormalization.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError('Axis ' + str(self.axis) + ' of '\n                             'input tensor should have a defined dimension '\n                             'but the layer received an input with shape ' +\n                             str(input_shape) + '.')\n        self.input_spec = InputSpec(ndim=len(input_shape),\n                                    axes={self.axis: dim})\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name='gamma',\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name='beta',\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.moving_mean = self.add_weight(\n            shape=shape,\n            name='moving_mean',\n            initializer=self.moving_mean_initializer,\n            trainable=False)\n        self.moving_variance = self.add_weight(\n            shape=shape,\n            name='moving_variance',\n            initializer=self.moving_variance_initializer,\n            trainable=False)\n        self.built = True",
        "begin_line": 91,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00041841004184100416,
            "pseudo_dstar_susp": 0.0003563791874554526,
            "pseudo_tarantula_susp": 0.0008525149190110827,
            "pseudo_op2_susp": 0.0003563791874554526,
            "pseudo_barinel_susp": 0.0008532423208191126
        }
    },
    {
        "name": "keras.layers.normalization.BatchNormalization.call#130",
        "src_path": "keras/layers/normalization.py",
        "class_name": "keras.layers.normalization.BatchNormalization",
        "signature": "keras.layers.normalization.BatchNormalization.call(self, inputs, training=None)",
        "snippet": "    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n        # Prepare broadcasting shape.\n        ndim = len(input_shape)\n        reduction_axes = list(range(len(input_shape)))\n        del reduction_axes[self.axis]\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis]\n\n        # Determines whether broadcasting is needed.\n        needs_broadcasting = (sorted(reduction_axes) != list(range(ndim))[:-1])\n\n        def normalize_inference():\n            if needs_broadcasting:\n                # In this case we must explicitly broadcast all parameters.\n                broadcast_moving_mean = K.reshape(self.moving_mean,\n                                                  broadcast_shape)\n                broadcast_moving_variance = K.reshape(self.moving_variance,\n                                                      broadcast_shape)\n                if self.center:\n                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n                else:\n                    broadcast_beta = None\n                if self.scale:\n                    broadcast_gamma = K.reshape(self.gamma,\n                                                broadcast_shape)\n                else:\n                    broadcast_gamma = None\n                return K.batch_normalization(\n                    inputs,\n                    broadcast_moving_mean,\n                    broadcast_moving_variance,\n                    broadcast_beta,\n                    broadcast_gamma,\n                    axis=self.axis,\n                    epsilon=self.epsilon)\n            else:\n                return K.batch_normalization(\n                    inputs,\n                    self.moving_mean,\n                    self.moving_variance,\n                    self.beta,\n                    self.gamma,\n                    axis=self.axis,\n                    epsilon=self.epsilon)\n\n        # If the learning phase is *static* and set to inference:\n        if training in {0, False}:\n            return normalize_inference()\n\n        # If the learning is either dynamic, or set to training:\n        normed_training, mean, variance = K.normalize_batch_in_training(\n            inputs, self.gamma, self.beta, reduction_axes,\n            epsilon=self.epsilon)\n\n        if K.backend() != 'cntk':\n            sample_size = K.prod([K.shape(inputs)[axis]\n                                  for axis in reduction_axes])\n            sample_size = K.cast(sample_size, dtype=K.dtype(inputs))\n\n            # sample variance - unbiased estimator of population variance\n            variance *= sample_size / (sample_size - (1.0 + self.epsilon))\n\n        self.add_update([K.moving_average_update(self.moving_mean,\n                                                 mean,\n                                                 self.momentum),\n                         K.moving_average_update(self.moving_variance,\n                                                 variance,\n                                                 self.momentum)],\n                        inputs)\n\n        # Pick the normalized form corresponding to the training phase.\n        return K.in_train_phase(normed_training,\n                                normalize_inference,\n                                training=training)",
        "begin_line": 130,
        "end_line": 204,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004985044865403788,
            "pseudo_dstar_susp": 0.0006501950585175553,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.0006501950585175553,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.layers.normalization.BatchNormalization.normalize_inference#142",
        "src_path": "keras/layers/normalization.py",
        "class_name": "keras.layers.normalization.BatchNormalization",
        "signature": "keras.layers.normalization.BatchNormalization.normalize_inference()",
        "snippet": "        def normalize_inference():\n            if needs_broadcasting:\n                # In this case we must explicitly broadcast all parameters.\n                broadcast_moving_mean = K.reshape(self.moving_mean,\n                                                  broadcast_shape)\n                broadcast_moving_variance = K.reshape(self.moving_variance,\n                                                      broadcast_shape)\n                if self.center:\n                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n                else:\n                    broadcast_beta = None\n                if self.scale:\n                    broadcast_gamma = K.reshape(self.gamma,\n                                                broadcast_shape)\n                else:\n                    broadcast_gamma = None\n                return K.batch_normalization(\n                    inputs,\n                    broadcast_moving_mean,\n                    broadcast_moving_variance,\n                    broadcast_beta,\n                    broadcast_gamma,\n                    axis=self.axis,\n                    epsilon=self.epsilon)\n            else:\n                return K.batch_normalization(\n                    inputs,\n                    self.moving_mean,\n                    self.moving_variance,\n                    self.beta,\n                    self.gamma,\n                    axis=self.axis,\n                    epsilon=self.epsilon)",
        "begin_line": 142,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005017561465127947,
            "pseudo_dstar_susp": 0.0006514657980456026,
            "pseudo_tarantula_susp": 0.001053740779768177,
            "pseudo_op2_susp": 0.0006514657980456026,
            "pseudo_barinel_susp": 0.001053740779768177
        }
    },
    {
        "name": "keras.layers.normalization.BatchNormalization.get_config#206",
        "src_path": "keras/layers/normalization.py",
        "class_name": "keras.layers.normalization.BatchNormalization",
        "signature": "keras.layers.normalization.BatchNormalization.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'axis': self.axis,\n            'momentum': self.momentum,\n            'epsilon': self.epsilon,\n            'center': self.center,\n            'scale': self.scale,\n            'beta_initializer': initializers.serialize(self.beta_initializer),\n            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n            'moving_mean_initializer': initializers.serialize(self.moving_mean_initializer),\n            'moving_variance_initializer': initializers.serialize(self.moving_variance_initializer),\n            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n            'beta_constraint': constraints.serialize(self.beta_constraint),\n            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super(BatchNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 206,
        "end_line": 223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.normalization.BatchNormalization.compute_output_shape#225",
        "src_path": "keras/layers/normalization.py",
        "class_name": "keras.layers.normalization.BatchNormalization",
        "signature": "keras.layers.normalization.BatchNormalization.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 225,
        "end_line": 226,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000407000407000407,
            "pseudo_dstar_susp": 0.0003486750348675035,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.0003486750348675035,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.applications.vgg19.VGG19#10",
        "src_path": "keras/applications/vgg19.py",
        "class_name": "keras.applications.vgg19",
        "signature": "keras.applications.vgg19.VGG19(*args, **kwargs)",
        "snippet": "def VGG19(*args, **kwargs):\n    return vgg19.VGG19(*args, **kwargs)",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.training_arrays.fit_loop#21",
        "src_path": "keras/engine/training_arrays.py",
        "class_name": "keras.engine.training_arrays",
        "signature": "keras.engine.training_arrays.fit_loop(model, f, ins, out_labels=None, batch_size=None, epochs=100, verbose=1, callbacks=None, val_f=None, val_ins=None, shuffle=True, callback_metrics=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)",
        "snippet": "def fit_loop(model, f, ins,\n             out_labels=None,\n             batch_size=None,\n             epochs=100,\n             verbose=1,\n             callbacks=None,\n             val_f=None,\n             val_ins=None,\n             shuffle=True,\n             callback_metrics=None,\n             initial_epoch=0,\n             steps_per_epoch=None,\n             validation_steps=None):\n    \"\"\"Abstract fit function for `f(ins)`.\n\n    Assumes that f returns a list, labeled by out_labels.\n\n    # Arguments\n        model: Keras model instance.\n        f: Keras function returning a list of tensors\n        ins: List of tensors to be fed to `f`\n        out_labels: List of strings, display names of\n            the outputs of `f`\n        batch_size: Integer batch size or None if unknown.\n        epochs: Number of times to iterate over the data\n        verbose: Verbosity mode, 0, 1 or 2\n        callbacks: List of callbacks to be called during training\n        val_f: Keras function to call for validation\n        val_ins: List of tensors to be fed to `val_f`\n        shuffle: Whether to shuffle the data at the beginning of each epoch\n        callback_metrics: List of strings, the display names of the metrics\n            passed to the callbacks. They should be the\n            concatenation of list the display names of the outputs of\n             `f` and the list of display names of the outputs of `f_val`.\n        initial_epoch: Epoch at which to start training\n            (useful for resuming a previous training run)\n        steps_per_epoch: Total number of steps (batches of samples)\n            before declaring one epoch finished and starting the\n            next epoch. Ignored with the default value of `None`.\n        validation_steps: Number of steps to run validation for\n            (only if doing validation from data tensors).\n            Ignored with the default value of `None`.\n\n    # Returns\n        `History` object.\n    \"\"\"\n    do_validation = False\n    if val_f and val_ins:\n        do_validation = True\n        if (verbose and ins and\n           hasattr(ins[0], 'shape') and hasattr(val_ins[0], 'shape')):\n            print('Train on %d samples, validate on %d samples' %\n                  (ins[0].shape[0], val_ins[0].shape[0]))\n    if validation_steps:\n        do_validation = True\n        if steps_per_epoch is None:\n            raise ValueError('Can only use `validation_steps` '\n                             'when doing step-wise '\n                             'training, i.e. `steps_per_epoch` '\n                             'must be set.')\n    elif do_validation:\n        if steps_per_epoch:\n            raise ValueError('Must specify `validation_steps` '\n                             'to perform validation '\n                             'when doing step-wise training.')\n\n    num_train_samples = check_num_samples(ins,\n                                          batch_size=batch_size,\n                                          steps=steps_per_epoch,\n                                          steps_name='steps_per_epoch')\n    if num_train_samples is not None:\n        index_array = np.arange(num_train_samples)\n\n    model.history = cbks.History()\n    _callbacks = [cbks.BaseLogger(\n        stateful_metrics=model.stateful_metric_names)]\n    if verbose:\n        if steps_per_epoch is not None:\n            count_mode = 'steps'\n        else:\n            count_mode = 'samples'\n        _callbacks.append(\n            cbks.ProgbarLogger(\n                count_mode,\n                stateful_metrics=model.stateful_metric_names))\n    _callbacks += (callbacks or []) + [model.history]\n    callbacks = cbks.CallbackList(_callbacks)\n    out_labels = out_labels or []\n\n    # it's possible to callback a different model than itself\n    # (used by Sequential models)\n    if hasattr(model, 'callback_model') and model.callback_model:\n        callback_model = model.callback_model\n    else:\n        callback_model = model\n\n    callbacks.set_model(callback_model)\n    callbacks.set_params({\n        'batch_size': batch_size,\n        'epochs': epochs,\n        'steps': steps_per_epoch,\n        'samples': num_train_samples,\n        'verbose': verbose,\n        'do_validation': do_validation,\n        'metrics': callback_metrics or [],\n    })\n    callbacks.on_train_begin()\n    callback_model.stop_training = False\n    for cbk in callbacks:\n        cbk.validation_data = val_ins\n\n    # To prevent a slowdown,\n    # we find beforehand the arrays that need conversion.\n    feed = (model._feed_inputs +\n            model._feed_targets +\n            model._feed_sample_weights)\n    indices_for_conversion_to_dense = []\n    for i in range(len(feed)):\n        if issparse(ins[i]) and not K.is_sparse(feed[i]):\n            indices_for_conversion_to_dense.append(i)\n\n    for epoch in range(initial_epoch, epochs):\n        # Reset stateful metrics\n        for m in model.stateful_metric_functions:\n            m.reset_states()\n        callbacks.on_epoch_begin(epoch)\n        epoch_logs = {}\n        if steps_per_epoch is not None:\n            for step_index in range(steps_per_epoch):\n                batch_logs = {}\n                batch_logs['batch'] = step_index\n                batch_logs['size'] = 1\n                callbacks.on_batch_begin(step_index, batch_logs)\n                outs = f(ins)\n\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(step_index, batch_logs)\n                if callback_model.stop_training:\n                    break\n\n            if do_validation:\n                val_outs = test_loop(model, val_f, val_ins,\n                                     steps=validation_steps,\n                                     verbose=0)\n                val_outs = to_list(val_outs)\n                # Same labels assumed.\n                for l, o in zip(out_labels, val_outs):\n                    epoch_logs['val_' + l] = o\n        else:\n            if shuffle == 'batch':\n                index_array = batch_shuffle(index_array, batch_size)\n            elif shuffle:\n                np.random.shuffle(index_array)\n\n            batches = make_batches(num_train_samples, batch_size)\n            for batch_index, (batch_start, batch_end) in enumerate(batches):\n                batch_ids = index_array[batch_start:batch_end]\n                try:\n                    if isinstance(ins[-1], float):\n                        # Do not slice the training phase flag.\n                        ins_batch = slice_arrays(\n                            ins[:-1], batch_ids) + [ins[-1]]\n                    else:\n                        ins_batch = slice_arrays(ins, batch_ids)\n                except TypeError:\n                    raise TypeError('TypeError while preparing batch. '\n                                    'If using HDF5 input data, '\n                                    'pass shuffle=\"batch\".')\n                batch_logs = {}\n                batch_logs['batch'] = batch_index\n                batch_logs['size'] = len(batch_ids)\n                callbacks.on_batch_begin(batch_index, batch_logs)\n                for i in indices_for_conversion_to_dense:\n                    ins_batch[i] = ins_batch[i].toarray()\n\n                outs = f(ins_batch)\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(batch_index, batch_logs)\n                if callback_model.stop_training:\n                    break\n\n                if batch_index == len(batches) - 1:  # Last batch.\n                    if do_validation:\n                        val_outs = test_loop(model, val_f, val_ins,\n                                             batch_size=batch_size,\n                                             verbose=0)\n                        val_outs = to_list(val_outs)\n                        # Same labels assumed.\n                        for l, o in zip(out_labels, val_outs):\n                            epoch_logs['val_' + l] = o\n        callbacks.on_epoch_end(epoch, epoch_logs)\n        if callback_model.stop_training:\n            break\n    callbacks.on_train_end()\n    return model.history",
        "begin_line": 21,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000380517503805175,
            "pseudo_dstar_susp": 0.0003307972213033411,
            "pseudo_tarantula_susp": 0.0006693440428380187,
            "pseudo_op2_susp": 0.0003307972213033411,
            "pseudo_barinel_susp": 0.0006697923643670462
        }
    },
    {
        "name": "keras.engine.training_arrays.predict_loop#224",
        "src_path": "keras/engine/training_arrays.py",
        "class_name": "keras.engine.training_arrays",
        "signature": "keras.engine.training_arrays.predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None)",
        "snippet": "def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):\n    \"\"\"Abstract method to loop over some data in batches.\n\n    # Arguments\n        model: Keras model instance.\n        f: Keras function returning a list of tensors.\n        ins: list of tensors to be fed to `f`.\n        batch_size: integer batch size.\n        verbose: verbosity mode.\n        steps: Total number of steps (batches of samples)\n            before declaring `predict_loop` finished.\n            Ignored with the default value of `None`.\n\n    # Returns\n        Array of predictions (if the model has a single output)\n        or list of arrays of predictions\n        (if the model has multiple outputs).\n    \"\"\"\n    num_samples = check_num_samples(ins,\n                                    batch_size=batch_size,\n                                    steps=steps,\n                                    steps_name='steps')\n    if verbose == 1:\n        if steps is not None:\n            progbar = Progbar(target=steps)\n        else:\n            progbar = Progbar(target=num_samples)\n\n    indices_for_conversion_to_dense = []\n    for i in range(len(model._feed_inputs)):\n        if issparse(ins[i]) and not K.is_sparse(model._feed_inputs[i]):\n            indices_for_conversion_to_dense.append(i)\n\n    if steps is not None:\n        # Step-based predictions.\n        # Since we do not know how many samples\n        # we will see, we cannot pre-allocate\n        # the returned Numpy arrays.\n        # Instead, we store one array per batch seen\n        # and concatenate them upon returning.\n        unconcatenated_outs = []\n        for step in range(steps):\n            batch_outs = f(ins)\n            batch_outs = to_list(batch_outs)\n            if step == 0:\n                for batch_out in batch_outs:\n                    unconcatenated_outs.append([])\n            for i, batch_out in enumerate(batch_outs):\n                unconcatenated_outs[i].append(batch_out)\n            if verbose == 1:\n                progbar.update(step + 1)\n        if len(unconcatenated_outs) == 1:\n            return np.concatenate(unconcatenated_outs[0], axis=0)\n        return [np.concatenate(unconcatenated_outs[i], axis=0)\n                for i in range(len(unconcatenated_outs))]\n    else:\n        # Sample-based predictions.\n        outs = []\n        batches = make_batches(num_samples, batch_size)\n        index_array = np.arange(num_samples)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            if ins and isinstance(ins[-1], float):\n                # Do not slice the training phase flag.\n                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n            else:\n                ins_batch = slice_arrays(ins, batch_ids)\n            for i in indices_for_conversion_to_dense:\n                ins_batch[i] = ins_batch[i].toarray()\n\n            batch_outs = f(ins_batch)\n            batch_outs = to_list(batch_outs)\n            if batch_index == 0:\n                # Pre-allocate the results arrays.\n                for batch_out in batch_outs:\n                    shape = (num_samples,) + batch_out.shape[1:]\n                    outs.append(np.zeros(shape, dtype=batch_out.dtype))\n            for i, batch_out in enumerate(batch_outs):\n                outs[i][batch_start:batch_end] = batch_out\n            if verbose == 1:\n                progbar.update(batch_end)\n        return unpack_singleton(outs)",
        "begin_line": 224,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026490066225165563,
            "pseudo_dstar_susp": 0.00026490066225165563,
            "pseudo_tarantula_susp": 0.0002651113467656416,
            "pseudo_op2_susp": 0.00026490066225165563,
            "pseudo_barinel_susp": 0.0002651113467656416
        }
    },
    {
        "name": "keras.engine.training_arrays.test_loop#308",
        "src_path": "keras/engine/training_arrays.py",
        "class_name": "keras.engine.training_arrays",
        "signature": "keras.engine.training_arrays.test_loop(model, f, ins, batch_size=None, verbose=0, steps=None)",
        "snippet": "def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):\n    \"\"\"Abstract method to loop over some data in batches.\n\n    # Arguments\n        model: Keras model instance.\n        f: Keras function returning a list of tensors.\n        ins: list of tensors to be fed to `f`.\n        batch_size: integer batch size or `None`.\n        verbose: verbosity mode.\n        steps: Total number of steps (batches of samples)\n            before declaring predictions finished.\n            Ignored with the default value of `None`.\n\n    # Returns\n        Scalar loss (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n    \"\"\"\n\n    if hasattr(model, 'metrics'):\n        for m in model.stateful_metric_functions:\n            m.reset_states()\n        stateful_metric_indices = [\n            i for i, name in enumerate(model.metrics_names)\n            if str(name) in model.stateful_metric_names]\n    else:\n        stateful_metric_indices = []\n\n    num_samples = check_num_samples(ins,\n                                    batch_size=batch_size,\n                                    steps=steps,\n                                    steps_name='steps')\n    outs = []\n    if verbose == 1:\n        if steps is not None:\n            progbar = Progbar(target=steps)\n        else:\n            progbar = Progbar(target=num_samples)\n\n    # To prevent a slowdown,\n    # we find beforehand the arrays that need conversion.\n    feed = (model._feed_inputs +\n            model._feed_targets +\n            model._feed_sample_weights)\n    indices_for_conversion_to_dense = []\n    for i in range(len(feed)):\n        if issparse(ins[i]) and not K.is_sparse(feed[i]):\n            indices_for_conversion_to_dense.append(i)\n\n    if steps is not None:\n        for step in range(steps):\n            batch_outs = f(ins)\n            if isinstance(batch_outs, list):\n                if step == 0:\n                    for _ in enumerate(batch_outs):\n                        outs.append(0.)\n                for i, batch_out in enumerate(batch_outs):\n                    if i in stateful_metric_indices:\n                        outs[i] = float(batch_out)\n                    else:\n                        outs[i] += batch_out\n            else:\n                if step == 0:\n                    outs.append(0.)\n                outs[0] += batch_outs\n            if verbose == 1:\n                progbar.update(step + 1)\n        for i in range(len(outs)):\n            if i not in stateful_metric_indices:\n                outs[i] /= steps\n    else:\n        batches = make_batches(num_samples, batch_size)\n        index_array = np.arange(num_samples)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            if isinstance(ins[-1], float):\n                # Do not slice the training phase flag.\n                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n            else:\n                ins_batch = slice_arrays(ins, batch_ids)\n            for i in indices_for_conversion_to_dense:\n                ins_batch[i] = ins_batch[i].toarray()\n\n            batch_outs = f(ins_batch)\n            if isinstance(batch_outs, list):\n                if batch_index == 0:\n                    for batch_out in enumerate(batch_outs):\n                        outs.append(0.)\n                for i, batch_out in enumerate(batch_outs):\n                    if i in stateful_metric_indices:\n                        outs[i] = batch_out\n                    else:\n                        outs[i] += batch_out * len(batch_ids)\n            else:\n                if batch_index == 0:\n                    outs.append(0.)\n                outs[0] += batch_outs * len(batch_ids)\n\n            if verbose == 1:\n                progbar.update(batch_end)\n        for i in range(len(outs)):\n            if i not in stateful_metric_indices:\n                outs[i] /= num_samples\n    return unpack_singleton(outs)",
        "begin_line": 308,
        "end_line": 412,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003952569169960474,
            "pseudo_dstar_susp": 0.0006027727546714888,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0006027727546714888,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.__init__#94",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        self.input_spec = None\n        self.supports_masking = False\n        self.stateful = False\n\n        # These properties will be set upon call of self.build()\n        self._trainable_weights = []\n        self._non_trainable_weights = []\n        self._losses = []\n        self._updates = []\n        self._per_input_losses = {}\n        self._per_input_updates = {}\n        self._built = False\n\n        # These lists will be filled via successive calls\n        # to self._add_inbound_node().\n        self._inbound_nodes = []\n        self._outbound_nodes = []\n\n        # These properties should be set by the user via keyword arguments.\n        # note that 'dtype', 'input_shape' and 'batch_input_shape'\n        # are only applicable to input layers: do not pass these keywords\n        # to non-input layers.\n        allowed_kwargs = {'input_shape',\n                          'batch_input_shape',\n                          'batch_size',\n                          'dtype',\n                          'name',\n                          'trainable',\n                          'weights',\n                          'input_dtype',  # legacy\n                          }\n        for kwarg in kwargs:\n            if kwarg not in allowed_kwargs:\n                raise TypeError('Keyword argument not understood:', kwarg)\n        name = kwargs.get('name')\n        if not name:\n            prefix = self.__class__.__name__\n            name = _to_snake_case(prefix) + '_' + str(K.get_uid(prefix))\n        self.name = name\n\n        self.trainable = kwargs.get('trainable', True)\n        if 'input_shape' in kwargs or 'batch_input_shape' in kwargs:\n            # In this case we will later create an input layer\n            # to insert before the current layer\n            if 'batch_input_shape' in kwargs:\n                batch_input_shape = tuple(kwargs['batch_input_shape'])\n            elif 'input_shape' in kwargs:\n                if 'batch_size' in kwargs:\n                    batch_size = kwargs['batch_size']\n                else:\n                    batch_size = None\n                batch_input_shape = (\n                    batch_size,) + tuple(kwargs['input_shape'])\n            self.batch_input_shape = batch_input_shape\n\n            # Set dtype.\n            dtype = kwargs.get('dtype')\n            if dtype is None:\n                dtype = kwargs.get('input_dtype')\n            if dtype is None:\n                dtype = K.floatx()\n            self.dtype = dtype\n\n        if 'weights' in kwargs:\n            self._initial_weights = kwargs['weights']\n        else:\n            self._initial_weights = None",
        "begin_line": 94,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011655011655011655,
            "pseudo_dstar_susp": 0.0014858841010401188,
            "pseudo_tarantula_susp": 0.0006191950464396285,
            "pseudo_op2_susp": 0.0014858841010401188,
            "pseudo_barinel_susp": 0.0006191950464396285
        }
    },
    {
        "name": "keras.engine.base_layer.Layer._node_key#164",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer._node_key(layer, node_index)",
        "snippet": "    def _node_key(layer, node_index):\n        \"\"\"Converts a layer and its index to a unique (immutable type) name.\n\n        This function is used internally with `self._network_nodes`.\n\n        # Arguments\n            layer: The layer.\n            node_index: The layer's position (e.g. via enumerate) in a list of\n                nodes.\n\n        # Returns\n            The unique name.\n        \"\"\"\n        return layer.name + '_ib-' + str(node_index)",
        "begin_line": 164,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00048192771084337347,
            "pseudo_dstar_susp": 0.001221001221001221,
            "pseudo_tarantula_susp": 0.0003402517863218782,
            "pseudo_op2_susp": 0.001221001221001221,
            "pseudo_barinel_susp": 0.0003402517863218782
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.losses#180",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.losses(self)",
        "snippet": "    def losses(self):\n        return self._losses",
        "begin_line": 180,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005022601707684581,
            "pseudo_dstar_susp": 0.000946073793755913,
            "pseudo_tarantula_susp": 0.00047732696897374703,
            "pseudo_op2_susp": 0.000946073793755913,
            "pseudo_barinel_susp": 0.00047732696897374703
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.updates#184",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.updates(self)",
        "snippet": "    def updates(self):\n        if not self.trainable and not self.stateful:\n            return []\n        return self._updates",
        "begin_line": 184,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00032425421530479895,
            "pseudo_dstar_susp": 0.0005515719801434088,
            "pseudo_tarantula_susp": 0.000308546744831842,
            "pseudo_op2_susp": 0.0005515719801434088,
            "pseudo_barinel_susp": 0.000308546744831842
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.built#190",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.built(self)",
        "snippet": "    def built(self):\n        return self._built",
        "begin_line": 190,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007052186177715092,
            "pseudo_dstar_susp": 0.0017452006980802793,
            "pseudo_tarantula_susp": 0.0003667033370003667,
            "pseudo_op2_susp": 0.0017452006980802793,
            "pseudo_barinel_susp": 0.0003667033370003667
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.built#194",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.built(self, value)",
        "snippet": "    def built(self, value):\n        self._built = value",
        "begin_line": 194,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006688963210702341,
            "pseudo_dstar_susp": 0.001589825119236884,
            "pseudo_tarantula_susp": 0.0003575259206292456,
            "pseudo_op2_susp": 0.001589825119236884,
            "pseudo_barinel_susp": 0.0003575259206292456
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.trainable_weights#198",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        trainable = getattr(self, 'trainable', True)\n        if trainable:\n            return self._trainable_weights\n        else:\n            return []",
        "begin_line": 198,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004752851711026616,
            "pseudo_dstar_susp": 0.001218026796589525,
            "pseudo_tarantula_susp": 0.000326797385620915,
            "pseudo_op2_susp": 0.001218026796589525,
            "pseudo_barinel_susp": 0.000326797385620915
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.non_trainable_weights#210",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.non_trainable_weights(self)",
        "snippet": "    def non_trainable_weights(self):\n        trainable = getattr(self, 'trainable', True)\n        if not trainable:\n            return self._trainable_weights + self._non_trainable_weights\n        else:\n            return self._non_trainable_weights",
        "begin_line": 210,
        "end_line": 215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042319085907744394,
            "pseudo_dstar_susp": 0.0008865248226950354,
            "pseudo_tarantula_susp": 0.0003259452411994785,
            "pseudo_op2_susp": 0.0008865248226950354,
            "pseudo_barinel_susp": 0.0003259452411994785
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.add_weight#222",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)",
        "snippet": "    def add_weight(self,\n                   name,\n                   shape,\n                   dtype=None,\n                   initializer=None,\n                   regularizer=None,\n                   trainable=True,\n                   constraint=None):\n        \"\"\"Adds a weight variable to the layer.\n\n        # Arguments\n            name: String, the name for the weight variable.\n            shape: The shape tuple of the weight.\n            dtype: The dtype of the weight.\n            initializer: An Initializer instance (callable).\n            regularizer: An optional Regularizer instance.\n            trainable: A boolean, whether the weight should\n                be trained via backprop or not (assuming\n                that the layer itself is also trainable).\n            constraint: An optional Constraint instance.\n\n        # Returns\n            The created weight variable.\n        \"\"\"\n        initializer = initializers.get(initializer)\n        if dtype is None:\n            dtype = K.floatx()\n        weight = K.variable(initializer(shape),\n                            dtype=dtype,\n                            name=name,\n                            constraint=constraint)\n        if regularizer is not None:\n            with K.name_scope('weight_regularizer'):\n                self.add_loss(regularizer(weight))\n        if trainable:\n            self._trainable_weights.append(weight)\n        else:\n            self._non_trainable_weights.append(weight)\n        return weight",
        "begin_line": 222,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011160714285714285,
            "pseudo_dstar_susp": 0.005376344086021506,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.005376344086021506,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.assert_input_compatibility#262",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.assert_input_compatibility(self, inputs)",
        "snippet": "    def assert_input_compatibility(self, inputs):\n        \"\"\"Checks compatibility between the layer and provided inputs.\n\n        This checks that the tensor(s) `input`\n        verify the input assumptions of the layer\n        (if any). If not, exceptions are raised.\n\n        # Arguments\n            inputs: input tensor or list of input tensors.\n\n        # Raises\n            ValueError: in case of mismatch between\n                the provided inputs and the expectations of the layer.\n        \"\"\"\n        inputs = to_list(inputs)\n        for x in inputs:\n            try:\n                K.is_keras_tensor(x)\n            except ValueError:\n                raise ValueError('Layer ' + self.name + ' was called with '\n                                 'an input that isn\\'t a symbolic tensor. '\n                                 'Received type: ' +\n                                 str(type(x)) + '. Full input: ' +\n                                 str(inputs) + '. All inputs to the layer '\n                                 'should be tensors.')\n\n        if not self.input_spec:\n            return\n        if not isinstance(self.input_spec, (list, tuple)):\n            input_spec = to_list(self.input_spec)\n        else:\n            input_spec = self.input_spec\n        if len(inputs) != len(input_spec):\n            raise ValueError('Layer ' + self.name + ' expects ' +\n                             str(len(input_spec)) + ' inputs, '\n                             'but it received ' + str(len(inputs)) +\n                             ' input tensors. Input received: ' +\n                             str(inputs))\n        for input_index, (x, spec) in enumerate(zip(inputs, input_spec)):\n            if spec is None:\n                continue\n\n            # Check ndim.\n            if spec.ndim is not None:\n                if K.ndim(x) != spec.ndim:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected ndim=' +\n                                     str(spec.ndim) + ', found ndim=' +\n                                     str(K.ndim(x)))\n            if spec.max_ndim is not None:\n                ndim = K.ndim(x)\n                if ndim is not None and ndim > spec.max_ndim:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected max_ndim=' +\n                                     str(spec.max_ndim) + ', found ndim=' +\n                                     str(K.ndim(x)))\n            if spec.min_ndim is not None:\n                ndim = K.ndim(x)\n                if ndim is not None and ndim < spec.min_ndim:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected min_ndim=' +\n                                     str(spec.min_ndim) + ', found ndim=' +\n                                     str(K.ndim(x)))\n            # Check dtype.\n            if spec.dtype is not None:\n                if K.dtype(x) != spec.dtype:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected dtype=' +\n                                     str(spec.dtype) + ', found dtype=' +\n                                     str(K.dtype(x)))\n            # Check specific shape axes.\n            if spec.axes:\n                try:\n                    x_shape = K.int_shape(x)\n                except TypeError:\n                    x_shape = None\n                if x_shape is not None:\n                    for axis, value in spec.axes.items():\n                        if (value is not None and\n                                x_shape[int(axis)] not in {value, None}):\n                            raise ValueError(\n                                'Input ' + str(input_index) +\n                                ' is incompatible with layer ' +\n                                self.name + ': expected axis ' +\n                                str(axis) + ' of input shape to have '\n                                'value ' + str(value) +\n                                ' but got shape ' + str(x_shape))\n            # Check shape.\n            if spec.shape is not None:\n                try:\n                    x_shape = K.int_shape(x)\n                except TypeError:\n                    x_shape = None\n                if x_shape is not None:\n                    for spec_dim, dim in zip(spec.shape, x_shape):\n                        if spec_dim is not None and dim is not None:\n                            if spec_dim != dim:\n                                raise ValueError(\n                                    'Input ' + str(input_index) +\n                                    ' is incompatible with layer ' +\n                                    self.name + ': expected shape=' +\n                                    str(spec.shape) + ', found shape=' +\n                                    str(x_shape))",
        "begin_line": 262,
        "end_line": 368,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.call#370",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.call(self, inputs, **kwargs)",
        "snippet": "    def call(self, inputs, **kwargs):\n        \"\"\"This is where the layer's logic lives.\n\n        # Arguments\n            inputs: Input tensor, or list/tuple of input tensors.\n            **kwargs: Additional keyword arguments.\n\n        # Returns\n            A tensor or list/tuple of tensors.\n        \"\"\"\n        return inputs",
        "begin_line": 370,
        "end_line": 380,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00037764350453172205,
            "pseudo_dstar_susp": 0.0003303600925008259,
            "pseudo_tarantula_susp": 0.0006626905235255136,
            "pseudo_op2_susp": 0.0003303600925008259,
            "pseudo_barinel_susp": 0.0006626905235255136
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.__call__#382",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.__call__(self, inputs, **kwargs)",
        "snippet": "    def __call__(self, inputs, **kwargs):\n        \"\"\"Wrapper around self.call(), for handling internal references.\n\n        If a Keras tensor is passed:\n            - We call self._add_inbound_node().\n            - If necessary, we `build` the layer to match\n                the _keras_shape of the input(s).\n            - We update the _keras_shape of every input tensor with\n                its new shape (obtained via self.compute_output_shape).\n                This is done as part of _add_inbound_node().\n            - We update the _keras_history of the output tensor(s)\n                with the current layer.\n                This is done as part of _add_inbound_node().\n\n        # Arguments\n            inputs: Can be a tensor or list/tuple of tensors.\n            **kwargs: Additional keyword arguments to be passed to `call()`.\n\n        # Returns\n            Output of the layer's `call` method.\n\n        # Raises\n            ValueError: in case the layer is missing shape information\n                for its `build` call.\n        \"\"\"\n        if isinstance(inputs, list):\n            inputs = inputs[:]\n        with K.name_scope(self.name):\n            # Handle laying building (weight creating, input spec locking).\n            if not self.built:\n                # Raise exceptions in case the input is not compatible\n                # with the input_spec specified in the layer constructor.\n                self.assert_input_compatibility(inputs)\n\n                # Collect input shapes to build layer.\n                input_shapes = []\n                for x_elem in to_list(inputs):\n                    if hasattr(x_elem, '_keras_shape'):\n                        input_shapes.append(x_elem._keras_shape)\n                    elif hasattr(K, 'int_shape'):\n                        input_shapes.append(K.int_shape(x_elem))\n                    else:\n                        raise ValueError('You tried to call layer \"' +\n                                         self.name +\n                                         '\". This layer has no information'\n                                         ' about its expected input shape, '\n                                         'and thus cannot be built. '\n                                         'You can build it manually via: '\n                                         '`layer.build(batch_input_shape)`')\n                self.build(unpack_singleton(input_shapes))\n                self.built = True\n\n                # Load weights that were specified at layer instantiation.\n                if self._initial_weights is not None:\n                    self.set_weights(self._initial_weights)\n\n            # Raise exceptions in case the input is not compatible\n            # with the input_spec set at build time.\n            self.assert_input_compatibility(inputs)\n\n            # Handle mask propagation.\n            previous_mask = _collect_previous_mask(inputs)\n            user_kwargs = copy.copy(kwargs)\n            if not is_all_none(previous_mask):\n                # The previous layer generated a mask.\n                if has_arg(self.call, 'mask'):\n                    if 'mask' not in kwargs:\n                        # If mask is explicitly passed to __call__,\n                        # we should override the default mask.\n                        kwargs['mask'] = previous_mask\n            # Handle automatic shape inference (only useful for Theano).\n            input_shape = _collect_input_shape(inputs)\n\n            # Actually call the layer,\n            # collecting output(s), mask(s), and shape(s).\n            output = self.call(inputs, **kwargs)\n            output_mask = self.compute_mask(inputs, previous_mask)\n\n            # If the layer returns tensors from its inputs, unmodified,\n            # we copy them to avoid loss of tensor metadata.\n            output_ls = to_list(output)\n            inputs_ls = to_list(inputs)\n            output_ls_copy = []\n            for x in output_ls:\n                if x in inputs_ls:\n                    x = K.identity(x)\n                output_ls_copy.append(x)\n            output = unpack_singleton(output_ls_copy)\n\n            # Inferring the output shape is only relevant for Theano.\n            if all([s is not None\n                    for s in to_list(input_shape)]):\n                output_shape = self.compute_output_shape(input_shape)\n            else:\n                if isinstance(input_shape, list):\n                    output_shape = [None for _ in input_shape]\n                else:\n                    output_shape = None\n\n            if (not isinstance(output_mask, (list, tuple)) and\n                    len(output_ls) > 1):\n                # Augment the mask to match the length of the output.\n                output_mask = [output_mask] * len(output_ls)\n\n            # Add an inbound node to the layer, so that it keeps track\n            # of the call and of all new variables created during the call.\n            # This also updates the layer history of the output tensor(s).\n            # If the input tensor(s) had not previous Keras history,\n            # this does nothing.\n            self._add_inbound_node(input_tensors=inputs,\n                                   output_tensors=output,\n                                   input_masks=previous_mask,\n                                   output_masks=output_mask,\n                                   input_shapes=input_shape,\n                                   output_shapes=output_shape,\n                                   arguments=user_kwargs)\n\n            # Apply activity regularizer if any:\n            if (hasattr(self, 'activity_regularizer') and\n                    self.activity_regularizer is not None):\n                with K.name_scope('activity_regularizer'):\n                    regularization_losses = [\n                        self.activity_regularizer(x)\n                        for x in to_list(output)]\n                self.add_loss(regularization_losses,\n                              inputs=to_list(inputs))\n        return output",
        "begin_line": 382,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.25,
            "pseudo_dstar_susp": 0.09090909090909091,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.09090909090909091,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.base_layer.Layer._add_inbound_node#510",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer._add_inbound_node(self, input_tensors, output_tensors, input_masks, output_masks, input_shapes, output_shapes, arguments=None)",
        "snippet": "    def _add_inbound_node(self, input_tensors, output_tensors,\n                          input_masks, output_masks,\n                          input_shapes, output_shapes, arguments=None):\n        \"\"\"Internal method to create an inbound node for the layer.\n\n        # Arguments\n            input_tensors: list of input tensors.\n            output_tensors: list of output tensors.\n            input_masks: list of input masks (a mask can be a tensor, or None).\n            output_masks: list of output masks\n                (a mask can be a tensor, or None).\n            input_shapes: list of input shape tuples.\n            output_shapes: list of output shape tuples.\n            arguments: dictionary of keyword arguments that were passed to the\n                `call` method of the layer at the call that created the node.\n        \"\"\"\n        input_tensors = to_list(input_tensors)\n        output_tensors = to_list(output_tensors)\n        input_masks = to_list(input_masks)\n        output_masks = to_list(output_masks)\n        input_shapes = to_list(input_shapes)\n        output_shapes = to_list(output_shapes)\n\n        # Collect input tensor(s) coordinates.\n        inbound_layers = []\n        node_indices = []\n        tensor_indices = []\n        for x in input_tensors:\n            if hasattr(x, '_keras_history'):\n                inbound_layer, node_index, tensor_index = x._keras_history\n                inbound_layers.append(inbound_layer)\n                node_indices.append(node_index)\n                tensor_indices.append(tensor_index)\n            else:\n                inbound_layers.append(None)\n                node_indices.append(None)\n                tensor_indices.append(None)\n\n        # Create node, add it to inbound nodes.\n        Node(\n            self,\n            inbound_layers=inbound_layers,\n            node_indices=node_indices,\n            tensor_indices=tensor_indices,\n            input_tensors=input_tensors,\n            output_tensors=output_tensors,\n            input_masks=input_masks,\n            output_masks=output_masks,\n            input_shapes=input_shapes,\n            output_shapes=output_shapes,\n            arguments=arguments\n        )\n\n        # Update tensor history, _keras_shape and _uses_learning_phase.\n        for i in range(len(output_tensors)):\n            output_tensors[i]._keras_shape = output_shapes[i]\n            uses_lp = any(\n                [getattr(x, '_uses_learning_phase', False)\n                 for x in input_tensors])\n            uses_lp = getattr(self, 'uses_learning_phase', False) or uses_lp\n            output_tensors[i]._uses_learning_phase = getattr(\n                output_tensors[i], '_uses_learning_phase', False) or uses_lp\n            output_tensors[i]._keras_history = (self,\n                                                len(self._inbound_nodes) - 1,\n                                                i)",
        "begin_line": 510,
        "end_line": 574,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019267822736030828,
            "pseudo_dstar_susp": 0.1,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.1,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.compute_output_shape#576",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        \"\"\"Computes the output shape of the layer.\n\n        Assumes that the layer will be built\n        to match that input shape provided.\n\n        # Arguments\n            input_shape: Shape tuple (tuple of integers)\n                or list of shape tuples (one per output tensor of the layer).\n                Shape tuples can include None for free dimensions,\n                instead of an integer.\n\n        # Returns\n            An input shape tuple.\n        \"\"\"\n        return input_shape",
        "begin_line": 576,
        "end_line": 591,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000380517503805175,
            "pseudo_dstar_susp": 0.0003307972213033411,
            "pseudo_tarantula_susp": 0.0006693440428380187,
            "pseudo_op2_susp": 0.0003307972213033411,
            "pseudo_barinel_susp": 0.0006697923643670462
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.compute_mask#593",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        \"\"\"Computes an output mask tensor.\n\n        # Arguments\n            inputs: Tensor or list of tensors.\n            mask: Tensor or list of tensors.\n\n        # Returns\n            None or a tensor (or list of tensors,\n                one per output tensor of the layer).\n        \"\"\"\n        if not self.supports_masking:\n            if mask is not None:\n                if isinstance(mask, list):\n                    if any(m is not None for m in mask):\n                        raise TypeError('Layer ' + self.name +\n                                        ' does not support masking, '\n                                        'but was passed an input_mask: ' +\n                                        str(mask))\n                else:\n                    raise TypeError('Layer ' + self.name +\n                                    ' does not support masking, '\n                                    'but was passed an input_mask: ' +\n                                    str(mask))\n            # masking not explicitly supported: return None as mask\n            return None\n        # if masking is explicitly supported, by default\n        # carry over the input mask\n        return mask",
        "begin_line": 593,
        "end_line": 621,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0030211480362537764,
            "pseudo_dstar_susp": 0.0078125,
            "pseudo_tarantula_susp": 0.000664451827242525,
            "pseudo_op2_susp": 0.0078125,
            "pseudo_barinel_susp": 0.000664451827242525
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.build#623",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        \"\"\"Creates the layer weights.\n\n        Must be implemented on all layers that have weights.\n\n        # Arguments\n            input_shape: Keras tensor (future input to layer)\n                or list/tuple of Keras tensors to reference\n                for weight shape computations.\n        \"\"\"\n        self.built = True",
        "begin_line": 623,
        "end_line": 633,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004139072847682119,
            "pseudo_dstar_susp": 0.0008045052292839903,
            "pseudo_tarantula_susp": 0.0004351610095735422,
            "pseudo_op2_susp": 0.0008045052292839903,
            "pseudo_barinel_susp": 0.0004351610095735422
        }
    },
    {
        "name": "keras.engine.base_layer.Layer._get_node_attribute_at_index#635",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer._get_node_attribute_at_index(self, node_index, attr, attr_name)",
        "snippet": "    def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n        \"\"\"Retrieves an attribute (e.g. input_tensors) from a node.\n\n        This is used to implement the methods:\n            - get_input_shape_at\n            - get_output_shape_at\n            - get_input_at\n            etc...\n\n        # Arguments\n            node_index: Integer index of the node from which\n                to retrieve the attribute.\n            attr: Exact node attribute name.\n            attr_name: Human-readable attribute name, for error messages.\n\n        # Returns\n            The layer's attribute `attr` at the node of index `node_index`.\n\n        # Raises\n            RuntimeError: If the layer has no inbound nodes.\n            ValueError: If the index is does not match any node.\n        \"\"\"\n        if not self._inbound_nodes:\n            raise RuntimeError('The layer has never been called '\n                               'and thus has no defined ' + attr_name + '.')\n        if not len(self._inbound_nodes) > node_index:\n            raise ValueError('Asked to get ' + attr_name +\n                             ' at node ' + str(node_index) +\n                             ', but the layer has only ' +\n                             str(len(self._inbound_nodes)) + ' inbound nodes.')\n        values = getattr(self._inbound_nodes[node_index], attr)\n        return unpack_singleton(values)",
        "begin_line": 635,
        "end_line": 666,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008064516129032258,
            "pseudo_dstar_susp": 0.0024154589371980675,
            "pseudo_tarantula_susp": 0.0003969829297340214,
            "pseudo_op2_susp": 0.0024154589371980675,
            "pseudo_barinel_susp": 0.0003969829297340214
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_input_shape_at#668",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_input_shape_at(self, node_index)",
        "snippet": "    def get_input_shape_at(self, node_index):\n        \"\"\"Retrieves the input shape(s) of a layer at a given node.\n\n        # Arguments\n            node_index: Integer, index of the node\n                from which to retrieve the attribute.\n                E.g. `node_index=0` will correspond to the\n                first time the layer was called.\n\n        # Returns\n            A shape tuple\n            (or list of shape tuples if the layer has multiple inputs).\n        \"\"\"\n        return self._get_node_attribute_at_index(node_index,\n                                                 'input_shapes',\n                                                 'input shape')",
        "begin_line": 668,
        "end_line": 683,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_output_shape_at#685",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_output_shape_at(self, node_index)",
        "snippet": "    def get_output_shape_at(self, node_index):\n        \"\"\"Retrieves the output shape(s) of a layer at a given node.\n\n        # Arguments\n            node_index: Integer, index of the node\n                from which to retrieve the attribute.\n                E.g. `node_index=0` will correspond to the\n                first time the layer was called.\n\n        # Returns\n            A shape tuple\n            (or list of shape tuples if the layer has multiple outputs).\n        \"\"\"\n        return self._get_node_attribute_at_index(node_index,\n                                                 'output_shapes',\n                                                 'output shape')",
        "begin_line": 685,
        "end_line": 700,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_input_at#702",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_input_at(self, node_index)",
        "snippet": "    def get_input_at(self, node_index):\n        \"\"\"Retrieves the input tensor(s) of a layer at a given node.\n\n        # Arguments\n            node_index: Integer, index of the node\n                from which to retrieve the attribute.\n                E.g. `node_index=0` will correspond to the\n                first time the layer was called.\n\n        # Returns\n            A tensor (or list of tensors if the layer has multiple inputs).\n        \"\"\"\n        return self._get_node_attribute_at_index(node_index,\n                                                 'input_tensors',\n                                                 'input')",
        "begin_line": 702,
        "end_line": 716,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_output_at#718",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_output_at(self, node_index)",
        "snippet": "    def get_output_at(self, node_index):\n        \"\"\"Retrieves the output tensor(s) of a layer at a given node.\n\n        # Arguments\n            node_index: Integer, index of the node\n                from which to retrieve the attribute.\n                E.g. `node_index=0` will correspond to the\n                first time the layer was called.\n\n        # Returns\n            A tensor (or list of tensors if the layer has multiple outputs).\n        \"\"\"\n        return self._get_node_attribute_at_index(node_index,\n                                                 'output_tensors',\n                                                 'output')",
        "begin_line": 718,
        "end_line": 732,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_input_mask_at#734",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_input_mask_at(self, node_index)",
        "snippet": "    def get_input_mask_at(self, node_index):\n        \"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n\n        # Arguments\n            node_index: Integer, index of the node\n                from which to retrieve the attribute.\n                E.g. `node_index=0` will correspond to the\n                first time the layer was called.\n\n        # Returns\n            A mask tensor\n            (or list of tensors if the layer has multiple inputs).\n        \"\"\"\n        return self._get_node_attribute_at_index(node_index,\n                                                 'input_masks',\n                                                 'input mask')",
        "begin_line": 734,
        "end_line": 749,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_output_mask_at#751",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_output_mask_at(self, node_index)",
        "snippet": "    def get_output_mask_at(self, node_index):\n        \"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n\n        # Arguments\n            node_index: Integer, index of the node\n                from which to retrieve the attribute.\n                E.g. `node_index=0` will correspond to the\n                first time the layer was called.\n\n        # Returns\n            A mask tensor\n            (or list of tensors if the layer has multiple outputs).\n        \"\"\"\n        return self._get_node_attribute_at_index(node_index,\n                                                 'output_masks',\n                                                 'output mask')",
        "begin_line": 751,
        "end_line": 766,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.input#769",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.input(self)",
        "snippet": "    def input(self):\n        \"\"\"Retrieves the input tensor(s) of a layer.\n\n        Only applicable if the layer has exactly one inbound node,\n        i.e. if it is connected to one incoming layer.\n\n        # Returns\n            Input tensor or list of input tensors.\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if len(self._inbound_nodes) > 1:\n            raise AttributeError('Layer ' + self.name +\n                                 ' has multiple inbound nodes, '\n                                 'hence the notion of \"layer input\" '\n                                 'is ill-defined. '\n                                 'Use `get_input_at(node_index)` instead.')\n        elif not self._inbound_nodes:\n            raise AttributeError('Layer ' + self.name +\n                                 ' is not connected, no input to return.')\n        return self._get_node_attribute_at_index(0, 'input_tensors',\n                                                 'input')",
        "begin_line": 769,
        "end_line": 792,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008064516129032258,
            "pseudo_dstar_susp": 0.0024154589371980675,
            "pseudo_tarantula_susp": 0.0003969829297340214,
            "pseudo_op2_susp": 0.0024154589371980675,
            "pseudo_barinel_susp": 0.0003969829297340214
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.output#795",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.output(self)",
        "snippet": "    def output(self):\n        \"\"\"Retrieves the output tensor(s) of a layer.\n\n        Only applicable if the layer has exactly one inbound node,\n        i.e. if it is connected to one incoming layer.\n\n        # Returns\n            Output tensor or list of output tensors.\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if not self._inbound_nodes:\n            raise AttributeError('Layer ' + self.name +\n                                 ' has no inbound nodes.')\n        if len(self._inbound_nodes) > 1:\n            raise AttributeError('Layer ' + self.name +\n                                 ' has multiple inbound nodes, '\n                                 'hence the notion of \"layer output\" '\n                                 'is ill-defined. '\n                                 'Use `get_output_at(node_index)` instead.')\n        return self._get_node_attribute_at_index(0, 'output_tensors',\n                                                 'output')",
        "begin_line": 795,
        "end_line": 818,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.input_mask#821",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.input_mask(self)",
        "snippet": "    def input_mask(self):\n        \"\"\"Retrieves the input mask tensor(s) of a layer.\n\n        Only applicable if the layer has exactly one inbound node,\n        i.e. if it is connected to one incoming layer.\n\n        # Returns\n            Input mask tensor (potentially None) or list of input\n            mask tensors.\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if len(self._inbound_nodes) != 1:\n            raise AttributeError('Layer ' + self.name +\n                                 ' has multiple inbound nodes, ' +\n                                 'hence the notion of \"layer input mask\" '\n                                 'is ill-defined. '\n                                 'Use `get_input_mask_at(node_index)` '\n                                 'instead.')\n        return self._get_node_attribute_at_index(0, 'input_masks',\n                                                 'input mask')",
        "begin_line": 821,
        "end_line": 843,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.output_mask#846",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.output_mask(self)",
        "snippet": "    def output_mask(self):\n        \"\"\"Retrieves the output mask tensor(s) of a layer.\n\n        Only applicable if the layer has exactly one inbound node,\n        i.e. if it is connected to one incoming layer.\n\n        # Returns\n            Output mask tensor (potentially None) or list of output\n            mask tensors.\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if len(self._inbound_nodes) != 1:\n            raise AttributeError('Layer ' + self.name +\n                                 ' has multiple inbound nodes, '\n                                 'hence the notion of \"layer output mask\" '\n                                 'is ill-defined. '\n                                 'Use `get_output_mask_at(node_index)` '\n                                 'instead.')\n        return self._get_node_attribute_at_index(0, 'output_masks',\n                                                 'output mask')",
        "begin_line": 846,
        "end_line": 868,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.input_shape#871",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.input_shape(self)",
        "snippet": "    def input_shape(self):\n        \"\"\"Retrieves the input shape tuple(s) of a layer.\n\n        Only applicable if the layer has exactly one inbound node,\n        i.e. if it is connected to one incoming layer.\n\n        # Returns\n            Input shape tuple\n            (or list of input shape tuples, one tuple per input tensor).\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if not self._inbound_nodes:\n            raise AttributeError('The layer has never been called '\n                                 'and thus has no defined input shape.')\n        all_input_shapes = set(\n            [str(node.input_shapes) for node in self._inbound_nodes])\n        if len(all_input_shapes) == 1:\n            input_shapes = self._inbound_nodes[0].input_shapes\n            return unpack_singleton(input_shapes)\n        else:\n            raise AttributeError('The layer \"' + str(self.name) +\n                                 ' has multiple inbound nodes, '\n                                 'with different input shapes. Hence '\n                                 'the notion of \"input shape\" is '\n                                 'ill-defined for the layer. '\n                                 'Use `get_input_shape_at(node_index)` '\n                                 'instead.')",
        "begin_line": 871,
        "end_line": 900,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.output_shape#903",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.output_shape(self)",
        "snippet": "    def output_shape(self):\n        \"\"\"Retrieves the output shape tuple(s) of a layer.\n\n        Only applicable if the layer has one inbound node,\n        or if all inbound nodes have the same output shape.\n\n        # Returns\n            Output shape tuple\n            (or list of input shape tuples, one tuple per output tensor).\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if not self._inbound_nodes:\n            raise AttributeError('The layer has never been called '\n                                 'and thus has no defined output shape.')\n        all_output_shapes = set(\n            [str(node.output_shapes) for node in self._inbound_nodes])\n        if len(all_output_shapes) == 1:\n            output_shapes = self._inbound_nodes[0].output_shapes\n            return unpack_singleton(output_shapes)\n        else:\n            raise AttributeError('The layer \"' + str(self.name) +\n                                 ' has multiple inbound nodes, '\n                                 'with different output shapes. Hence '\n                                 'the notion of \"output shape\" is '\n                                 'ill-defined for the layer. '\n                                 'Use `get_output_shape_at(node_index)` '\n                                 'instead.')",
        "begin_line": 903,
        "end_line": 932,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008771929824561403,
            "pseudo_dstar_susp": 0.0013315579227696406,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0013315579227696406,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.add_loss#934",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.add_loss(self, losses, inputs=None)",
        "snippet": "    def add_loss(self, losses, inputs=None):\n        \"\"\"Adds losses to the layer.\n\n        The loss may potentially be conditional on some inputs tensors,\n        for instance activity losses are conditional on the layer's inputs.\n\n        # Arguments\n            losses: loss tensor or list of loss tensors\n                to add to the layer.\n            inputs: input tensor or list of inputs tensors to mark\n                the losses as conditional on these inputs.\n                If None is passed, the loss is assumed unconditional\n                (e.g. L2 weight regularization, which only depends\n                on the layer's weights variables, not on any inputs tensors).\n        \"\"\"\n        if losses is None or losses == []:\n            return\n        # Update self.losses\n        losses = to_list(losses)\n        if hasattr(self, '_losses'):\n            self._losses += losses\n        # Update self._per_input_updates\n        if isinstance(inputs, list) and inputs == []:\n            inputs = None\n        if inputs is not None:\n            inputs_hash = object_list_uid(inputs)\n        else:\n            # Updates indexed by None are unconditional\n            # rather than input-dependent\n            inputs_hash = None\n        if inputs_hash not in self._per_input_losses:\n            self._per_input_losses[inputs_hash] = []\n        self._per_input_losses[inputs_hash] += losses",
        "begin_line": 934,
        "end_line": 966,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003862495171881035,
            "pseudo_dstar_susp": 0.0003351206434316354,
            "pseudo_tarantula_susp": 0.0006939625260235947,
            "pseudo_op2_susp": 0.0003351206434316354,
            "pseudo_barinel_susp": 0.0006939625260235947
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.add_update#968",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.add_update(self, updates, inputs=None)",
        "snippet": "    def add_update(self, updates, inputs=None):\n        \"\"\"Adds updates to the layer.\n\n        The updates may potentially be conditional on some inputs tensors,\n        for instance batch norm updates are conditional on the layer's inputs.\n\n        # Arguments\n            updates: update op or list of update ops\n                to add to the layer.\n            inputs: input tensor or list of inputs tensors to mark\n                the updates as conditional on these inputs.\n                If None is passed, the updates are assumed unconditional.\n        \"\"\"\n        if updates is None or updates == []:\n            return\n        # Update self.updates\n        updates = to_list(updates)\n        if hasattr(self, '_updates'):\n            self._updates += updates\n        # Update self._per_input_updates\n        if isinstance(inputs, list) and inputs == []:\n            inputs = None\n        if inputs is not None:\n            inputs_hash = object_list_uid(inputs)\n        else:\n            # Updates indexed by None are unconditional\n            # rather than input-dependent\n            inputs_hash = None\n        if inputs_hash not in self._per_input_updates:\n            self._per_input_updates[inputs_hash] = []\n        self._per_input_updates[inputs_hash] += updates",
        "begin_line": 968,
        "end_line": 998,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045998160073597056,
            "pseudo_dstar_susp": 0.0006321112515802782,
            "pseudo_tarantula_susp": 0.0006939625260235947,
            "pseudo_op2_susp": 0.0006321112515802782,
            "pseudo_barinel_susp": 0.0006939625260235947
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_updates_for#1000",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_updates_for(self, inputs)",
        "snippet": "    def get_updates_for(self, inputs):\n        if not self.trainable and not self.stateful:\n            return []\n        if inputs is not None:\n            inputs_hash = object_list_uid(inputs)\n        else:\n            inputs_hash = None\n        if inputs_hash in self._per_input_updates:\n            return self._per_input_updates[inputs_hash]\n        return []",
        "begin_line": 1000,
        "end_line": 1009,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003795066413662239,
            "pseudo_dstar_susp": 0.0008006405124099279,
            "pseudo_tarantula_susp": 0.0003278688524590164,
            "pseudo_op2_susp": 0.0008006405124099279,
            "pseudo_barinel_susp": 0.0003278688524590164
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_losses_for#1011",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_losses_for(self, inputs)",
        "snippet": "    def get_losses_for(self, inputs):\n        if inputs is not None:\n            inputs_hash = object_list_uid(inputs)\n        else:\n            inputs_hash = None\n        if inputs_hash in self._per_input_losses:\n            return self._per_input_losses[inputs_hash]\n        return []",
        "begin_line": 1011,
        "end_line": 1018,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010070493454179255,
            "pseudo_dstar_susp": 0.0012578616352201257,
            "pseudo_tarantula_susp": 0.0005592841163310962,
            "pseudo_op2_susp": 0.0012578616352201257,
            "pseudo_barinel_susp": 0.0005592841163310962
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.weights#1021",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.weights(self)",
        "snippet": "    def weights(self):\n        return self.trainable_weights + self.non_trainable_weights",
        "begin_line": 1021,
        "end_line": 1022,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000423728813559322,
            "pseudo_dstar_susp": 0.0008888888888888889,
            "pseudo_tarantula_susp": 0.0003262642740619902,
            "pseudo_op2_susp": 0.0008888888888888889,
            "pseudo_barinel_susp": 0.0003262642740619902
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.set_weights#1024",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.set_weights(self, weights)",
        "snippet": "    def set_weights(self, weights):\n        \"\"\"Sets the weights of the layer, from Numpy arrays.\n\n        # Arguments\n            weights: a list of Numpy arrays. The number\n                of arrays and their shape must match\n                number of the dimensions of the weights\n                of the layer (i.e. it should match the\n                output of `get_weights`).\n\n        # Raises\n            ValueError: If the provided weights list does not match the\n                layer's specifications.\n        \"\"\"\n        params = self.weights\n        if len(params) != len(weights):\n            raise ValueError('You called `set_weights(weights)` on layer \"' +\n                             self.name +\n                             '\" with a  weight list of length ' +\n                             str(len(weights)) +\n                             ', but the layer was expecting ' +\n                             str(len(params)) +\n                             ' weights. Provided weights: ' +\n                             str(weights)[:50] + '...')\n        if not params:\n            return\n        weight_value_tuples = []\n        param_values = K.batch_get_value(params)\n        for pv, p, w in zip(param_values, params, weights):\n            if pv.shape != w.shape:\n                raise ValueError('Layer weight shape ' +\n                                 str(pv.shape) +\n                                 ' not compatible with '\n                                 'provided weight shape ' + str(w.shape))\n            weight_value_tuples.append((p, w))\n        K.batch_set_value(weight_value_tuples)",
        "begin_line": 1024,
        "end_line": 1059,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_weights#1061",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_weights(self)",
        "snippet": "    def get_weights(self):\n        \"\"\"Returns the current weights of the layer.\n\n        # Returns\n            Weights values as a list of numpy arrays.\n        \"\"\"\n        params = self.weights\n        return K.batch_get_value(params)",
        "begin_line": 1061,
        "end_line": 1068,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.792754770069463e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_config#1070",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_config(self)",
        "snippet": "    def get_config(self):\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable)\n        containing the configuration of a layer.\n        The same layer can be reinstantiated later\n        (without its trained weights) from this configuration.\n\n        The config of a layer does not include connectivity\n        information, nor the layer class name. These are handled\n        by `Network` (one layer of abstraction above).\n\n        # Returns\n            Python dictionary.\n        \"\"\"\n        config = {'name': self.name,\n                  'trainable': self.trainable}\n        if hasattr(self, 'batch_input_shape'):\n            config['batch_input_shape'] = self.batch_input_shape\n        if hasattr(self, 'dtype'):\n            config['dtype'] = self.dtype\n        return config",
        "begin_line": 1070,
        "end_line": 1091,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019455252918287938,
            "pseudo_dstar_susp": 0.00088261253309797,
            "pseudo_tarantula_susp": 0.0011862396204033216,
            "pseudo_op2_susp": 0.00088261253309797,
            "pseudo_barinel_susp": 0.0011862396204033216
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.from_config#1094",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        \"\"\"Creates a layer from its config.\n\n        This method is the reverse of `get_config`,\n        capable of instantiating the same layer from the config\n        dictionary. It does not handle layer connectivity\n        (handled by Network), nor weights (handled by `set_weights`).\n\n        # Arguments\n            config: A Python dictionary, typically the\n                output of get_config.\n\n        # Returns\n            A layer instance.\n        \"\"\"\n        return cls(**config)",
        "begin_line": 1094,
        "end_line": 1109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003097893432465923,
            "pseudo_dstar_susp": 0.0005425935973955507,
            "pseudo_tarantula_susp": 0.0002876042565429968,
            "pseudo_op2_susp": 0.0005425935973955507,
            "pseudo_barinel_susp": 0.0002876042565429968
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.count_params#1111",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.count_params(self)",
        "snippet": "    def count_params(self):\n        \"\"\"Counts the total number of scalars composing the weights.\n\n        # Returns\n            An integer count.\n\n        # Raises\n            RuntimeError: if the layer isn't yet built\n                (in which case its weights aren't yet defined).\n        \"\"\"\n        if not self.built:\n            if self.__class__.__name__ == 'Sequential':\n                self.build()\n            else:\n                raise RuntimeError('You tried to call `count_params` on ' +\n                                   self.name + ', but the layer isn\\'t built. '\n                                   'You can build it manually via: `' +\n                                   self.name + '.build(batch_input_shape)`.')\n        return count_params(self.weights)",
        "begin_line": 1111,
        "end_line": 1129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.000687757909215956,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.000687757909215956,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.engine.base_layer.InputSpec.__init__#1152",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.InputSpec",
        "signature": "keras.engine.base_layer.InputSpec.__init__(self, dtype=None, shape=None, ndim=None, max_ndim=None, min_ndim=None, axes=None)",
        "snippet": "    def __init__(self, dtype=None,\n                 shape=None,\n                 ndim=None,\n                 max_ndim=None,\n                 min_ndim=None,\n                 axes=None):\n        self.dtype = dtype\n        self.shape = shape\n        if shape is not None:\n            self.ndim = len(shape)\n        else:\n            self.ndim = ndim\n        self.max_ndim = max_ndim\n        self.min_ndim = min_ndim\n        self.axes = axes or {}",
        "begin_line": 1152,
        "end_line": 1166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006666666666666666,
            "pseudo_dstar_susp": 0.0015772870662460567,
            "pseudo_tarantula_susp": 0.00035688793718772306,
            "pseudo_op2_susp": 0.0015772870662460567,
            "pseudo_barinel_susp": 0.00035688793718772306
        }
    },
    {
        "name": "keras.engine.base_layer.Node.__init__#1224",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Node",
        "signature": "keras.engine.base_layer.Node.__init__(self, outbound_layer, inbound_layers, node_indices, tensor_indices, input_tensors, output_tensors, input_masks, output_masks, input_shapes, output_shapes, arguments=None)",
        "snippet": "    def __init__(self, outbound_layer,\n                 inbound_layers, node_indices, tensor_indices,\n                 input_tensors, output_tensors,\n                 input_masks, output_masks,\n                 input_shapes, output_shapes,\n                 arguments=None):\n        # Layer instance (NOT a list).\n        # this is the layer that takes a list of input tensors\n        # and turns them into a list of output tensors.\n        # the current node will be added to\n        # the inbound_nodes of outbound_layer.\n        self.outbound_layer = outbound_layer\n\n        # The following 3 properties describe where\n        # the input tensors come from: which layers,\n        # and for each layer, which node and which\n        # tensor output of each node.\n\n        # List of layer instances.\n        self.inbound_layers = inbound_layers\n        # List of integers, 1:1 mapping with inbound_layers.\n        self.node_indices = node_indices\n        # List of integers, 1:1 mapping with inbound_layers.\n        self.tensor_indices = tensor_indices\n\n        # Following 2 properties:\n        # tensor inputs and outputs of outbound_layer.\n\n        # List of tensors. 1:1 mapping with inbound_layers.\n        self.input_tensors = input_tensors\n        # List of tensors, created by outbound_layer.call().\n        self.output_tensors = output_tensors\n\n        # Following 2 properties: input and output masks.\n        # List of tensors, 1:1 mapping with input_tensor.\n        self.input_masks = input_masks\n        # List of tensors, created by outbound_layer.compute_mask().\n        self.output_masks = output_masks\n\n        # Following 2 properties: input and output shapes.\n\n        # List of shape tuples, shapes of input_tensors.\n        self.input_shapes = input_shapes\n        # List of shape tuples, shapes of output_tensors.\n        self.output_shapes = output_shapes\n\n        # Optional keyword arguments to layer's `call`.\n        self.arguments = arguments\n\n        # Add nodes to all layers involved.\n        for layer in inbound_layers:\n            if layer is not None:\n                layer._outbound_nodes.append(self)\n        outbound_layer._inbound_nodes.append(self)",
        "begin_line": 1224,
        "end_line": 1277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007830853563038371,
            "pseudo_dstar_susp": 0.0022172949002217295,
            "pseudo_tarantula_susp": 0.0003878975950349108,
            "pseudo_op2_susp": 0.0022172949002217295,
            "pseudo_barinel_susp": 0.0003878975950349108
        }
    },
    {
        "name": "keras.engine.base_layer.Node.get_config#1279",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Node",
        "signature": "keras.engine.base_layer.Node.get_config(self)",
        "snippet": "    def get_config(self):\n        inbound_names = []\n        for layer in self.inbound_layers:\n            if layer:\n                inbound_names.append(layer.name)\n            else:\n                inbound_names.append(None)\n        if self.outbound_layer:\n            outbound_layer = self.outbound_layer.name\n        else:\n            outbound_layer = None\n        return {'outbound_layer': outbound_layer,\n                'inbound_layers': inbound_names,\n                'node_indices': self.node_indices,\n                'tensor_indices': self.tensor_indices}",
        "begin_line": 1279,
        "end_line": 1293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.base_layer._collect_previous_mask#1296",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer",
        "signature": "keras.engine.base_layer._collect_previous_mask(input_tensors)",
        "snippet": "def _collect_previous_mask(input_tensors):\n    \"\"\"Retrieves the output mask(s) of the previous node.\n\n    # Arguments\n        input_tensors: A tensor or list of tensors.\n\n    # Returns\n        A mask tensor or list of mask tensors.\n    \"\"\"\n    input_tensors = to_list(input_tensors)\n    masks = []\n    for x in input_tensors:\n        if hasattr(x, '_keras_history'):\n            inbound_layer, node_index, tensor_index = x._keras_history\n            node = inbound_layer._inbound_nodes[node_index]\n            mask = node.output_masks[tensor_index]\n            masks.append(mask)\n        else:\n            masks.append(None)\n    return unpack_singleton(masks)",
        "begin_line": 1296,
        "end_line": 1315,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000779423226812159,
            "pseudo_dstar_susp": 0.002188183807439825,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.002188183807439825,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.engine.base_layer._to_snake_case#1318",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer",
        "signature": "keras.engine.base_layer._to_snake_case(name)",
        "snippet": "def _to_snake_case(name):\n    intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\\1_\\2', name)\n    insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\n    # If the class is private the name starts with \"_\" which is not secure\n    # for creating scopes. We prefix the name with \"private\" in this case.\n    if insecure[0] != '_':\n        return insecure\n    return 'private' + insecure",
        "begin_line": 1318,
        "end_line": 1325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005,
            "pseudo_dstar_susp": 0.0013440860215053765,
            "pseudo_tarantula_susp": 0.0003365870077415012,
            "pseudo_op2_susp": 0.0013440860215053765,
            "pseudo_barinel_susp": 0.0003365870077415012
        }
    },
    {
        "name": "keras.engine.base_layer._collect_input_shape#1328",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer",
        "signature": "keras.engine.base_layer._collect_input_shape(input_tensors)",
        "snippet": "def _collect_input_shape(input_tensors):\n    \"\"\"Collects the output shape(s) of a list of Keras tensors.\n\n    # Arguments\n        input_tensors: list of input tensors (or single input tensor).\n\n    # Returns\n        List of shape tuples (or single tuple), one tuple per input.\n    \"\"\"\n    input_tensors = to_list(input_tensors)\n    shapes = []\n    for x in input_tensors:\n        try:\n            shapes.append(K.int_shape(x))\n        except TypeError:\n            shapes.append(None)\n    return unpack_singleton(shapes)",
        "begin_line": 1328,
        "end_line": 1344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007331378299120235,
            "pseudo_dstar_susp": 0.0019120458891013384,
            "pseudo_tarantula_susp": 0.00037678975131876413,
            "pseudo_op2_susp": 0.0019120458891013384,
            "pseudo_barinel_susp": 0.00037678975131876413
        }
    },
    {
        "name": "keras.legacy.interfaces.legacy_support#26",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.legacy_support(func)",
        "snippet": "    def legacy_support(func):\n        @six.wraps(func)\n        def wrapper(*args, **kwargs):\n            if object_type == 'class':\n                object_name = args[0].__class__.__name__\n            else:\n                object_name = func.__name__\n            if preprocessor:\n                args, kwargs, converted = preprocessor(args, kwargs)\n            else:\n                converted = []\n            if check_positional_args:\n                if len(args) > len(allowed_positional_args) + 1:\n                    raise TypeError('`' + object_name +\n                                    '` can accept only ' +\n                                    str(len(allowed_positional_args)) +\n                                    ' positional arguments ' +\n                                    str(tuple(allowed_positional_args)) +\n                                    ', but you passed the following '\n                                    'positional arguments: ' +\n                                    str(list(args[1:])))\n            for key in value_conversions:\n                if key in kwargs:\n                    old_value = kwargs[key]\n                    if old_value in value_conversions[key]:\n                        kwargs[key] = value_conversions[key][old_value]\n            for old_name, new_name in conversions:\n                if old_name in kwargs:\n                    value = kwargs.pop(old_name)\n                    if new_name in kwargs:\n                        raise_duplicate_arg_error(old_name, new_name)\n                    kwargs[new_name] = value\n                    converted.append((new_name, old_name))\n            if converted:\n                signature = '`' + object_name + '('\n                for i, value in enumerate(args[1:]):\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(args[1:]) - 1 or kwargs:\n                        signature += ', '\n                for i, (name, value) in enumerate(kwargs.items()):\n                    signature += name + '='\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(kwargs) - 1:\n                        signature += ', '\n                signature += ')`'\n                warnings.warn('Update your `' + object_name +\n                              '` call to the Keras 2 API: ' + signature, stacklevel=2)\n            return func(*args, **kwargs)\n        wrapper._original_function = func\n        return wrapper",
        "begin_line": 26,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006195786864931846,
            "pseudo_dstar_susp": 0.001430615164520744,
            "pseudo_tarantula_susp": 0.00033400133600534405,
            "pseudo_op2_susp": 0.001430615164520744,
            "pseudo_barinel_susp": 0.00033400133600534405
        }
    },
    {
        "name": "keras.legacy.interfaces.wrapper#28",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.wrapper(*args, **kwargs)",
        "snippet": "        def wrapper(*args, **kwargs):\n            if object_type == 'class':\n                object_name = args[0].__class__.__name__\n            else:\n                object_name = func.__name__\n            if preprocessor:\n                args, kwargs, converted = preprocessor(args, kwargs)\n            else:\n                converted = []\n            if check_positional_args:\n                if len(args) > len(allowed_positional_args) + 1:\n                    raise TypeError('`' + object_name +\n                                    '` can accept only ' +\n                                    str(len(allowed_positional_args)) +\n                                    ' positional arguments ' +\n                                    str(tuple(allowed_positional_args)) +\n                                    ', but you passed the following '\n                                    'positional arguments: ' +\n                                    str(list(args[1:])))\n            for key in value_conversions:\n                if key in kwargs:\n                    old_value = kwargs[key]\n                    if old_value in value_conversions[key]:\n                        kwargs[key] = value_conversions[key][old_value]\n            for old_name, new_name in conversions:\n                if old_name in kwargs:\n                    value = kwargs.pop(old_name)\n                    if new_name in kwargs:\n                        raise_duplicate_arg_error(old_name, new_name)\n                    kwargs[new_name] = value\n                    converted.append((new_name, old_name))\n            if converted:\n                signature = '`' + object_name + '('\n                for i, value in enumerate(args[1:]):\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(args[1:]) - 1 or kwargs:\n                        signature += ', '\n                for i, (name, value) in enumerate(kwargs.items()):\n                    signature += name + '='\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(kwargs) - 1:\n                        signature += ', '\n                signature += ')`'\n                warnings.warn('Update your `' + object_name +\n                              '` call to the Keras 2 API: ' + signature, stacklevel=2)\n            return func(*args, **kwargs)",
        "begin_line": 28,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009861932938856016,
            "pseudo_dstar_susp": 0.004149377593360996,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.004149377593360996,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.legacy.interfaces.embedding_kwargs_preprocessor#124",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.embedding_kwargs_preprocessor(args, kwargs)",
        "snippet": "def embedding_kwargs_preprocessor(args, kwargs):\n    converted = []\n    if 'dropout' in kwargs:\n        kwargs.pop('dropout')\n        warnings.warn('The `dropout` argument is no longer support in `Embedding`. '\n                      'You can apply a `keras.layers.SpatialDropout1D` layer '\n                      'right after the `Embedding` layer to get the same behavior.', stacklevel=3)\n    return args, kwargs, converted",
        "begin_line": 124,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.interfaces.recurrent_args_preprocessor#156",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.recurrent_args_preprocessor(args, kwargs)",
        "snippet": "def recurrent_args_preprocessor(args, kwargs):\n    converted = []\n    if 'forget_bias_init' in kwargs:\n        if kwargs['forget_bias_init'] == 'one':\n            kwargs.pop('forget_bias_init')\n            kwargs['unit_forget_bias'] = True\n            converted.append(('forget_bias_init', 'unit_forget_bias'))\n        else:\n            kwargs.pop('forget_bias_init')\n            warnings.warn('The `forget_bias_init` argument '\n                          'has been ignored. Use `unit_forget_bias=True` '\n                          'instead to initialize with ones.', stacklevel=3)\n    if 'input_dim' in kwargs:\n        input_length = kwargs.pop('input_length', None)\n        input_dim = kwargs.pop('input_dim')\n        input_shape = (input_length, input_dim)\n        kwargs['input_shape'] = input_shape\n        converted.append(('input_dim', 'input_shape'))\n        warnings.warn('The `input_dim` and `input_length` arguments '\n                      'in recurrent layers are deprecated. '\n                      'Use `input_shape` instead.', stacklevel=3)\n    return args, kwargs, converted",
        "begin_line": 156,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.interfaces.conv1d_args_preprocessor#241",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.conv1d_args_preprocessor(args, kwargs)",
        "snippet": "def conv1d_args_preprocessor(args, kwargs):\n    converted = []\n    if 'input_dim' in kwargs:\n        if 'input_length' in kwargs:\n            length = kwargs.pop('input_length')\n        else:\n            length = None\n        input_shape = (length, kwargs.pop('input_dim'))\n        kwargs['input_shape'] = input_shape\n        converted.append(('input_shape', 'input_dim'))\n    return args, kwargs, converted",
        "begin_line": 241,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.interfaces.conv2d_args_preprocessor#268",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.conv2d_args_preprocessor(args, kwargs)",
        "snippet": "def conv2d_args_preprocessor(args, kwargs):\n    converted = []\n    if len(args) > 4:\n        raise TypeError('Layer can receive at most 3 positional arguments.')\n    if len(args) == 4:\n        if isinstance(args[2], int) and isinstance(args[3], int):\n            new_keywords = ['padding', 'strides', 'data_format']\n            for kwd in new_keywords:\n                if kwd in kwargs:\n                    raise ValueError(\n                        'It seems that you are using the Keras 2 '\n                        'and you are passing both `kernel_size` and `strides` '\n                        'as integer positional arguments. For safety reasons, '\n                        'this is disallowed. Pass `strides` '\n                        'as a keyword argument instead.')\n            kernel_size = (args[2], args[3])\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    elif len(args) == 3 and isinstance(args[2], int):\n        if 'nb_col' in kwargs:\n            kernel_size = (args[2], kwargs.pop('nb_col'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    elif len(args) == 2:\n        if 'nb_row' in kwargs and 'nb_col' in kwargs:\n            kernel_size = (kwargs.pop('nb_row'), kwargs.pop('nb_col'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    elif len(args) == 1:\n        if 'nb_row' in kwargs and 'nb_col' in kwargs:\n            kernel_size = (kwargs.pop('nb_row'), kwargs.pop('nb_col'))\n            kwargs['kernel_size'] = kernel_size\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    return args, kwargs, converted",
        "begin_line": 268,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.legacy.interfaces.separable_conv2d_args_preprocessor#321",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.separable_conv2d_args_preprocessor(args, kwargs)",
        "snippet": "def separable_conv2d_args_preprocessor(args, kwargs):\n    converted = []\n    if 'init' in kwargs:\n        init = kwargs.pop('init')\n        kwargs['depthwise_initializer'] = init\n        kwargs['pointwise_initializer'] = init\n        converted.append(('init', 'depthwise_initializer/pointwise_initializer'))\n    args, kwargs, _converted = conv2d_args_preprocessor(args, kwargs)\n    return args, kwargs, converted + _converted",
        "begin_line": 321,
        "end_line": 329,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00041841004184100416,
            "pseudo_dstar_susp": 0.0003563791874554526,
            "pseudo_tarantula_susp": 0.0008525149190110827,
            "pseudo_op2_susp": 0.0003563791874554526,
            "pseudo_barinel_susp": 0.0008532423208191126
        }
    },
    {
        "name": "keras.legacy.interfaces.deconv2d_args_preprocessor#346",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.deconv2d_args_preprocessor(args, kwargs)",
        "snippet": "def deconv2d_args_preprocessor(args, kwargs):\n    converted = []\n    if len(args) == 5:\n        if isinstance(args[4], tuple):\n            args = args[:-1]\n            converted.append(('output_shape', None))\n    if 'output_shape' in kwargs:\n        kwargs.pop('output_shape')\n        converted.append(('output_shape', None))\n    args, kwargs, _converted = conv2d_args_preprocessor(args, kwargs)\n    return args, kwargs, converted + _converted",
        "begin_line": 346,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.interfaces.conv3d_args_preprocessor#376",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.conv3d_args_preprocessor(args, kwargs)",
        "snippet": "def conv3d_args_preprocessor(args, kwargs):\n    converted = []\n    if len(args) > 5:\n        raise TypeError('Layer can receive at most 4 positional arguments.')\n    if len(args) == 5:\n        if isinstance(args[2], int) and isinstance(args[3], int) and isinstance(args[4], int):\n            kernel_size = (args[2], args[3], args[4])\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'kernel_dim*'))\n    elif len(args) == 4 and isinstance(args[3], int):\n        if isinstance(args[2], int) and isinstance(args[3], int):\n            new_keywords = ['padding', 'strides', 'data_format']\n            for kwd in new_keywords:\n                if kwd in kwargs:\n                    raise ValueError(\n                        'It seems that you are using the Keras 2 '\n                        'and you are passing both `kernel_size` and `strides` '\n                        'as integer positional arguments. For safety reasons, '\n                        'this is disallowed. Pass `strides` '\n                        'as a keyword argument instead.')\n        if 'kernel_dim3' in kwargs:\n            kernel_size = (args[2], args[3], kwargs.pop('kernel_dim3'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'kernel_dim*'))\n    elif len(args) == 3:\n        if 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n            kernel_size = (args[2],\n                           kwargs.pop('kernel_dim2'),\n                           kwargs.pop('kernel_dim3'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'kernel_dim*'))\n    elif len(args) == 2:\n        if 'kernel_dim1' in kwargs and 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n            kernel_size = (kwargs.pop('kernel_dim1'),\n                           kwargs.pop('kernel_dim2'),\n                           kwargs.pop('kernel_dim3'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'kernel_dim*'))\n    elif len(args) == 1:\n        if 'kernel_dim1' in kwargs and 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n            kernel_size = (kwargs.pop('kernel_dim1'),\n                           kwargs.pop('kernel_dim2'),\n                           kwargs.pop('kernel_dim3'))\n            kwargs['kernel_size'] = kernel_size\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    return args, kwargs, converted",
        "begin_line": 376,
        "end_line": 421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.interfaces.batchnorm_args_preprocessor#441",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.batchnorm_args_preprocessor(args, kwargs)",
        "snippet": "def batchnorm_args_preprocessor(args, kwargs):\n    converted = []\n    if len(args) > 1:\n        raise TypeError('The `BatchNormalization` layer '\n                        'does not accept positional arguments. '\n                        'Use keyword arguments instead.')\n    if 'mode' in kwargs:\n        value = kwargs.pop('mode')\n        if value != 0:\n            raise TypeError('The `mode` argument of `BatchNormalization` '\n                            'no longer exists. `mode=1` and `mode=2` '\n                            'are no longer supported.')\n        converted.append(('mode', None))\n    return args, kwargs, converted",
        "begin_line": 441,
        "end_line": 454,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003952569169960474,
            "pseudo_dstar_susp": 0.0003417634996582365,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0003417634996582365,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.legacy.interfaces.convlstm2d_args_preprocessor#457",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.convlstm2d_args_preprocessor(args, kwargs)",
        "snippet": "def convlstm2d_args_preprocessor(args, kwargs):\n    converted = []\n    if 'forget_bias_init' in kwargs:\n        value = kwargs.pop('forget_bias_init')\n        if value == 'one':\n            kwargs['unit_forget_bias'] = True\n            converted.append(('forget_bias_init', 'unit_forget_bias'))\n        else:\n            warnings.warn('The `forget_bias_init` argument '\n                          'has been ignored. Use `unit_forget_bias=True` '\n                          'instead to initialize with ones.', stacklevel=3)\n    args, kwargs, _converted = conv2d_args_preprocessor(args, kwargs)\n    return args, kwargs, converted + _converted",
        "begin_line": 457,
        "end_line": 469,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.legacy.interfaces.zeropadding2d_args_preprocessor#498",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.zeropadding2d_args_preprocessor(args, kwargs)",
        "snippet": "def zeropadding2d_args_preprocessor(args, kwargs):\n    converted = []\n    if 'padding' in kwargs and isinstance(kwargs['padding'], dict):\n        if set(kwargs['padding'].keys()) <= {'top_pad', 'bottom_pad',\n                                             'left_pad', 'right_pad'}:\n            top_pad = kwargs['padding'].get('top_pad', 0)\n            bottom_pad = kwargs['padding'].get('bottom_pad', 0)\n            left_pad = kwargs['padding'].get('left_pad', 0)\n            right_pad = kwargs['padding'].get('right_pad', 0)\n            kwargs['padding'] = ((top_pad, bottom_pad), (left_pad, right_pad))\n            warnings.warn('The `padding` argument in the Keras 2 API no longer'\n                          'accepts dict types. You can now input argument as: '\n                          '`padding=(top_pad, bottom_pad, left_pad, right_pad)`.', stacklevel=3)\n    elif len(args) == 2 and isinstance(args[1], dict):\n        if set(args[1].keys()) <= {'top_pad', 'bottom_pad',\n                                   'left_pad', 'right_pad'}:\n            top_pad = args[1].get('top_pad', 0)\n            bottom_pad = args[1].get('bottom_pad', 0)\n            left_pad = args[1].get('left_pad', 0)\n            right_pad = args[1].get('right_pad', 0)\n            args = (args[0], ((top_pad, bottom_pad), (left_pad, right_pad)))\n            warnings.warn('The `padding` argument in the Keras 2 API no longer'\n                          'accepts dict types. You can now input argument as: '\n                          '`padding=((top_pad, bottom_pad), (left_pad, right_pad))`', stacklevel=3)\n    return args, kwargs, converted",
        "begin_line": 498,
        "end_line": 522,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0004139072847682119,
            "pseudo_tarantula_susp": 0.0015600624024961,
            "pseudo_op2_susp": 0.0004139072847682119,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.legacy.interfaces.generator_methods_args_preprocessor#571",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.generator_methods_args_preprocessor(args, kwargs)",
        "snippet": "def generator_methods_args_preprocessor(args, kwargs):\n    converted = []\n    if len(args) < 3:\n        if 'samples_per_epoch' in kwargs:\n            samples_per_epoch = kwargs.pop('samples_per_epoch')\n            if len(args) > 1:\n                generator = args[1]\n            else:\n                generator = kwargs['generator']\n            if hasattr(generator, 'batch_size'):\n                kwargs['steps_per_epoch'] = samples_per_epoch // generator.batch_size\n            else:\n                kwargs['steps_per_epoch'] = samples_per_epoch\n            converted.append(('samples_per_epoch', 'steps_per_epoch'))\n\n    keras1_args = {'samples_per_epoch', 'val_samples', 'nb_epoch', 'nb_val_samples', 'nb_worker'}\n    if keras1_args.intersection(kwargs.keys()):\n        warnings.warn('The semantics of the Keras 2 argument '\n                      '`steps_per_epoch` is not the same as the '\n                      'Keras 1 argument `samples_per_epoch`. '\n                      '`steps_per_epoch` is the number of batches '\n                      'to draw from the generator at each epoch. '\n                      'Basically steps_per_epoch = samples_per_epoch/batch_size. '\n                      'Similarly `nb_val_samples`->`validation_steps` and '\n                      '`val_samples`->`steps` arguments have changed. '\n                      'Update your method calls accordingly.', stacklevel=3)\n\n    return args, kwargs, converted",
        "begin_line": 571,
        "end_line": 598,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.legacy.interfaces.add_weight_args_preprocessing#623",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.add_weight_args_preprocessing(args, kwargs)",
        "snippet": "def add_weight_args_preprocessing(args, kwargs):\n    if len(args) > 1:\n        if isinstance(args[1], (tuple, list)):\n            kwargs['shape'] = args[1]\n            args = (args[0],) + args[2:]\n            if len(args) > 1:\n                if isinstance(args[1], six.string_types):\n                    kwargs['name'] = args[1]\n                    args = (args[0],) + args[2:]\n    return args, kwargs, []",
        "begin_line": 623,
        "end_line": 632,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010964912280701754,
            "pseudo_dstar_susp": 0.0049504950495049506,
            "pseudo_tarantula_susp": 0.00047058823529411766,
            "pseudo_op2_susp": 0.0049504950495049506,
            "pseudo_barinel_susp": 0.00047058823529411766
        }
    },
    {
        "name": "keras.legacy.interfaces.get_updates_arg_preprocessing#640",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.get_updates_arg_preprocessing(args, kwargs)",
        "snippet": "def get_updates_arg_preprocessing(args, kwargs):\n    # Old interface: (params, constraints, loss)\n    # New interface: (loss, params)\n    if len(args) > 4:\n        raise TypeError('`get_update` call received more arguments '\n                        'than expected.')\n    elif len(args) == 4:\n        # Assuming old interface.\n        opt, params, _, loss = args\n        kwargs['loss'] = loss\n        kwargs['params'] = params\n        return [opt], kwargs, []\n    elif len(args) == 3:\n        if isinstance(args[1], (list, tuple)):\n            assert isinstance(args[2], dict)\n            assert 'loss' in kwargs\n            opt, params, _ = args\n            kwargs['params'] = params\n            return [opt], kwargs, []\n    return args, kwargs, []",
        "begin_line": 640,
        "end_line": 659,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003270111183780249,
            "pseudo_dstar_susp": 0.0005595970900951316,
            "pseudo_tarantula_susp": 0.0003112356053532524,
            "pseudo_op2_susp": 0.0005595970900951316,
            "pseudo_barinel_susp": 0.0003112356053532524
        }
    },
    {
        "name": "keras.utils.vis_utils._check_pydot#16",
        "src_path": "keras/utils/vis_utils.py",
        "class_name": "keras.utils.vis_utils",
        "signature": "keras.utils.vis_utils._check_pydot()",
        "snippet": "def _check_pydot():\n    \"\"\"Raise errors if `pydot` or GraphViz unavailable.\"\"\"\n    if pydot is None:\n        raise ImportError(\n            'Failed to import `pydot`. '\n            'Please install `pydot`. '\n            'For example with `pip install pydot`.')\n    try:\n        # Attempt to create an image of a blank graph\n        # to check the pydot/graphviz installation.\n        pydot.Dot.create(pydot.Dot())\n    except OSError:\n        raise OSError(\n            '`pydot` failed to call GraphViz.'\n            'Please install GraphViz (https://www.graphviz.org/) '\n            'and ensure that its executables are in the $PATH.')",
        "begin_line": 16,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.utils.vis_utils.model_to_dot#34",
        "src_path": "keras/utils/vis_utils.py",
        "class_name": "keras.utils.vis_utils",
        "signature": "keras.utils.vis_utils.model_to_dot(model, show_shapes=False, show_layer_names=True, rankdir='TB')",
        "snippet": "def model_to_dot(model,\n                 show_shapes=False,\n                 show_layer_names=True,\n                 rankdir='TB'):\n    \"\"\"Convert a Keras model to dot format.\n\n    # Arguments\n        model: A Keras model instance.\n        show_shapes: whether to display shape information.\n        show_layer_names: whether to display layer names.\n        rankdir: `rankdir` argument passed to PyDot,\n            a string specifying the format of the plot:\n            'TB' creates a vertical plot;\n            'LR' creates a horizontal plot.\n\n    # Returns\n        A `pydot.Dot` instance representing the Keras model.\n    \"\"\"\n    from ..layers.wrappers import Wrapper\n    from ..models import Sequential\n\n    _check_pydot()\n    dot = pydot.Dot()\n    dot.set('rankdir', rankdir)\n    dot.set('concentrate', True)\n    dot.set_node_defaults(shape='record')\n\n    if isinstance(model, Sequential):\n        if not model.built:\n            model.build()\n    layers = model.layers\n\n    # Create graph nodes.\n    for layer in layers:\n        layer_id = str(id(layer))\n\n        # Append a wrapped layer's label to node's label, if it exists.\n        layer_name = layer.name\n        class_name = layer.__class__.__name__\n        if isinstance(layer, Wrapper):\n            layer_name = '{}({})'.format(layer_name, layer.layer.name)\n            child_class_name = layer.layer.__class__.__name__\n            class_name = '{}({})'.format(class_name, child_class_name)\n\n        # Create node's label.\n        if show_layer_names:\n            label = '{}: {}'.format(layer_name, class_name)\n        else:\n            label = class_name\n\n        # Rebuild the label as a table including input/output shapes.\n        if show_shapes:\n            try:\n                outputlabels = str(layer.output_shape)\n            except AttributeError:\n                outputlabels = 'multiple'\n            if hasattr(layer, 'input_shape'):\n                inputlabels = str(layer.input_shape)\n            elif hasattr(layer, 'input_shapes'):\n                inputlabels = ', '.join(\n                    [str(ishape) for ishape in layer.input_shapes])\n            else:\n                inputlabels = 'multiple'\n            label = '%s\\n|{input:|output:}|{{%s}|{%s}}' % (label,\n                                                           inputlabels,\n                                                           outputlabels)\n        node = pydot.Node(layer_id, label=label)\n        dot.add_node(node)\n\n    # Connect nodes with edges.\n    for layer in layers:\n        layer_id = str(id(layer))\n        for i, node in enumerate(layer._inbound_nodes):\n            node_key = layer.name + '_ib-' + str(i)\n            if node_key in model._network_nodes:\n                for inbound_layer in node.inbound_layers:\n                    inbound_layer_id = str(id(inbound_layer))\n                    dot.add_edge(pydot.Edge(inbound_layer_id, layer_id))\n    return dot",
        "begin_line": 34,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.utils.vis_utils.plot_model#115",
        "src_path": "keras/utils/vis_utils.py",
        "class_name": "keras.utils.vis_utils",
        "signature": "keras.utils.vis_utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')",
        "snippet": "def plot_model(model,\n               to_file='model.png',\n               show_shapes=False,\n               show_layer_names=True,\n               rankdir='TB'):\n    \"\"\"Converts a Keras model to dot format and save to a file.\n\n    # Arguments\n        model: A Keras model instance\n        to_file: File name of the plot image.\n        show_shapes: whether to display shape information.\n        show_layer_names: whether to display layer names.\n        rankdir: `rankdir` argument passed to PyDot,\n            a string specifying the format of the plot:\n            'TB' creates a vertical plot;\n            'LR' creates a horizontal plot.\n    \"\"\"\n    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)\n    _, extension = os.path.splitext(to_file)\n    if not extension:\n        extension = 'png'\n    else:\n        extension = extension[1:]\n    dot.write(to_file, format=extension)",
        "begin_line": 115,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.get_uid#64",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.get_uid(prefix='')",
        "snippet": "def get_uid(prefix=''):\n    \"\"\"Get the uid for the default graph.\n\n    # Arguments\n        prefix: An optional prefix of the graph.\n\n    # Returns\n        A unique identifier for the graph.\n    \"\"\"\n    global _GRAPH_UID_DICTS\n    graph = tf.get_default_graph()\n    if graph not in _GRAPH_UID_DICTS:\n        _GRAPH_UID_DICTS[graph] = defaultdict(int)\n    _GRAPH_UID_DICTS[graph][prefix] += 1\n    return _GRAPH_UID_DICTS[graph][prefix]",
        "begin_line": 64,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000779423226812159,
            "pseudo_dstar_susp": 0.002188183807439825,
            "pseudo_tarantula_susp": 0.0003869969040247678,
            "pseudo_op2_susp": 0.002188183807439825,
            "pseudo_barinel_susp": 0.0003869969040247678
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.reset_uids#81",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.reset_uids()",
        "snippet": "def reset_uids():\n    \"\"\"Resets graph identifiers.\n    \"\"\"\n    global _GRAPH_UID_DICTS\n    _GRAPH_UID_DICTS = {}",
        "begin_line": 81,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00035650623885918,
            "pseudo_dstar_susp": 0.0008726003490401396,
            "pseudo_tarantula_susp": 0.00028050490883590464,
            "pseudo_op2_susp": 0.0008726003490401396,
            "pseudo_barinel_susp": 0.00028050490883590464
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.clear_session#88",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.clear_session()",
        "snippet": "def clear_session():\n    \"\"\"Destroys the current TF graph and creates a new one.\n\n    Useful to avoid clutter from old models / layers.\n    \"\"\"\n    global _SESSION\n    global _GRAPH_LEARNING_PHASES\n    tf.reset_default_graph()\n    reset_uids()\n    _SESSION = None\n    phase = tf.placeholder_with_default(False,\n                                        shape=(),\n                                        name='keras_learning_phase')\n    _GRAPH_LEARNING_PHASES = {}\n    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase",
        "begin_line": 88,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00035650623885918,
            "pseudo_dstar_susp": 0.0008726003490401396,
            "pseudo_tarantula_susp": 0.00028050490883590464,
            "pseudo_op2_susp": 0.0008726003490401396,
            "pseudo_barinel_susp": 0.00028050490883590464
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.learning_phase#121",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.learning_phase()",
        "snippet": "def learning_phase():\n    \"\"\"Returns the learning phase flag.\n\n    The learning phase flag is a bool tensor (0 = test, 1 = train)\n    to be passed as input to any Keras function\n    that uses a different behavior at train time and test time.\n\n    # Returns\n        Learning phase (scalar integer tensor or Python integer).\n    \"\"\"\n    graph = tf.get_default_graph()\n    if graph not in _GRAPH_LEARNING_PHASES:\n        phase = tf.placeholder_with_default(False,\n                                            shape=(),\n                                            name='keras_learning_phase')\n        _GRAPH_LEARNING_PHASES[graph] = phase\n    return _GRAPH_LEARNING_PHASES[graph]",
        "begin_line": 121,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031826861871419476,
            "pseudo_dstar_susp": 0.00031113876789047915,
            "pseudo_tarantula_susp": 0.0004351610095735422,
            "pseudo_op2_susp": 0.00031113876789047915,
            "pseudo_barinel_susp": 0.0004351610095735422
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.set_learning_phase#140",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.set_learning_phase(value)",
        "snippet": "def set_learning_phase(value):\n    \"\"\"Sets the learning phase to a fixed value.\n\n    # Arguments\n        value: Learning phase value, either 0 or 1 (integers).\n\n    # Raises\n        ValueError: if `value` is neither `0` nor `1`.\n    \"\"\"\n    global _GRAPH_LEARNING_PHASES\n    if value not in {0, 1}:\n        raise ValueError('Expected learning phase to be '\n                         '0 or 1.')\n    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = value",
        "begin_line": 140,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.get_session#156",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.get_session()",
        "snippet": "def get_session():\n    \"\"\"Returns the TF session to be used by the backend.\n\n    If a default TensorFlow session is available, we will return it.\n\n    Else, we will return the global Keras session.\n\n    If no global Keras session exists at this point:\n    we will create a new global session.\n\n    Note that you can manually set the global session\n    via `K.set_session(sess)`.\n\n    # Returns\n        A TensorFlow session.\n    \"\"\"\n    global _SESSION\n\n    default_session = tf.get_default_session()\n\n    if default_session is not None:\n        session = default_session\n    else:\n        if _SESSION is None:\n            if not os.environ.get('OMP_NUM_THREADS'):\n                config = tf.ConfigProto(allow_soft_placement=True)\n            else:\n                num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n                config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n                                        allow_soft_placement=True)\n            _SESSION = tf.Session(config=config)\n        session = _SESSION\n    if not _MANUAL_VAR_INIT:\n        with session.graph.as_default():\n            variables = tf.global_variables()\n            candidate_vars = []\n            for v in variables:\n                if not getattr(v, '_keras_initialized', False):\n                    candidate_vars.append(v)\n            if candidate_vars:\n                # This step is expensive, so we only run it on variables\n                # not already marked as initialized.\n                is_initialized = session.run(\n                    [tf.is_variable_initialized(v) for v in candidate_vars])\n                uninitialized_vars = []\n                for flag, v in zip(is_initialized, candidate_vars):\n                    if not flag:\n                        uninitialized_vars.append(v)\n                    v._keras_initialized = True\n                if uninitialized_vars:\n                    session.run(tf.variables_initializer(uninitialized_vars))\n    # hack for list_devices() function.\n    # list_devices() function is not available under tensorflow r1.3.\n    if not hasattr(session, 'list_devices'):\n        session.list_devices = lambda: device_lib.list_local_devices()\n    return session",
        "begin_line": 156,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0029411764705882353,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.00043047783039173483,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.00043047783039173483
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._TfDeviceCaptureOp.__init__#229",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend._TfDeviceCaptureOp",
        "signature": "keras.backend.tensorflow_backend._TfDeviceCaptureOp.__init__(self)",
        "snippet": "    def __init__(self):\n        self.device = None",
        "begin_line": 229,
        "end_line": 230,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031466331025802394,
            "pseudo_dstar_susp": 0.0003076923076923077,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.0003076923076923077,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._get_current_tf_device#237",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._get_current_tf_device()",
        "snippet": "def _get_current_tf_device():\n    \"\"\"Return explicit device of current context, otherwise returns `None`.\n\n    # Returns\n        If the current device scope is explicitly set, it returns a string with\n        the device (`CPU` or `GPU`). If the scope is not explicitly set, it will\n        return `None`.\n    \"\"\"\n    g = tf.get_default_graph()\n    op = _TfDeviceCaptureOp()\n    g._apply_device_functions(op)\n    return op.device",
        "begin_line": 237,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031466331025802394,
            "pseudo_dstar_susp": 0.0003076923076923077,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.0003076923076923077,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._is_current_explicit_device#251",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._is_current_explicit_device(device_type)",
        "snippet": "def _is_current_explicit_device(device_type):\n    \"\"\"Check if the current device is explicitly set on the device type specified.\n\n    # Arguments\n        device_type: A string containing `GPU` or `CPU` (case-insensitive).\n\n    # Returns\n        A boolean indicating if the current device scope is explicitly set on the device type.\n\n    # Raises\n        ValueError: If the `device_type` string indicates an unsupported device.\n    \"\"\"\n    device_type = device_type.upper()\n    if device_type not in ['CPU', 'GPU']:\n        raise ValueError('`device_type` should be either \"CPU\" or \"GPU\".')\n    device = _get_current_tf_device()\n    return (device is not None and device.device_type == device_type.upper())",
        "begin_line": 251,
        "end_line": 267,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031466331025802394,
            "pseudo_dstar_susp": 0.0003076923076923077,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.0003076923076923077,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._get_available_gpus#270",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._get_available_gpus()",
        "snippet": "def _get_available_gpus():\n    \"\"\"Get a list of available gpu devices (formatted as strings).\n\n    # Returns\n        A list of available GPU devices.\n    \"\"\"\n    global _LOCAL_DEVICES\n    if _LOCAL_DEVICES is None:\n        _LOCAL_DEVICES = get_session().list_devices()\n    return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']",
        "begin_line": 270,
        "end_line": 279,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003671071953010279,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.0006024096385542169,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._has_nchw_support#282",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._has_nchw_support()",
        "snippet": "def _has_nchw_support():\n    \"\"\"Check whether the current scope supports NCHW ops.\n\n    TensorFlow does not support NCHW on CPU. Therefore we check if we are not explicitly put on\n    CPU, and have GPUs available. In this case there will be soft-placing on the GPU device.\n\n    # Returns\n        bool: if the current scope device placement would support nchw\n    \"\"\"\n    explicitly_on_cpu = _is_current_explicit_device('CPU')\n    gpus_available = len(_get_available_gpus()) > 0\n    return (not explicitly_on_cpu and gpus_available)",
        "begin_line": 282,
        "end_line": 293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031466331025802394,
            "pseudo_dstar_susp": 0.0003076923076923077,
            "pseudo_tarantula_susp": 0.0003920031360250882,
            "pseudo_op2_susp": 0.0003076923076923077,
            "pseudo_barinel_susp": 0.0003920031360250882
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._to_tensor#298",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._to_tensor(x, dtype)",
        "snippet": "def _to_tensor(x, dtype):\n    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n\n    # Arguments\n        x: An object to be converted (numpy array, list, tensors).\n        dtype: The destination type.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.convert_to_tensor(x, dtype=dtype)",
        "begin_line": 298,
        "end_line": 308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045433893684688776,
            "pseudo_dstar_susp": 0.0008904719501335708,
            "pseudo_tarantula_susp": 0.0004351610095735422,
            "pseudo_op2_susp": 0.0008904719501335708,
            "pseudo_barinel_susp": 0.0004351610095735422
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_sparse#311",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_sparse(tensor)",
        "snippet": "def is_sparse(tensor):\n    \"\"\"Returns whether a tensor is a sparse tensor.\n\n    # Arguments\n        tensor: A tensor instance.\n\n    # Returns\n        A boolean.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> a = K.placeholder((2, 2), sparse=False)\n        >>> print(K.is_sparse(a))\n        False\n        >>> b = K.placeholder((2, 2), sparse=True)\n        >>> print(K.is_sparse(b))\n        True\n    ```\n    \"\"\"\n    return isinstance(tensor, tf.SparseTensor)",
        "begin_line": 311,
        "end_line": 331,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0029585798816568047,
            "pseudo_dstar_susp": 0.043478260869565216,
            "pseudo_tarantula_susp": 0.0005324813631522897,
            "pseudo_op2_susp": 0.043478260869565216,
            "pseudo_barinel_susp": 0.0005324813631522897
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.to_dense#334",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.to_dense(tensor)",
        "snippet": "def to_dense(tensor):\n    \"\"\"Converts a sparse tensor into a dense tensor and returns it.\n\n    # Arguments\n        tensor: A tensor instance (potentially sparse).\n\n    # Returns\n        A dense tensor.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> b = K.placeholder((2, 2), sparse=True)\n        >>> print(K.is_sparse(b))\n        True\n        >>> c = K.to_dense(b)\n        >>> print(K.is_sparse(c))\n        False\n    ```\n    \"\"\"\n    if is_sparse(tensor):\n        return tf.sparse_tensor_to_dense(tensor)\n    else:\n        return tensor",
        "begin_line": 334,
        "end_line": 357,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002638522427440633,
            "pseudo_dstar_susp": 0.007692307692307693,
            "pseudo_tarantula_susp": 0.0006565988181221273,
            "pseudo_op2_susp": 0.007692307692307693,
            "pseudo_barinel_susp": 0.0006565988181221273
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.variable#363",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.variable(value, dtype=None, name=None, constraint=None)",
        "snippet": "def variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val, dtype='float64', name='example_var')\n        >>> K.dtype(kvar)\n        'float64'\n        >>> print(kvar)\n        example_var\n        >>> K.eval(kvar)\n        array([[ 1.,  2.],\n               [ 3.,  4.]])\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if hasattr(value, 'tocoo'):\n        sparse_coo = value.tocoo()\n        indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n                                  np.expand_dims(sparse_coo.col, 1)), 1)\n        v = tf.SparseTensor(indices=indices,\n                            values=sparse_coo.data,\n                            dense_shape=sparse_coo.shape)\n        v._keras_shape = sparse_coo.shape\n        v._uses_learning_phase = False\n        return v\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n    if isinstance(value, np.ndarray):\n        v._keras_shape = value.shape\n    elif hasattr(value, 'get_shape'):\n        v._keras_shape = int_shape(value)\n    v._uses_learning_phase = False\n    # TODO: move to Variable constructor when supported in public release.\n    try:\n        v.constraint = constraint\n    except AttributeError:\n        v._constraint = constraint\n    return v",
        "begin_line": 363,
        "end_line": 413,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003278688524590164,
            "pseudo_dstar_susp": 0.058823529411764705,
            "pseudo_tarantula_susp": 0.0005803830528148578,
            "pseudo_op2_susp": 0.058823529411764705,
            "pseudo_barinel_susp": 0.0005803830528148578
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.constant#416",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.constant(value, dtype=None, shape=None, name=None)",
        "snippet": "def constant(value, dtype=None, shape=None, name=None):\n    \"\"\"Creates a constant tensor.\n\n    # Arguments\n        value: A constant value (or list)\n        dtype: The type of the elements of the resulting tensor.\n        shape: Optional dimensions of resulting tensor.\n        name: Optional name for the tensor.\n\n    # Returns\n        A Constant Tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    return tf.constant(value, dtype=dtype, shape=shape, name=name)",
        "begin_line": 416,
        "end_line": 430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011098779134295228,
            "pseudo_dstar_susp": 0.005235602094240838,
            "pseudo_tarantula_susp": 0.000473260766682442,
            "pseudo_op2_susp": 0.005235602094240838,
            "pseudo_barinel_susp": 0.000473260766682442
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_keras_tensor#433",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_keras_tensor(x)",
        "snippet": "def is_keras_tensor(x):\n    \"\"\"Returns whether `x` is a Keras tensor.\n\n    A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n    (`Layer` class) or by `Input`.\n\n    # Arguments\n        x: A candidate tensor.\n\n    # Returns\n        A boolean: Whether the argument is a Keras tensor.\n\n    # Raises\n        ValueError: In case `x` is not a symbolic tensor.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> from keras.layers import Input, Dense\n        >>> np_var = numpy.array([1, 2])\n        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n        ValueError\n        >>> k_var = tf.placeholder('float32', shape=(1,1))\n        >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n        False\n        >>> keras_var = K.variable(np_var)\n        >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n        False\n        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n        False\n        >>> keras_input = Input([10])\n        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n        True\n        >>> keras_layer_output = Dense(10)(keras_input)\n        >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n        True\n    ```\n    \"\"\"\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')",
        "begin_line": 433,
        "end_line": 476,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006906077348066298,
            "pseudo_dstar_susp": 0.0016611295681063123,
            "pseudo_tarantula_susp": 0.000362844702467344,
            "pseudo_op2_susp": 0.0016611295681063123,
            "pseudo_barinel_susp": 0.000362844702467344
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_tensor#479",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_tensor(x)",
        "snippet": "def is_tensor(x):\n    return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)",
        "begin_line": 479,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006578947368421052,
            "pseudo_dstar_susp": 0.0015290519877675841,
            "pseudo_tarantula_susp": 0.00035385704175513094,
            "pseudo_op2_susp": 0.0015290519877675841,
            "pseudo_barinel_susp": 0.00035385704175513094
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.placeholder#483",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)",
        "snippet": "def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n    \"\"\"Instantiates a placeholder tensor and returns it.\n\n    # Arguments\n        shape: Shape of the placeholder\n            (integer tuple, may include `None` entries).\n        ndim: Number of axes of the tensor.\n            At least one of {`shape`, `ndim`} must be specified.\n            If both are specified, `shape` is used.\n        dtype: Placeholder type.\n        sparse: Boolean, whether the placeholder should have a sparse type.\n        name: Optional name string for the placeholder.\n\n    # Returns\n        Tensor instance (with Keras metadata included).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> input_ph = K.placeholder(shape=(2, 4, 5))\n        >>> input_ph._keras_shape\n        (2, 4, 5)\n        >>> input_ph\n        <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n    if sparse:\n        x = tf.sparse_placeholder(dtype, shape=shape, name=name)\n    else:\n        x = tf.placeholder(dtype, shape=shape, name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    return x",
        "begin_line": 483,
        "end_line": 520,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014619883040935672,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.0005787037037037037,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.0005787037037037037
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_placeholder#523",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_placeholder(x)",
        "snippet": "def is_placeholder(x):\n    \"\"\"Returns whether `x` is a placeholder.\n\n    # Arguments\n        x: A candidate placeholder.\n\n    # Returns\n        Boolean.\n    \"\"\"\n    try:\n        return x.op.type == 'Placeholder'\n    except AttributeError:\n        return False",
        "begin_line": 523,
        "end_line": 535,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.shape#538",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.shape(x)",
        "snippet": "def shape(x):\n    \"\"\"Returns the symbolic shape of a tensor or variable.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A symbolic shape (which is itself a tensor).\n\n    # Examples\n    ```python\n        # TensorFlow example\n        >>> from keras import backend as K\n        >>> tf_session = K.get_session()\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))\n        >>> K.shape(kvar)\n        <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n        >>> K.shape(inputs)\n        <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n        # To get integer shape (Instead, you can use K.int_shape(x))\n        >>> K.shape(kvar).eval(session=tf_session)\n        array([2, 2], dtype=int32)\n        >>> K.shape(inputs).eval(session=tf_session)\n        array([2, 4, 5], dtype=int32)\n    ```\n    \"\"\"\n    return tf.shape(x)",
        "begin_line": 538,
        "end_line": 566,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045004500450045,
            "pseudo_dstar_susp": 0.0006257822277847309,
            "pseudo_tarantula_susp": 0.0006035003017501509,
            "pseudo_op2_susp": 0.0006257822277847309,
            "pseudo_barinel_susp": 0.0006035003017501509
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.int_shape#569",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.int_shape(x)",
        "snippet": "def int_shape(x):\n    \"\"\"Returns the shape of tensor or variable as a tuple of int or None entries.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tuple of integers (or None entries).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> inputs = K.placeholder(shape=(2, 4, 5))\n        >>> K.int_shape(inputs)\n        (2, 4, 5)\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> K.int_shape(kvar)\n        (2, 2)\n    ```\n    \"\"\"\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n    try:\n        return tuple(x.get_shape().as_list())\n    except ValueError:\n        return None",
        "begin_line": 569,
        "end_line": 595,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.037037037037037035,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.037037037037037035,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ndim#598",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ndim(x)",
        "snippet": "def ndim(x):\n    \"\"\"Returns the number of axes in a tensor, as an integer.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        Integer (scalar), number of axes.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> inputs = K.placeholder(shape=(2, 4, 5))\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> K.ndim(inputs)\n        3\n        >>> K.ndim(kvar)\n        2\n    ```\n    \"\"\"\n    dims = x.get_shape()._dims\n    if dims is not None:\n        return len(dims)\n    return None",
        "begin_line": 598,
        "end_line": 622,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0023752969121140144,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0023752969121140144,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.dtype#625",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.dtype(x)",
        "snippet": "def dtype(x):\n    \"\"\"Returns the dtype of a Keras tensor or variable, as a string.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        String, dtype of `x`.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> K.dtype(K.placeholder(shape=(2,4,5)))\n        'float32'\n        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n        'float32'\n        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n        'float64'\n        # Keras variable\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))\n        >>> K.dtype(kvar)\n        'float32_ref'\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n        >>> K.dtype(kvar)\n        'float32_ref'\n    ```\n    \"\"\"\n    return x.dtype.base_dtype.name",
        "begin_line": 625,
        "end_line": 652,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005027652086475615,
            "pseudo_dstar_susp": 0.0013568521031207597,
            "pseudo_tarantula_susp": 0.0003498950314905528,
            "pseudo_op2_susp": 0.0013568521031207597,
            "pseudo_barinel_susp": 0.0003498950314905528
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.eval#655",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.eval(x)",
        "snippet": "def eval(x):\n    \"\"\"Evaluates the value of a variable.\n\n    # Arguments\n        x: A variable.\n\n    # Returns\n        A Numpy array.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n        >>> K.eval(kvar)\n        array([[ 1.,  2.],\n               [ 3.,  4.]], dtype=float32)\n    ```\n    \"\"\"\n    return to_dense(x).eval(session=get_session())",
        "begin_line": 655,
        "end_line": 673,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0031645569620253164,
            "pseudo_dstar_susp": 0.0013908205841446453,
            "pseudo_tarantula_susp": 0.0007604562737642585,
            "pseudo_op2_susp": 0.0013908205841446453,
            "pseudo_barinel_susp": 0.0007604562737642585
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.zeros#676",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.zeros(shape, dtype=None, name=None)",
        "snippet": "def zeros(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-zeros variable and returns it.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable\n        dtype: String, data type of returned Keras variable\n        name: String, name of returned Keras variable\n\n    # Returns\n        A variable (including Keras metadata), filled with `0.0`.\n        Note that if `shape` was symbolic, we cannot return a variable,\n        and will return a dynamically-shaped tensor instead.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.zeros((3,4))\n        >>> K.eval(kvar)\n        array([[ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.]], dtype=float32)\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    tf_dtype = tf.as_dtype(dtype)\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n    if py_all(v.get_shape().as_list()):\n        return variable(v, dtype=dtype, name=name)\n    return v",
        "begin_line": 676,
        "end_line": 705,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033311125916055963,
            "pseudo_dstar_susp": 0.0005777007510109763,
            "pseudo_tarantula_susp": 0.0003169572107765452,
            "pseudo_op2_susp": 0.0005777007510109763,
            "pseudo_barinel_susp": 0.0003169572107765452
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ones#708",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ones(shape, dtype=None, name=None)",
        "snippet": "def ones(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-ones variable and returns it.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable.\n        dtype: String, data type of returned Keras variable.\n        name: String, name of returned Keras variable.\n\n    # Returns\n        A Keras variable, filled with `1.0`.\n        Note that if `shape` was symbolic, we cannot return a variable,\n        and will return a dynamically-shaped tensor instead.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.ones((3,4))\n        >>> K.eval(kvar)\n        array([[ 1.,  1.,  1.,  1.],\n               [ 1.,  1.,  1.,  1.],\n               [ 1.,  1.,  1.,  1.]], dtype=float32)\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    tf_dtype = tf.as_dtype(dtype)\n    v = tf.ones(shape=shape, dtype=tf_dtype, name=name)\n    if py_all(v.get_shape().as_list()):\n        return variable(v, dtype=dtype, name=name)\n    return v",
        "begin_line": 708,
        "end_line": 737,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.eye#740",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.eye(size, dtype=None, name=None)",
        "snippet": "def eye(size, dtype=None, name=None):\n    \"\"\"Instantiate an identity matrix and returns it.\n\n    # Arguments\n        size: Integer, number of rows/columns.\n        dtype: String, data type of returned Keras variable.\n        name: String, name of returned Keras variable.\n\n    # Returns\n        A Keras variable, an identity matrix.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.eye(3)\n        >>> K.eval(kvar)\n        array([[ 1.,  0.,  0.],\n               [ 0.,  1.,  0.],\n               [ 0.,  0.,  1.]], dtype=float32)\n    ```\n\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    tf_dtype = tf.as_dtype(dtype)\n    return variable(tf.eye(size, dtype=tf_dtype), dtype, name)",
        "begin_line": 740,
        "end_line": 765,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.zeros_like#768",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.zeros_like(x, dtype=None, name=None)",
        "snippet": "def zeros_like(x, dtype=None, name=None):\n    \"\"\"Instantiates an all-zeros variable of the same shape as another tensor.\n\n    # Arguments\n        x: Keras variable or Keras tensor.\n        dtype: String, dtype of returned Keras variable.\n             None uses the dtype of x.\n        name: String, name for the variable to create.\n\n    # Returns\n        A Keras variable with the shape of x filled with zeros.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.variable(np.random.random((2,3)))\n        >>> kvar_zeros = K.zeros_like(kvar)\n        >>> K.eval(kvar_zeros)\n        array([[ 0.,  0.,  0.],\n               [ 0.,  0.,  0.]], dtype=float32)\n    ```\n    \"\"\"\n    return tf.zeros_like(x, dtype=dtype, name=name)",
        "begin_line": 768,
        "end_line": 790,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.971023593792052e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ones_like#793",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ones_like(x, dtype=None, name=None)",
        "snippet": "def ones_like(x, dtype=None, name=None):\n    \"\"\"Instantiates an all-ones variable of the same shape as another tensor.\n\n    # Arguments\n        x: Keras variable or tensor.\n        dtype: String, dtype of returned Keras variable.\n             None uses the dtype of x.\n        name: String, name for the variable to create.\n\n    # Returns\n        A Keras variable with the shape of x filled with ones.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.variable(np.random.random((2,3)))\n        >>> kvar_ones = K.ones_like(kvar)\n        >>> K.eval(kvar_ones)\n        array([[ 1.,  1.,  1.],\n               [ 1.,  1.,  1.]], dtype=float32)\n    ```\n    \"\"\"\n    return tf.ones_like(x, dtype=dtype, name=name)",
        "begin_line": 793,
        "end_line": 815,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.identity#818",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.identity(x, name=None)",
        "snippet": "def identity(x, name=None):\n    \"\"\"Returns a tensor with the same content as the input tensor.\n\n    # Arguments\n        x: The input tensor.\n        name: String, name for the variable to create.\n\n    # Returns\n        A tensor of the same shape, type and content.\n    \"\"\"\n    return tf.identity(x, name)",
        "begin_line": 818,
        "end_line": 828,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.random_uniform_variable#831",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)",
        "snippet": "def random_uniform_variable(shape, low, high, dtype=None,\n                            name=None, seed=None):\n    \"\"\"Instantiates a variable with values drawn from a uniform distribution.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable.\n        low: Float, lower boundary of the output interval.\n        high: Float, upper boundary of the output interval.\n        dtype: String, dtype of returned Keras variable.\n        name: String, name of returned Keras variable.\n        seed: Integer, random seed.\n\n    # Returns\n        A Keras variable, filled with drawn samples.\n\n    # Example\n    ```python\n        # TensorFlow example\n        >>> kvar = K.random_uniform_variable((2,3), 0, 1)\n        >>> kvar\n        <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>\n        >>> K.eval(kvar)\n        array([[ 0.10940075,  0.10047495,  0.476143  ],\n               [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    tf_dtype = tf.as_dtype(dtype)\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e8)\n    value = tf.random_uniform_initializer(\n        low, high, dtype=tf_dtype, seed=seed)(shape)\n    return variable(value, dtype=dtype, name=name)",
        "begin_line": 831,
        "end_line": 865,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.random_normal_variable#868",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)",
        "snippet": "def random_normal_variable(shape, mean, scale, dtype=None,\n                           name=None, seed=None):\n    \"\"\"Instantiates a variable with values drawn from a normal distribution.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable.\n        mean: Float, mean of the normal distribution.\n        scale: Float, standard deviation of the normal distribution.\n        dtype: String, dtype of returned Keras variable.\n        name: String, name of returned Keras variable.\n        seed: Integer, random seed.\n\n    # Returns\n        A Keras variable, filled with drawn samples.\n\n    # Example\n    ```python\n        # TensorFlow example\n        >>> kvar = K.random_normal_variable((2,3), 0, 1)\n        >>> kvar\n        <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>\n        >>> K.eval(kvar)\n        array([[ 1.19591331,  0.68685907, -0.63814116],\n               [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    tf_dtype = tf.as_dtype(dtype)\n    if seed is None:\n        # ensure that randomness is conditioned by the Numpy RNG\n        seed = np.random.randint(10e8)\n    value = tf.random_normal_initializer(\n        mean, scale, dtype=tf_dtype, seed=seed)(shape)\n    return variable(value, dtype=dtype, name=name)",
        "begin_line": 868,
        "end_line": 902,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.count_params#905",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.count_params(x)",
        "snippet": "def count_params(x):\n    \"\"\"Returns the static number of elements in a Keras variable or tensor.\n\n    # Arguments\n        x: Keras variable or tensor.\n\n    # Returns\n        Integer, the number of elements in `x`, i.e., the product of the\n        array's static dimensions.\n\n    # Example\n    ```python\n        >>> kvar = K.zeros((2,3))\n        >>> K.count_params(kvar)\n        6\n        >>> K.eval(kvar)\n        array([[ 0.,  0.,  0.],\n               [ 0.,  0.,  0.]], dtype=float32)\n    ```\n    \"\"\"\n    return np.prod(int_shape(x))",
        "begin_line": 905,
        "end_line": 925,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001937984496124031,
            "pseudo_dstar_susp": 0.0006863417982155113,
            "pseudo_tarantula_susp": 0.0016051364365971107,
            "pseudo_op2_susp": 0.0006863417982155113,
            "pseudo_barinel_susp": 0.0016051364365971107
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.cast#928",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.cast(x, dtype)",
        "snippet": "def cast(x, dtype):\n    \"\"\"Casts a tensor to a different dtype and returns it.\n\n    You can cast a Keras variable but it still returns a Keras tensor.\n\n    # Arguments\n        x: Keras tensor (or variable).\n        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n\n    # Returns\n        Keras tensor with dtype `dtype`.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> input = K.placeholder((2, 3), dtype='float32')\n        >>> input\n        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n        # It doesn't work in-place as below.\n        >>> K.cast(input, dtype='float16')\n        <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>\n        >>> input\n        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n        # you need to assign it.\n        >>> input = K.cast(input, dtype='float16')\n        >>> input\n        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>\n    ```\n    \"\"\"\n    return tf.cast(x, dtype)",
        "begin_line": 928,
        "end_line": 957,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009699321047526673,
            "pseudo_dstar_susp": 0.0012468827930174563,
            "pseudo_tarantula_susp": 0.0005506607929515419,
            "pseudo_op2_susp": 0.0012468827930174563,
            "pseudo_barinel_susp": 0.0005518763796909492
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.update#963",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.update(x, new_x)",
        "snippet": "def update(x, new_x):\n    \"\"\"Update the value of `x` to `new_x`.\n\n    # Arguments\n        x: A `Variable`.\n        new_x: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf.assign(x, new_x)",
        "begin_line": 963,
        "end_line": 973,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034494653328734045,
            "pseudo_dstar_susp": 0.0005875440658049354,
            "pseudo_tarantula_susp": 0.0003198976327575176,
            "pseudo_op2_susp": 0.0005875440658049354,
            "pseudo_barinel_susp": 0.0003198976327575176
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.update_add#976",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.update_add(x, increment)",
        "snippet": "def update_add(x, increment):\n    \"\"\"Update the value of `x` by adding `increment`.\n\n    # Arguments\n        x: A `Variable`.\n        increment: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf.assign_add(x, increment)",
        "begin_line": 976,
        "end_line": 986,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003301419610432486,
            "pseudo_dstar_susp": 0.0005688282138794084,
            "pseudo_tarantula_susp": 0.00031426775612822125,
            "pseudo_op2_susp": 0.0005688282138794084,
            "pseudo_barinel_susp": 0.00031426775612822125
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.moving_average_update#1002",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.moving_average_update(x, value, momentum)",
        "snippet": "def moving_average_update(x, value, momentum):\n    \"\"\"Compute the moving average of a variable.\n\n    # Arguments\n        x: A `Variable`.\n        value: A tensor with the same shape as `x`.\n        momentum: The moving average momentum.\n\n    # Returns\n        An operation to update the variable.\n    \"\"\"\n    return moving_averages.assign_moving_average(\n        x, value, momentum, zero_debias=True)",
        "begin_line": 1002,
        "end_line": 1014,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000407000407000407,
            "pseudo_dstar_susp": 0.0003486750348675035,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.0003486750348675035,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.dot#1019",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.dot(x, y)",
        "snippet": "def dot(x, y):\n    \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n\n    When attempting to multiply a nD tensor\n    with a nD tensor, it reproduces the Theano behavior.\n    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor, dot product of `x` and `y`.\n\n    # Examples\n    ```python\n        # dot product between tensors\n        >>> x = K.placeholder(shape=(2, 3))\n        >>> y = K.placeholder(shape=(3, 4))\n        >>> xy = K.dot(x, y)\n        >>> xy\n        <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n    ```\n\n    ```python\n        # dot product between tensors\n        >>> x = K.placeholder(shape=(32, 28, 3))\n        >>> y = K.placeholder(shape=(3, 4))\n        >>> xy = K.dot(x, y)\n        >>> xy\n        <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>\n    ```\n\n    ```python\n        # Theano-like behavior example\n        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n        >>> y = K.ones((4, 3, 5))\n        >>> xy = K.dot(x, y)\n        >>> K.int_shape(xy)\n        (2, 4, 5)\n    ```\n    \"\"\"\n    if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n        x_shape = []\n        for i, s in zip(int_shape(x), tf.unstack(tf.shape(x))):\n            if i is not None:\n                x_shape.append(i)\n            else:\n                x_shape.append(s)\n        x_shape = tuple(x_shape)\n        y_shape = []\n        for i, s in zip(int_shape(y), tf.unstack(tf.shape(y))):\n            if i is not None:\n                y_shape.append(i)\n            else:\n                y_shape.append(s)\n        y_shape = tuple(y_shape)\n        y_permute_dim = list(range(ndim(y)))\n        y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\n        xt = tf.reshape(x, [-1, x_shape[-1]])\n        yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])\n        return tf.reshape(tf.matmul(xt, yt),\n                          x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n    if is_sparse(x):\n        out = tf.sparse_tensor_dense_matmul(x, y)\n    else:\n        out = tf.matmul(x, y)\n    return out",
        "begin_line": 1019,
        "end_line": 1086,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001876172607879925,
            "pseudo_dstar_susp": 0.007352941176470588,
            "pseudo_tarantula_susp": 0.0005896226415094339,
            "pseudo_op2_susp": 0.007352941176470588,
            "pseudo_barinel_susp": 0.0005896226415094339
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.batch_dot#1089",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.batch_dot(x, y, axes=None)",
        "snippet": "def batch_dot(x, y, axes=None):\n    \"\"\"Batchwise dot product.\n\n    `batch_dot` is used to compute dot product of `x` and `y` when\n    `x` and `y` are data in batch, i.e. in a shape of\n    `(batch_size, :)`.\n    `batch_dot` results in a tensor or variable with less dimensions\n    than the input. If the number of dimensions is reduced to 1,\n    we use `expand_dims` to make sure that ndim is at least 2.\n\n    # Arguments\n        x: Keras tensor or variable with `ndim >= 2`.\n        y: Keras tensor or variable with `ndim >= 2`.\n        axes: list of (or single) int with target dimensions.\n            The lengths of `axes[0]` and `axes[1]` should be the same.\n\n    # Returns\n        A tensor with shape equal to the concatenation of `x`'s shape\n        (less the dimension that was summed over) and `y`'s shape\n        (less the batch dimension and the dimension that was summed over).\n        If the final rank is 1, we reshape it to `(batch_size, 1)`.\n\n    # Examples\n        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n        `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal\n        of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n        elements.\n\n        Shape inference:\n        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n        If `axes` is (1, 2), to find the output shape of resultant tensor,\n            loop through each dimension in `x`'s shape and `y`'s shape:\n\n        * `x.shape[0]` : 100 : append to output shape\n        * `x.shape[1]` : 20 : do not append to output shape,\n            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n        * `y.shape[0]` : 100 : do not append to output shape,\n            always ignore first dimension of `y`\n        * `y.shape[1]` : 30 : append to output shape\n        * `y.shape[2]` : 20 : do not append to output shape,\n            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n        `output_shape` = `(100, 30)`\n\n    ```python\n        >>> x_batch = K.ones(shape=(32, 20, 1))\n        >>> y_batch = K.ones(shape=(32, 30, 20))\n        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n        >>> K.int_shape(xy_batch_dot)\n        (32, 1, 30)\n    ```\n    \"\"\"\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    x_ndim = ndim(x)\n    y_ndim = ndim(y)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [x_ndim - 1, y_ndim - 2]\n    if py_any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError('Multiple target dimensions are not supported. ' +\n                         'Expected: None, int, (int, int), ' +\n                         'Provided: ' + str(axes))\n    if x_ndim > y_ndim:\n        diff = x_ndim - y_ndim\n        y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))\n    elif y_ndim > x_ndim:\n        diff = y_ndim - x_ndim\n        x = tf.reshape(x, tf.concat([tf.shape(x), [1] * (diff)], axis=0))\n    else:\n        diff = 0\n    if ndim(x) == 2 and ndim(y) == 2:\n        if axes[0] == axes[1]:\n            out = tf.reduce_sum(tf.multiply(x, y), axes[0])\n        else:\n            out = tf.reduce_sum(tf.multiply(tf.transpose(x, [1, 0]), y), axes[1])\n    else:\n        if axes is not None:\n            adj_x = None if axes[0] == ndim(x) - 1 else True\n            adj_y = True if axes[1] == ndim(y) - 1 else None\n        else:\n            adj_x = None\n            adj_y = None\n        out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n    if diff:\n        if x_ndim > y_ndim:\n            idx = x_ndim + y_ndim - 3\n        else:\n            idx = x_ndim - 1\n        out = tf.squeeze(out, list(range(idx, idx + diff)))\n    if ndim(out) == 1:\n        out = expand_dims(out, 1)\n    return out",
        "begin_line": 1089,
        "end_line": 1180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.transpose#1183",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.transpose(x)",
        "snippet": "def transpose(x):\n    \"\"\"Transposes a tensor and returns it.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n\n    # Examples\n    ```python\n        >>> var = K.variable([[1, 2, 3], [4, 5, 6]])\n        >>> K.eval(var)\n        array([[ 1.,  2.,  3.],\n               [ 4.,  5.,  6.]], dtype=float32)\n        >>> var_transposed = K.transpose(var)\n        >>> K.eval(var_transposed)\n        array([[ 1.,  4.],\n               [ 2.,  5.],\n               [ 3.,  6.]], dtype=float32)\n    ```\n\n    ```python\n        >>> inputs = K.placeholder((2, 3))\n        >>> inputs\n        <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>\n        >>> input_transposed = K.transpose(inputs)\n        >>> input_transposed\n        <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n\n    ```\n    \"\"\"\n    return tf.transpose(x)",
        "begin_line": 1183,
        "end_line": 1215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.gather#1218",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.gather(reference, indices)",
        "snippet": "def gather(reference, indices):\n    \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n\n    # Arguments\n        reference: A tensor.\n        indices: An integer tensor of indices.\n\n    # Returns\n        A tensor of same type as `reference`.\n    \"\"\"\n    return tf.nn.embedding_lookup(reference, indices)",
        "begin_line": 1218,
        "end_line": 1228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000100999899000101,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.max#1234",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.max(x, axis=None, keepdims=False)",
        "snippet": "def max(x, axis=None, keepdims=False):\n    \"\"\"Maximum value in a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to find maximum values. If `None` (default), finds the\n            maximum over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with maximum values of `x`.\n    \"\"\"\n    return tf.reduce_max(x, axis, keepdims)",
        "begin_line": 1234,
        "end_line": 1250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000407000407000407,
            "pseudo_dstar_susp": 0.0003486750348675035,
            "pseudo_tarantula_susp": 0.0008051529790660225,
            "pseudo_op2_susp": 0.0003486750348675035,
            "pseudo_barinel_susp": 0.0008051529790660225
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.min#1253",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.min(x, axis=None, keepdims=False)",
        "snippet": "def min(x, axis=None, keepdims=False):\n    \"\"\"Minimum value in a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to find minimum values. If `None` (default), finds the\n            minimum over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with miminum values of `x`.\n    \"\"\"\n    return tf.reduce_min(x, axis, keepdims)",
        "begin_line": 1253,
        "end_line": 1269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.sum#1272",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.sum(x, axis=None, keepdims=False)",
        "snippet": "def sum(x, axis=None, keepdims=False):\n    \"\"\"Sum of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to sum over. If `None` (default), sums over all\n            dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with sum of `x`.\n    \"\"\"\n    return tf.reduce_sum(x, axis, keepdims)",
        "begin_line": 1272,
        "end_line": 1288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.802042073761112e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.prod#1291",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.prod(x, axis=None, keepdims=False)",
        "snippet": "def prod(x, axis=None, keepdims=False):\n    \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the product. If `None` (default), computes\n            the product over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the product of elements of `x`.\n    \"\"\"\n    return tf.reduce_prod(x, axis, keepdims)",
        "begin_line": 1291,
        "end_line": 1307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009970089730807576,
            "pseudo_dstar_susp": 0.0006747638326585695,
            "pseudo_tarantula_susp": 0.0011111111111111111,
            "pseudo_op2_susp": 0.0006747638326585695,
            "pseudo_barinel_susp": 0.0011111111111111111
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.cumsum#1310",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.cumsum(x, axis=0)",
        "snippet": "def cumsum(x, axis=0):\n    \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the sum.\n\n    # Returns\n        A tensor of the cumulative sum of values of `x` along `axis`.\n    \"\"\"\n    return tf.cumsum(x, axis=axis)",
        "begin_line": 1310,
        "end_line": 1320,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.cumprod#1323",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.cumprod(x, axis=0)",
        "snippet": "def cumprod(x, axis=0):\n    \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the product.\n\n    # Returns\n        A tensor of the cumulative product of values of `x` along `axis`.\n    \"\"\"\n    return tf.cumprod(x, axis=axis)",
        "begin_line": 1323,
        "end_line": 1333,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.var#1336",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.var(x, axis=None, keepdims=False)",
        "snippet": "def var(x, axis=None, keepdims=False):\n    \"\"\"Variance of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the variance. If `None` (default), computes\n            the variance over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the variance of elements of `x`.\n    \"\"\"\n    if x.dtype.base_dtype == tf.bool:\n        x = tf.cast(x, floatx())\n    m = tf.reduce_mean(x, axis, True)\n    devs_squared = tf.square(x - m)\n    return tf.reduce_mean(devs_squared,\n                          axis,\n                          keepdims)",
        "begin_line": 1336,
        "end_line": 1358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.std#1361",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.std(x, axis=None, keepdims=False)",
        "snippet": "def std(x, axis=None, keepdims=False):\n    \"\"\"Standard deviation of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the standard deviation. If `None` (default),\n            computes the standard deviation over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the standard deviation of elements of `x`.\n    \"\"\"\n    return tf.sqrt(var(x, axis=axis, keepdims=keepdims))",
        "begin_line": 1361,
        "end_line": 1377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.mean#1380",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.mean(x, axis=None, keepdims=False)",
        "snippet": "def mean(x, axis=None, keepdims=False):\n    \"\"\"Mean of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the mean. If `None` (default), computes\n            the mean over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1 for each entry in `axis`. If `keepdims` is `True`,\n            the reduced dimensions are retained with length 1.\n\n    # Returns\n        A tensor with the mean of elements of `x`.\n    \"\"\"\n    if x.dtype.base_dtype == tf.bool:\n        x = tf.cast(x, floatx())\n    return tf.reduce_mean(x, axis, keepdims)",
        "begin_line": 1380,
        "end_line": 1398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014245014245014246,
            "pseudo_dstar_susp": 0.0013831258644536654,
            "pseudo_tarantula_susp": 0.0005793742757821553,
            "pseudo_op2_susp": 0.0013831258644536654,
            "pseudo_barinel_susp": 0.0005793742757821553
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.any#1401",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.any(x, axis=None, keepdims=False)",
        "snippet": "def any(x, axis=None, keepdims=False):\n    \"\"\"Bitwise reduction (logical OR).\n\n    # Arguments\n        x: Tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the logical or. If `None` (default), computes\n            the logical or over all dimensions.\n        keepdims: whether the drop or broadcast the reduction axes.\n\n    # Returns\n        A uint8 tensor (0s and 1s).\n    \"\"\"\n    x = tf.cast(x, tf.bool)\n    return tf.reduce_any(x, axis, keepdims)",
        "begin_line": 1401,
        "end_line": 1415,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.all#1418",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.all(x, axis=None, keepdims=False)",
        "snippet": "def all(x, axis=None, keepdims=False):\n    \"\"\"Bitwise reduction (logical AND).\n\n    # Arguments\n        x: Tensor or variable.\n        axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the logical and. If `None` (default), computes\n            the logical and over all dimensions.\n        keepdims: whether the drop or broadcast the reduction axes.\n\n    # Returns\n        A uint8 tensor (0s and 1s).\n    \"\"\"\n    x = tf.cast(x, tf.bool)\n    return tf.reduce_all(x, axis, keepdims)",
        "begin_line": 1418,
        "end_line": 1432,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.argmax#1435",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.argmax(x, axis=-1)",
        "snippet": "def argmax(x, axis=-1):\n    \"\"\"Returns the index of the maximum value along an axis.\n\n    # Arguments\n        x: Tensor or variable.\n        axis: axis along which to perform the reduction.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.argmax(x, axis)",
        "begin_line": 1435,
        "end_line": 1445,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000689655172413793,
            "pseudo_dstar_susp": 0.0008305647840531562,
            "pseudo_tarantula_susp": 0.0006706908115358819,
            "pseudo_op2_susp": 0.0008305647840531562,
            "pseudo_barinel_susp": 0.0006697923643670462
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.square#1461",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.square(x)",
        "snippet": "def square(x):\n    \"\"\"Element-wise square.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.square(x)",
        "begin_line": 1461,
        "end_line": 1470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003773584905660377,
            "pseudo_dstar_susp": 0.0007942811755361397,
            "pseudo_tarantula_susp": 0.0003254149040026033,
            "pseudo_op2_susp": 0.0007942811755361397,
            "pseudo_barinel_susp": 0.0003254149040026033
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.abs#1473",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.abs(x)",
        "snippet": "def abs(x):\n    \"\"\"Element-wise absolute value.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.abs(x)",
        "begin_line": 1473,
        "end_line": 1482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.164222873900293e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.sqrt#1485",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.sqrt(x)",
        "snippet": "def sqrt(x):\n    \"\"\"Element-wise square root.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    zero = _to_tensor(0., x.dtype.base_dtype)\n    inf = _to_tensor(np.inf, x.dtype.base_dtype)\n    x = tf.clip_by_value(x, zero, inf)\n    return tf.sqrt(x)",
        "begin_line": 1485,
        "end_line": 1497,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002756339581036384,
            "pseudo_dstar_susp": 0.0002754062241806665,
            "pseudo_tarantula_susp": 0.0002871912693854107,
            "pseudo_op2_susp": 0.0002754062241806665,
            "pseudo_barinel_susp": 0.0002871912693854107
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.exp#1500",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.exp(x)",
        "snippet": "def exp(x):\n    \"\"\"Element-wise exponential.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.exp(x)",
        "begin_line": 1500,
        "end_line": 1509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.log#1512",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.log(x)",
        "snippet": "def log(x):\n    \"\"\"Element-wise log.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.log(x)",
        "begin_line": 1512,
        "end_line": 1521,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.logsumexp#1524",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.logsumexp(x, axis=None, keepdims=False)",
        "snippet": "def logsumexp(x, axis=None, keepdims=False):\n    \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n\n    This function is more numerically stable than log(sum(exp(x))).\n    It avoids overflows caused by taking the exp of large inputs and\n    underflows caused by taking the log of small inputs.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: axis: An integer or list of integers in [-rank(x), rank(x)),\n            the axes to compute the logsumexp. If `None` (default), computes\n            the logsumexp over all dimensions.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`, the reduced dimension is\n            retained with length 1.\n\n    # Returns\n        The reduced tensor.\n    \"\"\"\n    return tf.reduce_logsumexp(x, axis, keepdims)",
        "begin_line": 1524,
        "end_line": 1544,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042444821731748726,
            "pseudo_dstar_susp": 0.0003599712023038157,
            "pseudo_tarantula_susp": 0.0009017132551848512,
            "pseudo_op2_susp": 0.0003599712023038157,
            "pseudo_barinel_susp": 0.0009017132551848512
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.round#1547",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.round(x)",
        "snippet": "def round(x):\n    \"\"\"Element-wise rounding to the closest integer.\n\n    In case of tie, the rounding mode used is \"half to even\".\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.round(x)",
        "begin_line": 1547,
        "end_line": 1558,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.pow#1573",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.pow(x, a)",
        "snippet": "def pow(x, a):\n    \"\"\"Element-wise exponentiation.\n\n    # Arguments\n        x: Tensor or variable.\n        a: Python integer.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.pow(x, a)",
        "begin_line": 1573,
        "end_line": 1583,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.244707405010632e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.clip#1586",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.clip(x, min_value, max_value)",
        "snippet": "def clip(x, min_value, max_value):\n    \"\"\"Element-wise value clipping.\n\n    # Arguments\n        x: Tensor or variable.\n        min_value: Python float or integer.\n        max_value: Python float or integer.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if max_value is not None and max_value < min_value:\n        max_value = min_value\n    if max_value is None:\n        max_value = np.inf\n    min_value = _to_tensor(min_value, x.dtype.base_dtype)\n    max_value = _to_tensor(max_value, x.dtype.base_dtype)\n    return tf.clip_by_value(x, min_value, max_value)",
        "begin_line": 1586,
        "end_line": 1603,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.equal#1606",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.equal(x, y)",
        "snippet": "def equal(x, y):\n    \"\"\"Element-wise equality between two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n    \"\"\"\n    return tf.equal(x, y)",
        "begin_line": 1606,
        "end_line": 1616,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006277463904582549,
            "pseudo_dstar_susp": 0.0008237232289950577,
            "pseudo_tarantula_susp": 0.0006557377049180328,
            "pseudo_op2_susp": 0.0008237232289950577,
            "pseudo_barinel_susp": 0.0006557377049180328
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.not_equal#1619",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.not_equal(x, y)",
        "snippet": "def not_equal(x, y):\n    \"\"\"Element-wise inequality between two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n    \"\"\"\n    return tf.not_equal(x, y)",
        "begin_line": 1619,
        "end_line": 1629,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005402485143165856,
            "pseudo_dstar_susp": 0.0009505703422053232,
            "pseudo_tarantula_susp": 0.00048262548262548264,
            "pseudo_op2_susp": 0.0009505703422053232,
            "pseudo_barinel_susp": 0.00048262548262548264
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.greater#1632",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.greater(x, y)",
        "snippet": "def greater(x, y):\n    \"\"\"Element-wise truth value of (x > y).\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n    \"\"\"\n    return tf.greater(x, y)",
        "begin_line": 1632,
        "end_line": 1642,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.greater_equal#1645",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.greater_equal(x, y)",
        "snippet": "def greater_equal(x, y):\n    \"\"\"Element-wise truth value of (x >= y).\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n    \"\"\"\n    return tf.greater_equal(x, y)",
        "begin_line": 1645,
        "end_line": 1655,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.maximum#1684",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.maximum(x, y)",
        "snippet": "def maximum(x, y):\n    \"\"\"Element-wise maximum of two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.maximum(x, y)",
        "begin_line": 1684,
        "end_line": 1694,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010481081647626035,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.minimum#1697",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.minimum(x, y)",
        "snippet": "def minimum(x, y):\n    \"\"\"Element-wise minimum of two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.minimum(x, y)",
        "begin_line": 1697,
        "end_line": 1707,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.cos#1722",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.cos(x)",
        "snippet": "def cos(x):\n    \"\"\"Computes cos of x element-wise.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.cos(x)",
        "begin_line": 1722,
        "end_line": 1731,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._regular_normalize_batch_in_training#1734",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._regular_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
        "snippet": "def _regular_normalize_batch_in_training(x, gamma, beta,\n                                         reduction_axes, epsilon=1e-3):\n    \"\"\"Non-fused version of `normalize_batch_in_training`.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    mean, var = tf.nn.moments(x, reduction_axes,\n                              None, None, False)\n    normed = tf.nn.batch_normalization(x, mean, var,\n                                       beta, gamma,\n                                       epsilon)\n    return normed, mean, var",
        "begin_line": 1734,
        "end_line": 1754,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010481081647626035,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._broadcast_normalize_batch_in_training#1757",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._broadcast_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
        "snippet": "def _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                           reduction_axes, epsilon=1e-3):\n    \"\"\"Non-fused, broadcast version of `normalize_batch_in_training`.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    mean, var = tf.nn.moments(x, reduction_axes,\n                              None, None, False)\n    target_shape = []\n    for axis in range(ndim(x)):\n        if axis in reduction_axes:\n            target_shape.append(1)\n        else:\n            target_shape.append(tf.shape(x)[axis])\n    target_shape = tf.stack(target_shape)\n\n    broadcast_mean = tf.reshape(mean, target_shape)\n    broadcast_var = tf.reshape(var, target_shape)\n    if gamma is None:\n        broadcast_gamma = None\n    else:\n        broadcast_gamma = tf.reshape(gamma, target_shape)\n    if beta is None:\n        broadcast_beta = None\n    else:\n        broadcast_beta = tf.reshape(beta, target_shape)\n\n    normed = tf.nn.batch_normalization(\n        x,\n        broadcast_mean,\n        broadcast_var,\n        broadcast_beta,\n        broadcast_gamma,\n        epsilon)\n    return normed, mean, var",
        "begin_line": 1757,
        "end_line": 1800,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._fused_normalize_batch_in_training#1803",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._fused_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
        "snippet": "def _fused_normalize_batch_in_training(x, gamma, beta, reduction_axes,\n                                       epsilon=1e-3):\n    \"\"\"Fused version of `normalize_batch_in_training`.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    if list(reduction_axes) == [0, 1, 2]:\n        normalization_axis = 3\n        tf_data_format = 'NHWC'\n    else:\n        normalization_axis = 1\n        tf_data_format = 'NCHW'\n\n    if gamma is None:\n        gamma = tf.constant(1.0,\n                            dtype=x.dtype,\n                            shape=[x.get_shape()[normalization_axis]])\n    if beta is None:\n        beta = tf.constant(0.0,\n                           dtype=x.dtype,\n                           shape=[x.get_shape()[normalization_axis]])\n\n    return tf.nn.fused_batch_norm(\n        x,\n        gamma,\n        beta,\n        epsilon=epsilon,\n        data_format=tf_data_format)",
        "begin_line": 1803,
        "end_line": 1839,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.00047214353163361664,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.00047214353163361664,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.normalize_batch_in_training#1842",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)",
        "snippet": "def normalize_batch_in_training(x, gamma, beta,\n                                reduction_axes, epsilon=1e-3):\n    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n\n    # Arguments\n        x: Input tensor or variable.\n        gamma: Tensor by which to scale the input.\n        beta: Tensor with which to center the input.\n        reduction_axes: iterable of integers,\n            axes over which to normalize.\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n    \"\"\"\n    if ndim(x) == 4 and list(reduction_axes) in [[0, 1, 2], [0, 2, 3]]:\n        if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\n            return _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                                          reduction_axes,\n                                                          epsilon=epsilon)\n        return _fused_normalize_batch_in_training(\n            x, gamma, beta, reduction_axes,\n            epsilon=epsilon)\n    else:\n        if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n            return _regular_normalize_batch_in_training(x, gamma, beta,\n                                                        reduction_axes,\n                                                        epsilon=epsilon)\n        else:\n            return _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                                          reduction_axes,\n                                                          epsilon=epsilon)",
        "begin_line": 1842,
        "end_line": 1873,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.00047214353163361664,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.00047214353163361664,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.batch_normalization#1876",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001)",
        "snippet": "def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n    \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n\n    I.e. returns:\n    `output = (x - mean) / sqrt(var + epsilon) * gamma + beta`\n\n    # Arguments\n        x: Input tensor or variable.\n        mean: Mean of batch.\n        var: Variance of batch.\n        beta: Tensor with which to center the input.\n        gamma: Tensor by which to scale the input.\n        axis: Integer, the axis that should be normalized.\n            (typically the features axis).\n        epsilon: Fuzz factor.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if ndim(x) == 4:\n        # The CPU implementation of FusedBatchNorm only support NHWC\n        if axis == 1 or axis == -3:\n            tf_data_format = 'NCHW'\n        elif axis == 3 or axis == -1:\n            tf_data_format = 'NHWC'\n        else:\n            tf_data_format = None\n\n        if tf_data_format == 'NHWC' or tf_data_format == 'NCHW' and _has_nchw_support():\n            # The mean / var / beta / gamma may be processed by broadcast\n            # so it may have extra axes with 1, it is not needed and should be removed\n            if ndim(mean) > 1:\n                mean = tf.reshape(mean, (-1))\n            if ndim(var) > 1:\n                var = tf.reshape(var, (-1))\n            if beta is None:\n                beta = zeros_like(mean)\n            elif ndim(beta) > 1:\n                beta = tf.reshape(beta, (-1))\n            if gamma is None:\n                gamma = ones_like(mean)\n            elif ndim(gamma) > 1:\n                gamma = tf.reshape(gamma, (-1))\n            y, _, _ = tf.nn.fused_batch_norm(\n                x,\n                gamma,\n                beta,\n                epsilon=epsilon,\n                mean=mean,\n                variance=var,\n                data_format=tf_data_format,\n                is_training=False\n            )\n            return y\n    # default\n    return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)",
        "begin_line": 1876,
        "end_line": 1931,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.concatenate#1936",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.concatenate(tensors, axis=-1)",
        "snippet": "def concatenate(tensors, axis=-1):\n    \"\"\"Concatenates a list of tensors alongside the specified axis.\n\n    # Arguments\n        tensors: list of tensors to concatenate.\n        axis: concatenation axis.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if axis < 0:\n        rank = ndim(tensors[0])\n        if rank:\n            axis %= rank\n        else:\n            axis = 0\n\n    if py_all([is_sparse(x) for x in tensors]):\n        return tf.sparse_concat(axis, tensors)\n    else:\n        return tf.concat([to_dense(x) for x in tensors], axis)",
        "begin_line": 1936,
        "end_line": 1956,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005970149253731343,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005970149253731343,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.reshape#1959",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.reshape(x, shape)",
        "snippet": "def reshape(x, shape):\n    \"\"\"Reshapes a tensor to the specified shape.\n\n    # Arguments\n        x: Tensor or variable.\n        shape: Target shape tuple.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.reshape(x, shape)",
        "begin_line": 1959,
        "end_line": 1969,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.960573476702509e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.permute_dimensions#1972",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.permute_dimensions(x, pattern)",
        "snippet": "def permute_dimensions(x, pattern):\n    \"\"\"Permutes axes in a tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        pattern: A tuple of\n            dimension indices, e.g. `(0, 2, 1)`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.transpose(x, perm=pattern)",
        "begin_line": 1972,
        "end_line": 1983,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004784688995215311,
            "pseudo_dstar_susp": 0.0003850596842510589,
            "pseudo_tarantula_susp": 0.0011695906432748538,
            "pseudo_op2_susp": 0.0003850596842510589,
            "pseudo_barinel_susp": 0.0011695906432748538
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.resize_images#1986",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.resize_images(x, height_factor, width_factor, data_format, interpolation='nearest')",
        "snippet": "def resize_images(x,\n                  height_factor,\n                  width_factor,\n                  data_format,\n                  interpolation='nearest'):\n    \"\"\"Resizes the images contained in a 4D tensor.\n\n    # Arguments\n        x: Tensor or variable to resize.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        interpolation: A string, one of `nearest` or `bilinear`.\n\n    # Returns\n        A tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"\n    if data_format == 'channels_first':\n        rows, cols = 2, 3\n    else:\n        rows, cols = 1, 2\n\n    original_shape = int_shape(x)\n    new_shape = tf.shape(x)[rows:cols + 1]\n    new_shape *= tf.constant(np.array([height_factor, width_factor], dtype='int32'))\n\n    if data_format == 'channels_first':\n        x = permute_dimensions(x, [0, 2, 3, 1])\n    if interpolation == 'nearest':\n        x = tf.image.resize_nearest_neighbor(x, new_shape)\n    elif interpolation == 'bilinear':\n        x = tf.image.resize_bilinear(x, new_shape)\n    else:\n        raise ValueError('interpolation should be one '\n                         'of \"nearest\" or \"bilinear\".')\n    if data_format == 'channels_first':\n        x = permute_dimensions(x, [0, 3, 1, 2])\n\n    if original_shape[rows] is None:\n        new_height = None\n    else:\n        new_height = original_shape[rows] * height_factor\n\n    if original_shape[cols] is None:\n        new_width = None\n    else:\n        new_width = original_shape[cols] * width_factor\n\n    output_shape = (None, new_height, new_width, None)\n    x.set_shape(transpose_shape(output_shape, data_format, spatial_axes=(1, 2)))\n    return x",
        "begin_line": 1986,
        "end_line": 2039,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0007429420505200594,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0007429420505200594,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.resize_volumes#2042",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format)",
        "snippet": "def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    \"\"\"Resizes the volume contained in a 5D tensor.\n\n    # Arguments\n        x: Tensor or variable to resize.\n        depth_factor: Positive integer.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"\n    if data_format == 'channels_first':\n        output = repeat_elements(x, depth_factor, axis=2)\n        output = repeat_elements(output, height_factor, axis=3)\n        output = repeat_elements(output, width_factor, axis=4)\n        return output\n    elif data_format == 'channels_last':\n        output = repeat_elements(x, depth_factor, axis=1)\n        output = repeat_elements(output, height_factor, axis=2)\n        output = repeat_elements(output, width_factor, axis=3)\n        return output\n    else:\n        raise ValueError('Unknown data_format: ' + str(data_format))",
        "begin_line": 2042,
        "end_line": 2069,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.repeat_elements#2072",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.repeat_elements(x, rep, axis)",
        "snippet": "def repeat_elements(x, rep, axis):\n    \"\"\"Repeats the elements of a tensor along an axis, like `np.repeat`.\n\n    If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output\n    will have shape `(s1, s2 * rep, s3)`.\n\n    # Arguments\n        x: Tensor or variable.\n        rep: Python integer, number of times to repeat.\n        axis: Axis along which to repeat.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    x_shape = x.get_shape().as_list()\n    # For static axis\n    if x_shape[axis] is not None:\n        # slices along the repeat axis\n        splits = tf.split(value=x, num_or_size_splits=x_shape[axis], axis=axis)\n        # repeat each slice the given number of reps\n        x_rep = [s for s in splits for _ in range(rep)]\n        return concatenate(x_rep, axis)\n\n    # Here we use tf.tile to mimic behavior of np.repeat so that\n    # we can handle dynamic shapes (that include None).\n    # To do that, we need an auxiliary axis to repeat elements along\n    # it and then merge them along the desired axis.\n\n    # Repeating\n    auxiliary_axis = axis + 1\n    x_shape = tf.shape(x)\n    x_rep = tf.expand_dims(x, axis=auxiliary_axis)\n    reps = np.ones(len(x.get_shape()) + 1)\n    reps[auxiliary_axis] = rep\n    x_rep = tf.tile(x_rep, reps)\n\n    # Merging\n    reps = np.delete(reps, auxiliary_axis)\n    reps[axis] = rep\n    reps = tf.constant(reps, dtype='int32')\n    x_shape = x_shape * reps\n    x_rep = tf.reshape(x_rep, x_shape)\n\n    # Fix shape representation\n    x_shape = x.get_shape().as_list()\n    x_rep.set_shape(x_shape)\n    x_rep._keras_shape = tuple(x_shape)\n    return x_rep",
        "begin_line": 2072,
        "end_line": 2119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.repeat#2122",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.repeat(x, n)",
        "snippet": "def repeat(x, n):\n    \"\"\"Repeats a 2D tensor.\n\n    if `x` has shape (samples, dim) and `n` is `2`,\n    the output will have shape `(samples, 2, dim)`.\n\n    # Arguments\n        x: Tensor or variable.\n        n: Python integer, number of times to repeat.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    assert ndim(x) == 2\n    x = tf.expand_dims(x, 1)\n    pattern = tf.stack([1, n, 1])\n    return tf.tile(x, pattern)",
        "begin_line": 2122,
        "end_line": 2138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.arange#2141",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.arange(start, stop=None, step=1, dtype='int32')",
        "snippet": "def arange(start, stop=None, step=1, dtype='int32'):\n    \"\"\"Creates a 1D tensor containing a sequence of integers.\n\n    The function arguments use the same convention as\n    Theano's arange: if only one argument is provided,\n    it is in fact the \"stop\" argument and \"start\" is 0.\n\n    The default type of the returned tensor is `'int32'` to\n    match TensorFlow's default.\n\n    # Arguments\n        start: Start value.\n        stop: Stop value.\n        step: Difference between two successive values.\n        dtype: Integer dtype to use.\n\n    # Returns\n        An integer tensor.\n\n    \"\"\"\n    # Match the behavior of numpy and Theano by returning an empty sequence.\n    if stop is None:\n        try:\n            if start < 0:\n                start = 0\n        except TypeError:\n            # Handle case where start is a tensor\n            start = tf.cond(start < 0,\n                            true_fn=lambda: tf.constant(0, dtype=start.dtype),\n                            false_fn=lambda: start)\n\n    result = tf.range(start, limit=stop, delta=step, name='arange')\n    if dtype != 'int32':\n        result = cast(result, dtype)\n    return result",
        "begin_line": 2141,
        "end_line": 2175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.tile#2178",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.tile(x, n)",
        "snippet": "def tile(x, n):\n    \"\"\"Creates a tensor by tiling `x` by `n`.\n\n    # Arguments\n        x: A tensor or variable\n        n: A list of integer. The length must be the same as the number of\n            dimensions in `x`.\n\n    # Returns\n        A tiled tensor.\n    \"\"\"\n    if isinstance(n, int):\n        n = [n]\n    return tf.tile(x, n)",
        "begin_line": 2178,
        "end_line": 2191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.973438621679828e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.flatten#2194",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.flatten(x)",
        "snippet": "def flatten(x):\n    \"\"\"Flatten a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor, reshaped into 1-D\n    \"\"\"\n    return tf.reshape(x, [-1])",
        "begin_line": 2194,
        "end_line": 2203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010583130489998942,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.batch_flatten#2206",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.batch_flatten(x)",
        "snippet": "def batch_flatten(x):\n    \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n\n    In other words, it flattens each data samples of a batch.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n    return x",
        "begin_line": 2206,
        "end_line": 2218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.expand_dims#2221",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.expand_dims(x, axis=-1)",
        "snippet": "def expand_dims(x, axis=-1):\n    \"\"\"Adds a 1-sized dimension at index \"axis\".\n\n    # Arguments\n        x: A tensor or variable.\n        axis: Position where to add a new axis.\n\n    # Returns\n        A tensor with expanded dimensions.\n    \"\"\"\n    return tf.expand_dims(x, axis)",
        "begin_line": 2221,
        "end_line": 2231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.878629139660837e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.squeeze#2234",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.squeeze(x, axis)",
        "snippet": "def squeeze(x, axis):\n    \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n\n    # Arguments\n        x: A tensor or variable.\n        axis: Axis to drop.\n\n    # Returns\n        A tensor with the same data as `x` but reduced dimensions.\n    \"\"\"\n    return tf.squeeze(x, [axis])",
        "begin_line": 2234,
        "end_line": 2244,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.temporal_padding#2247",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.temporal_padding(x, padding=(1, 1))",
        "snippet": "def temporal_padding(x, padding=(1, 1)):\n    \"\"\"Pads the middle dimension of a 3D tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        padding: Tuple of 2 integers, how many zeros to\n            add at the start and end of dim 1.\n\n    # Returns\n        A padded 3D tensor.\n    \"\"\"\n    assert len(padding) == 2\n    pattern = [[0, 0], [padding[0], padding[1]], [0, 0]]\n    return tf.pad(x, pattern)",
        "begin_line": 2247,
        "end_line": 2260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.spatial_2d_padding#2263",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None)",
        "snippet": "def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n    \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        padding: Tuple of 2 tuples, padding pattern.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A padded 4D tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"\n    assert len(padding) == 2\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    data_format = normalize_data_format(data_format)\n\n    pattern = [[0, 0],\n               list(padding[0]),\n               list(padding[1]),\n               [0, 0]]\n    pattern = transpose_shape(pattern, data_format, spatial_axes=(1, 2))\n    return tf.pad(x, pattern)",
        "begin_line": 2263,
        "end_line": 2287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006097560975609756,
            "pseudo_dstar_susp": 0.0007112375533428165,
            "pseudo_tarantula_susp": 0.0029154518950437317,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0029154518950437317
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.spatial_3d_padding#2290",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None)",
        "snippet": "def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n    \"\"\"Pads 5D tensor with zeros along the depth, height, width dimensions.\n\n    Pads these dimensions with respectively\n    \"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right.\n\n    For 'channels_last' data_format,\n    the 2nd, 3rd and 4th dimension will be padded.\n    For 'channels_first' data_format,\n    the 3rd, 4th and 5th dimension will be padded.\n\n    # Arguments\n        x: Tensor or variable.\n        padding: Tuple of 3 tuples, padding pattern.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A padded 5D tensor.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n\n    \"\"\"\n    assert len(padding) == 3\n    assert len(padding[0]) == 2\n    assert len(padding[1]) == 2\n    assert len(padding[2]) == 2\n    data_format = normalize_data_format(data_format)\n\n    pattern = [\n        [0, 0],\n        [padding[0][0], padding[0][1]],\n        [padding[1][0], padding[1][1]],\n        [padding[2][0], padding[2][1]],\n        [0, 0]\n    ]\n    pattern = transpose_shape(pattern, data_format, spatial_axes=(1, 2, 3))\n\n    return tf.pad(x, pattern)",
        "begin_line": 2290,
        "end_line": 2328,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.00041946308724832214,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.00041946308724832214,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.stack#2331",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.stack(x, axis=0)",
        "snippet": "def stack(x, axis=0):\n    \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n\n    # Arguments\n        x: List of tensors.\n        axis: Axis along which to perform stacking.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.stack(x, axis=axis)",
        "begin_line": 2331,
        "end_line": 2341,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.one_hot#2344",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.one_hot(indices, num_classes)",
        "snippet": "def one_hot(indices, num_classes):\n    \"\"\"Computes the one-hot representation of an integer tensor.\n\n    # Arguments\n        indices: nD integer tensor of shape\n            `(batch_size, dim1, dim2, ... dim(n-1))`\n        num_classes: Integer, number of classes to consider.\n\n    # Returns\n        (n + 1)D one hot representation of the input\n        with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n    \"\"\"\n    return tf.one_hot(indices, depth=num_classes, axis=-1)",
        "begin_line": 2344,
        "end_line": 2356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.reverse#2359",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.reverse(x, axes)",
        "snippet": "def reverse(x, axes):\n    \"\"\"Reverses a tensor along the specified axes.\n\n    # Arguments\n        x: Tensor to reverse.\n        axes: Integer or iterable of integers.\n            Axes to reverse.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if isinstance(axes, int):\n        axes = [axes]\n    return tf.reverse(x, axes)",
        "begin_line": 2359,
        "end_line": 2372,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.699321047526673e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.get_value#2398",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.get_value(x)",
        "snippet": "def get_value(x):\n    \"\"\"Returns the value of a variable.\n\n    # Arguments\n        x: input variable.\n\n    # Returns\n        A Numpy array.\n    \"\"\"\n    return x.eval(session=get_session())",
        "begin_line": 2398,
        "end_line": 2407,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.98069151324652e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.batch_get_value#2410",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.batch_get_value(ops)",
        "snippet": "def batch_get_value(ops):\n    \"\"\"Returns the value of more than one tensor variable.\n\n    # Arguments\n        ops: list of ops to run.\n\n    # Returns\n        A list of Numpy arrays.\n    \"\"\"\n    if ops:\n        return get_session().run(ops)\n    else:\n        return []",
        "begin_line": 2410,
        "end_line": 2422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00027419797093501506,
            "pseudo_dstar_susp": 0.000273972602739726,
            "pseudo_tarantula_susp": 0.00027839643652561246,
            "pseudo_op2_susp": 0.000273972602739726,
            "pseudo_barinel_susp": 0.00027839643652561246
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.set_value#2425",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.set_value(x, value)",
        "snippet": "def set_value(x, value):\n    \"\"\"Sets the value of a variable, from a Numpy array.\n\n    # Arguments\n        x: Tensor to set to a new value.\n        value: Value to set the tensor to, as a Numpy array\n            (of the same shape).\n    \"\"\"\n    value = np.asarray(value, dtype=dtype(x))\n    tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])\n    if hasattr(x, '_assign_placeholder'):\n        assign_placeholder = x._assign_placeholder\n        assign_op = x._assign_op\n    else:\n        assign_placeholder = tf.placeholder(tf_dtype, shape=value.shape)\n        assign_op = x.assign(assign_placeholder)\n        x._assign_placeholder = assign_placeholder\n        x._assign_op = assign_op\n    get_session().run(assign_op, feed_dict={assign_placeholder: value})",
        "begin_line": 2425,
        "end_line": 2443,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.batch_set_value#2446",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.batch_set_value(tuples)",
        "snippet": "def batch_set_value(tuples):\n    \"\"\"Sets the values of many tensor variables at once.\n\n    # Arguments\n        tuples: a list of tuples `(tensor, value)`.\n            `value` should be a Numpy array.\n    \"\"\"\n    if tuples:\n        assign_ops = []\n        feed_dict = {}\n        for x, value in tuples:\n            value = np.asarray(value, dtype=dtype(x))\n            tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])\n            if hasattr(x, '_assign_placeholder'):\n                assign_placeholder = x._assign_placeholder\n                assign_op = x._assign_op\n            else:\n                assign_placeholder = tf.placeholder(tf_dtype,\n                                                    shape=value.shape)\n                assign_op = x.assign(assign_placeholder)\n                x._assign_placeholder = assign_placeholder\n                x._assign_op = assign_op\n            assign_ops.append(assign_op)\n            feed_dict[assign_placeholder] = value\n        get_session().run(assign_ops, feed_dict=feed_dict)",
        "begin_line": 2446,
        "end_line": 2470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00027495188342040145,
            "pseudo_dstar_susp": 0.0002747252747252747,
            "pseudo_tarantula_susp": 0.00027925160569673273,
            "pseudo_op2_susp": 0.0002747252747252747,
            "pseudo_barinel_susp": 0.00027925160569673273
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.get_variable_shape#2473",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.get_variable_shape(x)",
        "snippet": "def get_variable_shape(x):\n    \"\"\"Returns the shape of a variable.\n\n    # Arguments\n        x: A variable.\n\n    # Returns\n        A tuple of integers.\n    \"\"\"\n    return int_shape(x)",
        "begin_line": 2473,
        "end_line": 2482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.print_tensor#2485",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.print_tensor(x, message='')",
        "snippet": "def print_tensor(x, message=''):\n    \"\"\"Prints `message` and the tensor value when evaluated.\n\n     Note that `print_tensor` returns a new tensor identical to `x`\n     which should be used in the following code. Otherwise the\n     print operation is not taken into account during evaluation.\n\n     # Example\n     ```python\n         >>> x = K.print_tensor(x, message=\"x is: \")\n     ```\n\n    # Arguments\n        x: Tensor to print.\n        message: Message to print jointly with the tensor.\n\n    # Returns\n        The same tensor `x`, unchanged.\n    \"\"\"\n    return tf.Print(x, [x], message)",
        "begin_line": 2485,
        "end_line": 2504,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function.__init__#2530",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function.__init__(self, inputs, outputs, updates=None, name=None, **session_kwargs)",
        "snippet": "    def __init__(self, inputs, outputs,\n                 updates=None,\n                 name=None,\n                 **session_kwargs):\n        updates = updates or []\n        if not isinstance(inputs, (list, tuple)):\n            raise TypeError('`inputs` to a TensorFlow backend function '\n                            'should be a list or tuple.')\n        if not isinstance(outputs, (list, tuple)):\n            raise TypeError('`outputs` of a TensorFlow backend function '\n                            'should be a list or tuple.')\n        if not isinstance(updates, (list, tuple)):\n            raise TypeError('`updates` in a TensorFlow backend function '\n                            'should be a list or tuple.')\n        self.inputs = list(inputs)\n        self.outputs = list(outputs)\n        with tf.control_dependencies(self.outputs):\n            updates_ops = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    p, new_p = update\n                    updates_ops.append(tf.assign(p, new_p))\n                else:\n                    # assumed already an op\n                    updates_ops.append(update)\n            self.updates_op = tf.group(*updates_ops)\n        self.name = name\n        # additional tensor substitutions\n        self.feed_dict = session_kwargs.pop('feed_dict', {})\n        # additional operations\n        self.fetches = session_kwargs.pop('fetches', [])\n        if not isinstance(self.fetches, list):\n            self.fetches = [self.fetches]\n        # The main use case of `fetches` being passed to a model is the ability\n        # to run custom updates\n        # (since the outputs of fetches are never returned).\n        # This requires us to wrap fetches in `identity` ops.\n        self.fetches = [tf.identity(x) for x in self.fetches]\n        # self.session_kwargs is used for _legacy_call\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n        if session_kwargs:\n            raise ValueError('Some keys in session_kwargs are not '\n                             'supported at this '\n                             'time: %s', session_kwargs.keys())\n        self._callable_fn = None\n        self._feed_arrays = None\n        self._feed_symbols = None\n        self._symbol_vals = None\n        self._session = None",
        "begin_line": 2530,
        "end_line": 2580,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003779289493575208,
            "pseudo_dstar_susp": 0.0012091898428053204,
            "pseudo_tarantula_susp": 0.0003169572107765452,
            "pseudo_op2_susp": 0.0012091898428053204,
            "pseudo_barinel_susp": 0.0003169572107765452
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function._make_callable#2582",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function._make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)",
        "snippet": "    def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):\n        \"\"\"Generates a callable that runs the graph.\n\n        # Arguments\n            feed_arrays: List of input tensors to be fed\n                Numpy arrays at runtime.\n            feed_symbols: List of input tensors to be fed\n                symbolic tensors at runtime.\n            symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.\n            session: Session to use to generate the callable.\n\n        # Returns\n            Function that runs the graph according to the above options.\n        \"\"\"\n        # Prepare callable options.\n        callable_opts = config_pb2.CallableOptions()\n        # Handle external-data feed.\n        for x in feed_arrays:\n            callable_opts.feed.append(x.name)\n        if self.feed_dict:\n            for key in sorted(self.feed_dict.keys()):\n                callable_opts.feed.append(key.name)\n        # Handle symbolic feed.\n        for x, y in zip(feed_symbols, symbol_vals):\n            connection = callable_opts.tensor_connection.add()\n            if x.dtype != y.dtype:\n                y = tf.cast(y, dtype=x.dtype)\n            from_tensor = tf_ops._as_graph_element(y)\n            if from_tensor is None:\n                from_tensor = y\n            connection.from_tensor = from_tensor.name  # Data tensor\n            connection.to_tensor = x.name  # Placeholder\n        # Handle fetches.\n        for x in self.outputs + self.fetches:\n            callable_opts.fetch.append(x.name)\n        # Handle updates.\n        callable_opts.target.append(self.updates_op.name)\n        # Handle run_options.\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n        # Create callable.\n        callable_fn = session._make_callable_from_options(callable_opts)\n        # Cache parameters corresponding to the generated callable, so that\n        # we can detect future mismatches and refresh the callable.\n        self._callable_fn = callable_fn\n        self._feed_arrays = feed_arrays\n        self._feed_symbols = feed_symbols\n        self._symbol_vals = symbol_vals\n        self._session = session",
        "begin_line": 2582,
        "end_line": 2630,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003417634996582365,
            "pseudo_dstar_susp": 0.000779423226812159,
            "pseudo_tarantula_susp": 0.00028506271379703536,
            "pseudo_op2_susp": 0.000779423226812159,
            "pseudo_barinel_susp": 0.00028506271379703536
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function._call#2632",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function._call(self, inputs)",
        "snippet": "    def _call(self, inputs):\n        if not isinstance(inputs, (list, tuple)):\n            raise TypeError('`inputs` should be a list or tuple.')\n\n        session = get_session()\n        feed_arrays = []\n        array_vals = []\n        feed_symbols = []\n        symbol_vals = []\n        for tensor, value in zip(self.inputs, inputs):\n            if value is None:\n                continue\n            if is_tensor(value):\n                # Case: feeding symbolic tensor.\n                feed_symbols.append(tensor)\n                symbol_vals.append(value)\n            else:\n                feed_arrays.append(tensor)\n                # We need to do array conversion and type casting\n                # at this level, since\n                # `callable_fn` only supports exact matches.\n                array_vals.append(\n                    np.asarray(value,\n                               dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n        if self.feed_dict:\n            for key in sorted(self.feed_dict.keys()):\n                array_vals.append(\n                    np.asarray(self.feed_dict[key],\n                               dtype=tf.as_dtype(key.dtype).as_numpy_dtype))\n\n        # Refresh callable if anything has changed.\n        if (self._callable_fn is None or\n                feed_arrays != self._feed_arrays or\n                symbol_vals != self._symbol_vals or\n                feed_symbols != self._feed_symbols or\n                session != self._session):\n            self._make_callable(feed_arrays,\n                                feed_symbols,\n                                symbol_vals,\n                                session)\n        if self.run_metadata:\n            fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\n        else:\n            fetched = self._callable_fn(*array_vals)\n        return fetched[:len(self.outputs)]",
        "begin_line": 2632,
        "end_line": 2676,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034411562284927734,
            "pseudo_dstar_susp": 0.000791765637371338,
            "pseudo_tarantula_susp": 0.0002886836027713626,
            "pseudo_op2_susp": 0.000791765637371338,
            "pseudo_barinel_susp": 0.0002886836027713626
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function._legacy_call#2678",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function._legacy_call(self, inputs)",
        "snippet": "    def _legacy_call(self, inputs):\n        if not isinstance(inputs, (list, tuple)):\n            raise TypeError('`inputs` should be a list or tuple.')\n        feed_dict = self.feed_dict.copy()\n        for tensor, value in zip(self.inputs, inputs):\n            if is_sparse(tensor):\n                sparse_coo = value.tocoo()\n                indices = np.concatenate(\n                    (np.expand_dims(sparse_coo.row, 1),\n                     np.expand_dims(sparse_coo.col, 1)), 1)\n                value = (indices, sparse_coo.data, sparse_coo.shape)\n            feed_dict[tensor] = value\n        fetches = self.outputs + [self.updates_op] + self.fetches\n        session = get_session()\n        updated = session.run(fetches=fetches, feed_dict=feed_dict,\n                              **self.session_kwargs)\n        return updated[:len(self.outputs)]",
        "begin_line": 2678,
        "end_line": 2694,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function.__call__#2696",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function.__call__(self, inputs)",
        "snippet": "    def __call__(self, inputs):\n        if hasattr(get_session(), '_make_callable_from_options'):\n            if py_any(is_sparse(x) for x in self.inputs):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'Feeding from symbolic tensors is not '\n                        'supported with sparse inputs.')\n                return self._legacy_call(inputs)\n\n            # callable generated by Session._make_callable_from_options accepts\n            # `run_metadata` keyword argument since TF 1.10\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n\n            return self._call(inputs)\n        else:\n            if py_any(is_tensor(x) for x in inputs):\n                raise ValueError(\n                    'In order to feed symbolic tensors to a Keras model '\n                    'in TensorFlow, you need tensorflow 1.8 or higher.')\n            return self._legacy_call(inputs)",
        "begin_line": 2696,
        "end_line": 2721,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003780718336483932,
            "pseudo_dstar_susp": 0.0012106537530266344,
            "pseudo_tarantula_susp": 0.00028506271379703536,
            "pseudo_op2_susp": 0.0012106537530266344,
            "pseudo_barinel_susp": 0.00028506271379703536
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.function#2724",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.function(inputs, outputs, updates=None, **kwargs)",
        "snippet": "def function(inputs, outputs, updates=None, **kwargs):\n    \"\"\"Instantiates a Keras function.\n\n    # Arguments\n        inputs: List of placeholder tensors.\n        outputs: List of output tensors.\n        updates: List of update ops.\n        **kwargs: Passed to `tf.Session.run`.\n\n    # Returns\n        Output values as Numpy arrays.\n\n    # Raises\n        ValueError: if invalid kwargs are passed in.\n    \"\"\"\n    if kwargs:\n        for key in kwargs:\n            if not (has_arg(tf.Session.run, key, True) or has_arg(Function.__init__, key, True)):\n                msg = 'Invalid argument \"%s\" passed to K.function with TensorFlow backend' % key\n                raise ValueError(msg)\n    return Function(inputs, outputs, updates=updates, **kwargs)",
        "begin_line": 2724,
        "end_line": 2744,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033760972316002703,
            "pseudo_dstar_susp": 0.000758150113722517,
            "pseudo_tarantula_susp": 0.00028208744710860365,
            "pseudo_op2_susp": 0.000758150113722517,
            "pseudo_barinel_susp": 0.00028208744710860365
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.gradients#2747",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.gradients(loss, variables)",
        "snippet": "def gradients(loss, variables):\n    \"\"\"Returns the gradients of `loss` w.r.t. `variables`.\n\n    # Arguments\n        loss: Scalar tensor to minimize.\n        variables: List of variables.\n\n    # Returns\n        A gradients tensor.\n    \"\"\"\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)",
        "begin_line": 2747,
        "end_line": 2757,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033311125916055963,
            "pseudo_dstar_susp": 0.0005777007510109763,
            "pseudo_tarantula_susp": 0.0003169572107765452,
            "pseudo_op2_susp": 0.0005777007510109763,
            "pseudo_barinel_susp": 0.0003169572107765452
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.stop_gradient#2760",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.stop_gradient(variables)",
        "snippet": "def stop_gradient(variables):\n    \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n\n    # Arguments\n        variables: tensor or list of tensors to consider constant with respect\n            to any other variable.\n\n    # Returns\n        A single tensor or a list of tensors (depending on the passed argument)\n            that has constant gradient with respect to any other variable.\n    \"\"\"\n    if isinstance(variables, (list, tuple)):\n        return map(tf.stop_gradient, variables)\n    else:\n        return tf.stop_gradient(variables)",
        "begin_line": 2760,
        "end_line": 2774,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.rnn#2779",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)",
        "snippet": "def rnn(step_function, inputs, initial_states,\n        go_backwards=False, mask=None, constants=None,\n        unroll=False, input_length=None):\n    \"\"\"Iterates over the time dimension of a tensor.\n\n    # Arguments\n        step_function:\n            Parameters:\n                inputs: Tensor with shape (samples, ...) (no time dimension),\n                    representing input for the batch of samples at a certain\n                    time step.\n                states: List of tensors.\n            Returns:\n                outputs: Tensor with shape (samples, ...) (no time dimension),\n                new_states: List of tensors, same length and shapes\n                    as 'states'.\n        inputs: Tensor of temporal data of shape (samples, time, ...)\n            (at least 3D).\n        initial_states: Tensor with shape (samples, ...) (no time dimension),\n            containing the initial values for the states used in\n            the step function.\n        go_backwards: Boolean. If True, do the iteration over the time\n            dimension in reverse order and return the reversed sequence.\n        mask: Binary tensor with shape (samples, time),\n            with a zero for every element that is masked.\n        constants: A list of constant values passed at each step.\n        unroll: Whether to unroll the RNN or to use a symbolic loop\n            (`while_loop` or `scan` depending on backend).\n        input_length: Static number of timesteps in the input.\n\n    # Returns\n        A tuple, `(last_output, outputs, new_states)`.\n\n        last_output: The latest output of the rnn, of shape `(samples, ...)`\n        outputs: Tensor with shape `(samples, time, ...)` where each\n            entry `outputs[s, t]` is the output of the step function\n            at time `t` for sample `s`.\n        new_states: List of tensors, latest states returned by\n            the step function, of shape `(samples, ...)`.\n\n    # Raises\n        ValueError: If input dimension is less than 3.\n        ValueError: If `unroll` is `True`\n            but input timestep is not a fixed number.\n        ValueError: If `mask` is provided (not `None`)\n            but states is not provided (`len(states)` == 0).\n    \"\"\"\n    ndim = len(inputs.get_shape())\n    if ndim < 3:\n        raise ValueError('Input should be at least 3D.')\n\n    # Transpose to time-major, i.e.\n    # from (batch, time, ...) to (time, batch, ...)\n    axes = [1, 0] + list(range(2, ndim))\n    inputs = tf.transpose(inputs, (axes))\n\n    if mask is not None:\n        if mask.dtype != tf.bool:\n            mask = tf.cast(mask, tf.bool)\n        if len(mask.get_shape()) == ndim - 1:\n            mask = expand_dims(mask)\n        mask = tf.transpose(mask, axes)\n\n    if constants is None:\n        constants = []\n\n    global uses_learning_phase\n    uses_learning_phase = False\n\n    if unroll:\n        if not inputs.get_shape()[0]:\n            raise ValueError('Unrolling requires a '\n                             'fixed number of timesteps.')\n        states = initial_states\n        successive_states = []\n        successive_outputs = []\n\n        input_list = tf.unstack(inputs)\n        if go_backwards:\n            input_list.reverse()\n\n        if mask is not None:\n            mask_list = tf.unstack(mask)\n            if go_backwards:\n                mask_list.reverse()\n\n            for inp, mask_t in zip(input_list, mask_list):\n                output, new_states = step_function(inp, states + constants)\n                if getattr(output, '_uses_learning_phase', False):\n                    uses_learning_phase = True\n\n                # tf.where needs its condition tensor\n                # to be the same shape as its two\n                # result tensors, but in our case\n                # the condition (mask) tensor is\n                # (nsamples, 1), and A and B are (nsamples, ndimensions).\n                # So we need to\n                # broadcast the mask to match the shape of A and B.\n                # That's what the tile call does,\n                # it just repeats the mask along its second dimension\n                # n times.\n                tiled_mask_t = tf.tile(mask_t,\n                                       tf.stack([1, tf.shape(output)[1]]))\n\n                if not successive_outputs:\n                    prev_output = zeros_like(output)\n                else:\n                    prev_output = successive_outputs[-1]\n\n                output = tf.where(tiled_mask_t, output, prev_output)\n\n                return_states = []\n                for state, new_state in zip(states, new_states):\n                    # (see earlier comment for tile explanation)\n                    tiled_mask_t = tf.tile(mask_t,\n                                           tf.stack([1, tf.shape(new_state)[1]]))\n                    return_states.append(tf.where(tiled_mask_t,\n                                                  new_state,\n                                                  state))\n                states = return_states\n                successive_outputs.append(output)\n                successive_states.append(states)\n            last_output = successive_outputs[-1]\n            new_states = successive_states[-1]\n            outputs = tf.stack(successive_outputs)\n        else:\n            for inp in input_list:\n                output, states = step_function(inp, states + constants)\n                if getattr(output, '_uses_learning_phase', False):\n                    uses_learning_phase = True\n                successive_outputs.append(output)\n                successive_states.append(states)\n            last_output = successive_outputs[-1]\n            new_states = successive_states[-1]\n            outputs = tf.stack(successive_outputs)\n\n    else:\n        if go_backwards:\n            inputs = reverse(inputs, 0)\n\n        states = tuple(initial_states)\n\n        time_steps = tf.shape(inputs)[0]\n        outputs, _ = step_function(inputs[0], initial_states + constants)\n        output_ta = tensor_array_ops.TensorArray(\n            dtype=outputs.dtype,\n            size=time_steps,\n            tensor_array_name='output_ta')\n        input_ta = tensor_array_ops.TensorArray(\n            dtype=inputs.dtype,\n            size=time_steps,\n            tensor_array_name='input_ta')\n        input_ta = input_ta.unstack(inputs)\n        time = tf.constant(0, dtype='int32', name='time')\n\n        if mask is not None:\n            if not states:\n                raise ValueError('No initial states provided! '\n                                 'When using masking in an RNN, you should '\n                                 'provide initial states '\n                                 '(and your step function should return '\n                                 'as its first state at time `t` '\n                                 'the output at time `t-1`).')\n            if go_backwards:\n                mask = reverse(mask, 0)\n\n            mask_ta = tensor_array_ops.TensorArray(\n                dtype=tf.bool,\n                size=time_steps,\n                tensor_array_name='mask_ta')\n            mask_ta = mask_ta.unstack(mask)\n\n            def _step(time, output_ta_t, *states):\n                \"\"\"RNN step function.\n\n                # Arguments\n                    time: Current timestep value.\n                    output_ta_t: TensorArray.\n                    *states: List of states.\n\n                # Returns\n                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n                \"\"\"\n                current_input = input_ta.read(time)\n                mask_t = mask_ta.read(time)\n                output, new_states = step_function(current_input,\n                                                   tuple(states) +\n                                                   tuple(constants))\n                if getattr(output, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                for state, new_state in zip(states, new_states):\n                    new_state.set_shape(state.get_shape())\n                tiled_mask_t = tf.tile(mask_t,\n                                       tf.stack([1, tf.shape(output)[1]]))\n                output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [\n                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n                             new_states[i], states[i]) for i in range(len(states))\n                ]\n                output_ta_t = output_ta_t.write(time, output)\n                return (time + 1, output_ta_t) + tuple(new_states)\n        else:\n            def _step(time, output_ta_t, *states):\n                \"\"\"RNN step function.\n\n                # Arguments\n                    time: Current timestep value.\n                    output_ta_t: TensorArray.\n                    *states: List of states.\n\n                # Returns\n                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n                \"\"\"\n                current_input = input_ta.read(time)\n                output, new_states = step_function(current_input,\n                                                   tuple(states) +\n                                                   tuple(constants))\n                if getattr(output, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                for state, new_state in zip(states, new_states):\n                    new_state.set_shape(state.get_shape())\n                output_ta_t = output_ta_t.write(time, output)\n                return (time + 1, output_ta_t) + tuple(new_states)\n\n        final_outputs = control_flow_ops.while_loop(\n            cond=lambda time, *_: time < time_steps,\n            body=_step,\n            loop_vars=(time, output_ta) + states,\n            parallel_iterations=32,\n            swap_memory=True,\n            maximum_iterations=input_length)\n        last_time = final_outputs[0]\n        output_ta = final_outputs[1]\n        new_states = final_outputs[2:]\n\n        outputs = output_ta.stack()\n        last_output = output_ta.read(last_time - 1)\n\n    axes = [1, 0] + list(range(2, len(outputs.get_shape())))\n    outputs = tf.transpose(outputs, axes)\n    last_output._uses_learning_phase = uses_learning_phase\n    return last_output, outputs, new_states",
        "begin_line": 2779,
        "end_line": 3022,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._step#2951",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._step(time, output_ta_t, *states)",
        "snippet": "            def _step(time, output_ta_t, *states):\n                \"\"\"RNN step function.\n\n                # Arguments\n                    time: Current timestep value.\n                    output_ta_t: TensorArray.\n                    *states: List of states.\n\n                # Returns\n                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n                \"\"\"\n                current_input = input_ta.read(time)\n                mask_t = mask_ta.read(time)\n                output, new_states = step_function(current_input,\n                                                   tuple(states) +\n                                                   tuple(constants))\n                if getattr(output, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                for state, new_state in zip(states, new_states):\n                    new_state.set_shape(state.get_shape())\n                tiled_mask_t = tf.tile(mask_t,\n                                       tf.stack([1, tf.shape(output)[1]]))\n                output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [\n                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n                             new_states[i], states[i]) for i in range(len(states))\n                ]\n                output_ta_t = output_ta_t.write(time, output)\n                return (time + 1, output_ta_t) + tuple(new_states)",
        "begin_line": 2951,
        "end_line": 2980,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._step#2982",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._step(time, output_ta_t, *states)",
        "snippet": "            def _step(time, output_ta_t, *states):\n                \"\"\"RNN step function.\n\n                # Arguments\n                    time: Current timestep value.\n                    output_ta_t: TensorArray.\n                    *states: List of states.\n\n                # Returns\n                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n                \"\"\"\n                current_input = input_ta.read(time)\n                output, new_states = step_function(current_input,\n                                                   tuple(states) +\n                                                   tuple(constants))\n                if getattr(output, '_uses_learning_phase', False):\n                    global uses_learning_phase\n                    uses_learning_phase = True\n                for state, new_state in zip(states, new_states):\n                    new_state.set_shape(state.get_shape())\n                output_ta_t = output_ta_t.write(time, output)\n                return (time + 1, output_ta_t) + tuple(new_states)",
        "begin_line": 2982,
        "end_line": 3003,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.switch#3025",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.switch(condition, then_expression, else_expression)",
        "snippet": "def switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value.\n\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: tensor (`int` or `bool`).\n        then_expression: either a tensor, or a callable that returns a tensor.\n        else_expression: either a tensor, or a callable that returns a tensor.\n\n    # Returns\n        The selected tensor.\n\n    # Raises\n        ValueError: If rank of `condition` is greater than rank of expressions.\n    \"\"\"\n    if condition.dtype != tf.bool:\n        condition = tf.cast(condition, 'bool')\n    cond_ndim = ndim(condition)\n    if not cond_ndim:\n        if not callable(then_expression):\n            def then_expression_fn():\n                return then_expression\n        else:\n            then_expression_fn = then_expression\n        if not callable(else_expression):\n            def else_expression_fn():\n                return else_expression\n        else:\n            else_expression_fn = else_expression\n        x = tf.cond(condition,\n                    then_expression_fn,\n                    else_expression_fn)\n    else:\n        # tf.where needs its condition tensor\n        # to be the same shape as its two\n        # result tensors\n        if callable(then_expression):\n            then_expression = then_expression()\n        if callable(else_expression):\n            else_expression = else_expression()\n        expr_ndim = ndim(then_expression)\n        if cond_ndim > expr_ndim:\n            raise ValueError('Rank of `condition` should be less than or'\n                             ' equal to rank of `then_expression` and '\n                             '`else_expression`. ndim(condition)=' +\n                             str(cond_ndim) + ', ndim(then_expression)'\n                             '=' + str(expr_ndim))\n        if cond_ndim > 1:\n            ndim_diff = expr_ndim - cond_ndim\n            cond_shape = tf.concat([tf.shape(condition), [1] * ndim_diff], axis=0)\n            condition = tf.reshape(condition, cond_shape)\n            expr_shape = tf.shape(then_expression)\n            shape_diff = expr_shape - cond_shape\n            tile_shape = tf.where(shape_diff > 0, expr_shape, tf.ones_like(expr_shape))\n            condition = tf.tile(condition, tile_shape)\n        x = tf.where(condition, then_expression, else_expression)\n    return x",
        "begin_line": 3025,
        "end_line": 3083,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042444821731748726,
            "pseudo_dstar_susp": 0.0003599712023038157,
            "pseudo_tarantula_susp": 0.0009017132551848512,
            "pseudo_op2_susp": 0.0003599712023038157,
            "pseudo_barinel_susp": 0.0009017132551848512
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.then_expression_fn#3047",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.then_expression_fn()",
        "snippet": "            def then_expression_fn():\n                return then_expression",
        "begin_line": 3047,
        "end_line": 3048,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.0006527415143603133,
            "pseudo_tarantula_susp": 0.0008525149190110827,
            "pseudo_op2_susp": 0.0006527415143603133,
            "pseudo_barinel_susp": 0.0008532423208191126
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.else_expression_fn#3052",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.else_expression_fn()",
        "snippet": "            def else_expression_fn():\n                return else_expression",
        "begin_line": 3052,
        "end_line": 3053,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.095870474804439e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.in_train_phase#3086",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.in_train_phase(x, alt, training=None)",
        "snippet": "def in_train_phase(x, alt, training=None):\n    \"\"\"Selects `x` in train phase, and `alt` otherwise.\n\n    Note that `alt` should have the *same shape* as `x`.\n\n    # Arguments\n        x: What to return in train phase\n            (tensor or callable that returns a tensor).\n        alt: What to return otherwise\n            (tensor or callable that returns a tensor).\n        training: Optional scalar tensor\n            (or Python boolean, or Python integer)\n            specifying the learning phase.\n\n    # Returns\n        Either `x` or `alt` based on the `training` flag.\n        the `training` flag defaults to `K.learning_phase()`.\n    \"\"\"\n    if training is None:\n        training = learning_phase()\n        uses_learning_phase = True\n    else:\n        uses_learning_phase = False\n\n    if training is 1 or training is True:\n        if callable(x):\n            return x()\n        else:\n            return x\n\n    elif training is 0 or training is False:\n        if callable(alt):\n            return alt()\n        else:\n            return alt\n\n    # else: assume learning phase is a placeholder tensor.\n    x = switch(training, x, alt)\n    if uses_learning_phase:\n        x._uses_learning_phase = True\n    return x",
        "begin_line": 3086,
        "end_line": 3126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003204101249599487,
            "pseudo_dstar_susp": 0.00031318509238960227,
            "pseudo_tarantula_susp": 0.0004420866489832007,
            "pseudo_op2_susp": 0.00031318509238960227,
            "pseudo_barinel_susp": 0.0004420866489832007
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.in_test_phase#3129",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.in_test_phase(x, alt, training=None)",
        "snippet": "def in_test_phase(x, alt, training=None):\n    \"\"\"Selects `x` in test phase, and `alt` otherwise.\n\n    Note that `alt` should have the *same shape* as `x`.\n\n    # Arguments\n        x: What to return in test phase\n            (tensor or callable that returns a tensor).\n        alt: What to return otherwise\n            (tensor or callable that returns a tensor).\n        training: Optional scalar tensor\n            (or Python boolean, or Python integer)\n            specifying the learning phase.\n\n    # Returns\n        Either `x` or `alt` based on `K.learning_phase`.\n    \"\"\"\n    return in_train_phase(alt, x, training=training)",
        "begin_line": 3129,
        "end_line": 3146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.relu#3151",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.relu(x, alpha=0.0, max_value=None)",
        "snippet": "def relu(x, alpha=0., max_value=None):\n    \"\"\"Rectified linear unit.\n\n    With default values, it returns element-wise `max(x, 0)`.\n\n    # Arguments\n        x: A tensor or variable.\n        alpha: A scalar, slope of negative section (default=`0.`).\n        max_value: Saturation threshold.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if alpha != 0.:\n        x = tf.nn.leaky_relu(x, alpha)\n    else:\n        x = tf.nn.relu(x)\n\n    if max_value is not None:\n        max_value = _to_tensor(max_value, x.dtype.base_dtype)\n        x = tf.minimum(x, max_value)\n    return x",
        "begin_line": 3151,
        "end_line": 3172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.0009149130832570906,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0009149130832570906,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.elu#3175",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.elu(x, alpha=1.0)",
        "snippet": "def elu(x, alpha=1.):\n    \"\"\"Exponential linear unit.\n\n    # Arguments\n        x: A tensor or variable to compute the activation function for.\n        alpha: A scalar, slope of negative section.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    res = tf.nn.elu(x)\n    if alpha == 1:\n        return res\n    else:\n        return tf.where(x > 0, res, alpha * res)",
        "begin_line": 3175,
        "end_line": 3189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.softmax#3192",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.softmax(x, axis=-1)",
        "snippet": "def softmax(x, axis=-1):\n    \"\"\"Softmax of a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: The dimension softmax would be performed on.\n            The default is -1 which indicates the last dimension.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.softmax(x, axis=axis)",
        "begin_line": 3192,
        "end_line": 3203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002004008016032064,
            "pseudo_dstar_susp": 0.0009225092250922509,
            "pseudo_tarantula_susp": 0.0010649627263045794,
            "pseudo_op2_susp": 0.0009225092250922509,
            "pseudo_barinel_susp": 0.0010649627263045794
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.softplus#3206",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.softplus(x)",
        "snippet": "def softplus(x):\n    \"\"\"Softplus of a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.softplus(x)",
        "begin_line": 3206,
        "end_line": 3215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.softsign#3218",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.softsign(x)",
        "snippet": "def softsign(x):\n    \"\"\"Softsign of a tensor.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.softsign(x)",
        "begin_line": 3218,
        "end_line": 3227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.categorical_crossentropy#3230",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.categorical_crossentropy(target, output, from_logits=False, axis=-1)",
        "snippet": "def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor of the same shape as `output`.\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n        axis: Int specifying the channels axis. `axis=-1`\n            corresponds to data format `channels_last`,\n            and `axis=1` corresponds to data format\n            `channels_first`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `axis` is neither -1 nor one of\n            the axes of `output`.\n    \"\"\"\n    output_dimensions = list(range(len(output.get_shape())))\n    if axis != -1 and axis not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.get_shape()))))\n    # Note: tf.nn.softmax_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= tf.reduce_sum(output, axis, True)\n        # manual computation of crossentropy\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n        return - tf.reduce_sum(target * tf.log(output), axis)\n    else:\n        return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n                                                       logits=output)",
        "begin_line": 3230,
        "end_line": 3270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006729475100942127,
            "pseudo_dstar_susp": 0.0008271298593879239,
            "pseudo_tarantula_susp": 0.0006666666666666666,
            "pseudo_op2_susp": 0.0008271298593879239,
            "pseudo_barinel_susp": 0.0006666666666666666
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.sparse_categorical_crossentropy#3273",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1)",
        "snippet": "def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    \"\"\"Categorical crossentropy with integer targets.\n\n    # Arguments\n        target: An integer tensor.\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n        axis: Int specifying the channels axis. `axis=-1`\n            corresponds to data format `channels_last`,\n            and `axis=1` corresponds to data format\n            `channels_first`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `axis` is neither -1 nor one of\n            the axes of `output`.\n    \"\"\"\n    output_dimensions = list(range(len(output.get_shape())))\n    if axis != -1 and axis not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.get_shape()))))\n    # If the channels are not in the last axis, move them to be there:\n    if axis != -1 and axis != output_dimensions[-1]:\n        permutation = output_dimensions[:axis] + output_dimensions[axis + 1:]\n        permutation += [axis]\n        output = tf.transpose(output, perm=permutation)\n\n    # Note: tf.nn.sparse_softmax_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n        output = tf.log(output)\n\n    output_shape = output.get_shape()\n    targets = cast(flatten(target), 'int64')\n    logits = tf.reshape(output, [-1, int(output_shape[-1])])\n    res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=targets,\n        logits=logits)\n    if len(output_shape) >= 3:\n        # if our output includes timestep dimension\n        # or spatial dimensions we need to reshape\n        return tf.reshape(res, tf.shape(output)[:-1])\n    else:\n        return res",
        "begin_line": 3273,
        "end_line": 3326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.binary_crossentropy#3329",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.binary_crossentropy(target, output, from_logits=False)",
        "snippet": "def binary_crossentropy(target, output, from_logits=False):\n    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor with the same shape as `output`.\n        output: A tensor.\n        from_logits: Whether `output` is expected to be a logits tensor.\n            By default, we consider that `output`\n            encodes a probability distribution.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # Note: tf.nn.sigmoid_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # transform back to logits\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n        output = tf.log(output / (1 - output))\n\n    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n                                                   logits=output)",
        "begin_line": 3329,
        "end_line": 3351,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010583130489998942,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.sigmoid#3354",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.sigmoid(x)",
        "snippet": "def sigmoid(x):\n    \"\"\"Element-wise sigmoid.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.sigmoid(x)",
        "begin_line": 3354,
        "end_line": 3363,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.hard_sigmoid#3366",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.hard_sigmoid(x)",
        "snippet": "def hard_sigmoid(x):\n    \"\"\"Segment-wise linear approximation of sigmoid.\n\n    Faster than sigmoid.\n    Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n    In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    x = (0.2 * x) + 0.5\n    zero = _to_tensor(0., x.dtype.base_dtype)\n    one = _to_tensor(1., x.dtype.base_dtype)\n    x = tf.clip_by_value(x, zero, one)\n    return x",
        "begin_line": 3366,
        "end_line": 3383,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.987956138774042e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.tanh#3386",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.tanh(x)",
        "snippet": "def tanh(x):\n    \"\"\"Element-wise tanh.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.tanh(x)",
        "begin_line": 3386,
        "end_line": 3395,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.926180487369455e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.dropout#3398",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.dropout(x, level, noise_shape=None, seed=None)",
        "snippet": "def dropout(x, level, noise_shape=None, seed=None):\n    \"\"\"Sets entries in `x` to zero at random, while scaling the entire tensor.\n\n    # Arguments\n        x: tensor\n        level: fraction of the entries in the tensor\n            that will be set to 0.\n        noise_shape: shape for randomly generated keep/drop flags,\n            must be broadcastable to the shape of `x`\n        seed: random seed to ensure determinism.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    retain_prob = 1. - level\n    if seed is None:\n        seed = np.random.randint(10e6)\n    # the dummy 1. works around a TF bug\n    # (float32_ref vs. float32 incompatibility)\n    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)",
        "begin_line": 3398,
        "end_line": 3417,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.119927040583675e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.l2_normalize#3420",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.l2_normalize(x, axis=None)",
        "snippet": "def l2_normalize(x, axis=None):\n    \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n\n    # Arguments\n        x: Tensor or variable.\n        axis: axis along which to perform normalization.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.l2_normalize(x, axis=axis)",
        "begin_line": 3420,
        "end_line": 3430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.in_top_k#3433",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.in_top_k(predictions, targets, k)",
        "snippet": "def in_top_k(predictions, targets, k):\n    \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n\n    # Arguments\n        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.\n        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.\n        k: An `int`, number of top elements to consider.\n\n    # Returns\n        A 1D tensor of length `batch_size` and type `bool`.\n        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n        values of `predictions[i]`.\n    \"\"\"\n    return tf.nn.in_top_k(predictions, targets, k)",
        "begin_line": 3433,
        "end_line": 3446,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._preprocess_conv1d_input#3452",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._preprocess_conv1d_input(x, data_format)",
        "snippet": "def _preprocess_conv1d_input(x, data_format):\n    \"\"\"Transpose and cast the input before the conv1d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # tensorflow doesn't support float64 for conv layer before 1.8.0\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NWC'  # to pass TF Conv2dNative operations\n    if data_format == 'channels_first':\n        if not _has_nchw_support():\n            x = tf.transpose(x, (0, 2, 1))  # NCW -> NWC\n        else:\n            tf_data_format = 'NCW'\n    return x, tf_data_format",
        "begin_line": 3452,
        "end_line": 3472,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._preprocess_conv2d_input#3475",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._preprocess_conv2d_input(x, data_format, force_transpose=False)",
        "snippet": "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\n    \"\"\"Transpose and cast the input before the conv2d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # tensorflow doesn't support float64 for conv layer before 1.8.0\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NHWC'\n    if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\n            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n        else:\n            tf_data_format = 'NCHW'\n    return x, tf_data_format",
        "begin_line": 3475,
        "end_line": 3497,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00037650602409638556,
            "pseudo_dstar_susp": 0.0006169031462060457,
            "pseudo_tarantula_susp": 0.00046125461254612545,
            "pseudo_op2_susp": 0.0006169031462060457,
            "pseudo_barinel_susp": 0.00046125461254612545
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._preprocess_conv3d_input#3500",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._preprocess_conv3d_input(x, data_format)",
        "snippet": "def _preprocess_conv3d_input(x, data_format):\n    \"\"\"Transpose and cast the input before the conv3d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # tensorflow doesn't support float64 for conv layer before 1.8.0\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NDHWC'\n    if data_format == 'channels_first':\n        if not _has_nchw_support():\n            x = tf.transpose(x, (0, 2, 3, 4, 1))\n        else:\n            tf_data_format = 'NCDHW'\n    return x, tf_data_format",
        "begin_line": 3500,
        "end_line": 3520,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._preprocess_padding#3523",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._preprocess_padding(padding)",
        "snippet": "def _preprocess_padding(padding):\n    \"\"\"Convert keras' padding to tensorflow's padding.\n\n    # Arguments\n        padding: string, `\"same\"` or `\"valid\"`.\n\n    # Returns\n        a string, `\"SAME\"` or `\"VALID\"`.\n\n    # Raises\n        ValueError: if `padding` is invalid.\n    \"\"\"\n    if padding == 'same':\n        padding = 'SAME'\n    elif padding == 'valid':\n        padding = 'VALID'\n    else:\n        raise ValueError('Invalid padding: ' + str(padding))\n    return padding",
        "begin_line": 3523,
        "end_line": 3541,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0006105006105006105,
            "pseudo_tarantula_susp": 0.0004393673110720562,
            "pseudo_op2_susp": 0.0006105006105006105,
            "pseudo_barinel_susp": 0.0004393673110720562
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.conv1d#3544",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
        "snippet": "def conv1d(x, kernel, strides=1, padding='valid',\n           data_format=None, dilation_rate=1):\n    \"\"\"1D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: stride integer.\n        padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: integer dilate rate.\n\n    # Returns\n        A tensor, result of 1D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    kernel_shape = kernel.get_shape().as_list()\n    if padding == 'causal':\n        if data_format != 'channels_last':\n            raise ValueError('When using causal padding in `conv1d`, '\n                             '`data_format` must be \"channels_last\" '\n                             '(temporal data).')\n        # causal (dilated) convolution:\n        left_pad = dilation_rate * (kernel_shape[0] - 1)\n        x = temporal_padding(x, (left_pad, 0))\n        padding = 'valid'\n    padding = _preprocess_padding(padding)\n    x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n    x = tf.nn.convolution(\n        input=x,\n        filter=kernel,\n        dilation_rate=(dilation_rate,),\n        strides=(strides,),\n        padding=padding,\n        data_format=tf_data_format)\n\n    if data_format == 'channels_first' and tf_data_format == 'NWC':\n        x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW\n    return x",
        "begin_line": 3544,
        "end_line": 3587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.conv2d#3590",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
        "snippet": "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n\n    # Returns\n        A tensor, result of 2D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n\n    padding = _preprocess_padding(padding)\n    x = tf.nn.convolution(\n        input=x,\n        filter=kernel,\n        dilation_rate=dilation_rate,\n        strides=strides,\n        padding=padding,\n        data_format=tf_data_format)\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
        "begin_line": 3590,
        "end_line": 3626,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009970089730807576,
            "pseudo_dstar_susp": 0.0006747638326585695,
            "pseudo_tarantula_susp": 0.0011111111111111111,
            "pseudo_op2_susp": 0.0006747638326585695,
            "pseudo_barinel_susp": 0.0011111111111111111
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.conv2d_transpose#3629",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
        "snippet": "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D deconvolution (i.e. transposed convolution).\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        output_shape: 1D int tensor for the output shape.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n\n    # Returns\n        A tensor, result of transposed 2D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(output_shape, (tuple, list)):\n        output_shape = tf.stack(output_shape)\n\n    # tf.nn.atrous_conv2d_transpose input only supports NHWC format\n    if data_format == 'channels_first' and dilation_rate != (1, 1):\n        force_transpose = True\n    else:\n        force_transpose = False\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        output_shape = (output_shape[0],\n                        output_shape[2],\n                        output_shape[3],\n                        output_shape[1])\n    if output_shape[0] is None:\n        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n        output_shape = tf.stack(list(output_shape))\n\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    if dilation_rate == (1, 1):\n        x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,\n                                   padding=padding,\n                                   data_format=tf_data_format)\n    else:\n        assert dilation_rate[0] == dilation_rate[1]\n        x = tf.nn.atrous_conv2d_transpose(\n            x, kernel, output_shape, dilation_rate[0], padding)\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
        "begin_line": 3629,
        "end_line": 3689,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.separable_conv1d#3692",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1)",
        "snippet": "def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                     padding='valid', data_format=None, dilation_rate=1):\n    \"\"\"1D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        pointwise_kernel: kernel for the 1x1 convolution.\n        strides: stride integer.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: integer dilation rate.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(strides, int):\n        strides = (strides,)\n    if isinstance(dilation_rate, int):\n        dilation_rate = (dilation_rate,)\n\n    x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n    if tf_data_format == 'NWC':\n        tf_data_format = 'NHWC'\n    else:\n        tf_data_format = 'NCHW'\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n    else:\n        spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n    x = tf.expand_dims(x, spatial_start_dim)\n    depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n    pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)\n    dilation_rate = (1,) + dilation_rate\n\n    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                               strides=strides,\n                               padding=padding,\n                               rate=dilation_rate,\n                               data_format=tf_data_format)\n\n    x = tf.squeeze(x, [spatial_start_dim])\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW\n\n    return x",
        "begin_line": 3692,
        "end_line": 3746,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.separable_conv2d#3749",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
        "snippet": "def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        pointwise_kernel: kernel for the 1x1 convolution.\n        strides: strides tuple (length 2).\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: tuple of integers,\n            dilation rates for the separable convolution.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                               strides=strides,\n                               padding=padding,\n                               rate=dilation_rate,\n                               data_format=tf_data_format)\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
        "begin_line": 3749,
        "end_line": 3786,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003952569169960474,
            "pseudo_dstar_susp": 0.0003417634996582365,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0003417634996582365,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.depthwise_conv2d#3789",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
        "snippet": "def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                     data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution with separable filters.\n\n    # Arguments\n        x: input tensor\n        depthwise_kernel: convolution kernel for the depthwise convolution.\n        strides: strides tuple (length 2).\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        dilation_rate: tuple of integers,\n            dilation rates for the separable convolution.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    x = tf.nn.depthwise_conv2d(x, depthwise_kernel,\n                               strides=strides,\n                               padding=padding,\n                               rate=dilation_rate,\n                               data_format=tf_data_format)\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
        "begin_line": 3789,
        "end_line": 3825,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045998160073597056,
            "pseudo_dstar_susp": 0.00038022813688212925,
            "pseudo_tarantula_susp": 0.0010822510822510823,
            "pseudo_op2_susp": 0.00038022813688212925,
            "pseudo_barinel_susp": 0.0010822510822510823
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.conv3d#3828",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1))",
        "snippet": "def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1, 1)):\n    \"\"\"3D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 3 integers.\n\n    # Returns\n        A tensor, result of 3D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    x = tf.nn.convolution(\n        input=x,\n        filter=kernel,\n        dilation_rate=dilation_rate,\n        strides=strides,\n        padding=padding,\n        data_format=tf_data_format)\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        x = tf.transpose(x, (0, 4, 1, 2, 3))\n    return x",
        "begin_line": 3828,
        "end_line": 3862,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.conv3d_transpose#3865",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None)",
        "snippet": "def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                     padding='valid', data_format=None):\n    \"\"\"3D deconvolution (i.e. transposed convolution).\n\n    # Arguments\n        x: input tensor.\n        kernel: kernel tensor.\n        output_shape: 1D int tensor for the output shape.\n        strides: strides tuple.\n        padding: string, \"same\" or \"valid\".\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n\n    # Returns\n        A tensor, result of transposed 3D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    if isinstance(output_shape, (tuple, list)):\n        output_shape = tf.stack(output_shape)\n\n    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        output_shape = (output_shape[0],\n                        output_shape[2],\n                        output_shape[3],\n                        output_shape[4],\n                        output_shape[1])\n    if output_shape[0] is None:\n        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n        output_shape = tf.stack(list(output_shape))\n\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NDHWC':\n        strides = (1,) + strides + (1,)\n    else:\n        strides = (1, 1) + strides\n\n    x = tf.nn.conv3d_transpose(x, kernel, output_shape, strides,\n                               padding=padding,\n                               data_format=tf_data_format)\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        x = tf.transpose(x, (0, 4, 1, 2, 3))\n    return x",
        "begin_line": 3865,
        "end_line": 3913,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.pool2d#3916",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max')",
        "snippet": "def pool2d(x, pool_size, strides=(1, 1),\n           padding='valid', data_format=None,\n           pool_mode='max'):\n    \"\"\"2D Pooling.\n\n    # Arguments\n        x: Tensor or variable.\n        pool_size: tuple of 2 integers.\n        strides: tuple of 2 integers.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        pool_mode: string, `\"max\"` or `\"avg\"`.\n\n    # Returns\n        A tensor, result of 2D pooling.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n        ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NHWC':\n        strides = (1,) + strides + (1,)\n        pool_size = (1,) + pool_size + (1,)\n    else:\n        strides = (1, 1) + strides\n        pool_size = (1, 1) + pool_size\n\n    if pool_mode == 'max':\n        x = tf.nn.max_pool(x, pool_size, strides,\n                           padding=padding,\n                           data_format=tf_data_format)\n    elif pool_mode == 'avg':\n        x = tf.nn.avg_pool(x, pool_size, strides,\n                           padding=padding,\n                           data_format=tf_data_format)\n    else:\n        raise ValueError('Invalid pool_mode: ' + str(pool_mode))\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
        "begin_line": 3916,
        "end_line": 3960,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042444821731748726,
            "pseudo_dstar_susp": 0.0003599712023038157,
            "pseudo_tarantula_susp": 0.0009017132551848512,
            "pseudo_op2_susp": 0.0003599712023038157,
            "pseudo_barinel_susp": 0.0009017132551848512
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.pool3d#3963",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max')",
        "snippet": "def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n           data_format=None, pool_mode='max'):\n    \"\"\"3D Pooling.\n\n    # Arguments\n        x: Tensor or variable.\n        pool_size: tuple of 3 integers.\n        strides: tuple of 3 integers.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        pool_mode: string, `\"max\"` or `\"avg\"`.\n\n    # Returns\n        A tensor, result of 3D pooling.\n\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n        ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n    padding = _preprocess_padding(padding)\n    if tf_data_format == 'NDHWC':\n        strides = (1,) + strides + (1,)\n        pool_size = (1,) + pool_size + (1,)\n    else:\n        strides = (1, 1) + strides\n        pool_size = (1, 1) + pool_size\n\n    if pool_mode == 'max':\n        x = tf.nn.max_pool3d(x, pool_size, strides,\n                             padding=padding,\n                             data_format=tf_data_format)\n    elif pool_mode == 'avg':\n        x = tf.nn.avg_pool3d(x, pool_size, strides,\n                             padding=padding,\n                             data_format=tf_data_format)\n    else:\n        raise ValueError('Invalid pool_mode: ' + str(pool_mode))\n\n    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n        x = tf.transpose(x, (0, 4, 1, 2, 3))\n    return x",
        "begin_line": 3963,
        "end_line": 4006,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.bias_add#4009",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.bias_add(x, bias, data_format=None)",
        "snippet": "def bias_add(x, bias, data_format=None):\n    \"\"\"Adds a bias vector to a tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        bias: Bias tensor to add.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: In one of the two cases below:\n                    1. invalid `data_format` argument.\n                    2. invalid bias shape.\n                       the bias should be either a vector or\n                       a tensor with ndim(x) - 1 dimension\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    bias_shape = int_shape(bias)\n    if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n        raise ValueError('Unexpected bias dimensions %d, expect to be 1 or %d dimensions'\n                         % (len(bias_shape), ndim(x)))\n    if ndim(x) == 5:\n        if len(bias_shape) == 1:\n            new_shape = (1, 1, 1, 1, bias_shape[0])\n        else:\n            new_shape = (1,) + bias_shape\n        new_shape = transpose_shape(new_shape, data_format, spatial_axes=(1, 2, 3))\n        x += reshape(bias, new_shape)\n    elif ndim(x) == 4:\n        if data_format == 'channels_first':\n            if len(bias_shape) == 1:\n                if _has_nchw_support():\n                    x = tf.nn.bias_add(x, bias,\n                                       data_format='NCHW')\n                else:\n                    x += reshape(bias, (1, bias_shape[0], 1, 1))\n            else:\n                x += reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n        elif data_format == 'channels_last':\n            if len(bias_shape) == 1:\n                x = tf.nn.bias_add(x, bias,\n                                   data_format='NHWC')\n            else:\n                x += reshape(bias, (1,) + bias_shape)\n    elif ndim(x) == 3:\n        if len(bias_shape) == 1:\n            new_shape = (1, 1, bias_shape[0])\n        else:\n            new_shape = (1,) + bias_shape\n        new_shape = transpose_shape(new_shape, data_format, spatial_axes=(1,))\n        x += reshape(bias, new_shape)\n    else:\n        x = tf.nn.bias_add(x, bias)\n    return x",
        "begin_line": 4009,
        "end_line": 4064,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019157088122605363,
            "pseudo_dstar_susp": 0.007575757575757576,
            "pseudo_tarantula_susp": 0.0006455777921239509,
            "pseudo_op2_susp": 0.007575757575757576,
            "pseudo_barinel_susp": 0.000649772579597141
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.random_normal#4069",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
        "snippet": "def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with normal distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        mean: A float, mean of the normal distribution to draw samples.\n        stddev: A float, standard deviation of the normal distribution\n            to draw samples.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.random_normal(shape, mean=mean, stddev=stddev,\n                            dtype=dtype, seed=seed)",
        "begin_line": 4069,
        "end_line": 4088,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.random_uniform#4091",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)",
        "snippet": "def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with uniform distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        minval: A float, lower boundary of the uniform distribution\n            to draw samples.\n        maxval: A float, upper boundary of the uniform distribution\n            to draw samples.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.random_uniform(shape, minval=minval, maxval=maxval,\n                             dtype=dtype, seed=seed)",
        "begin_line": 4091,
        "end_line": 4111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011547344110854503,
            "pseudo_dstar_susp": 0.005681818181818182,
            "pseudo_tarantula_susp": 0.00047824007651841227,
            "pseudo_op2_susp": 0.005681818181818182,
            "pseudo_barinel_susp": 0.00047824007651841227
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.random_binomial#4114",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.random_binomial(shape, p=0.0, dtype=None, seed=None)",
        "snippet": "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with random binomial distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n                    tf.ones(shape, dtype=dtype),\n                    tf.zeros(shape, dtype=dtype))",
        "begin_line": 4114,
        "end_line": 4132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.truncated_normal#4135",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None)",
        "snippet": "def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with truncated random normal distribution of values.\n\n    The generated values follow a normal distribution\n    with specified mean and standard deviation,\n    except that values whose magnitude is more than\n    two standard deviations from the mean are dropped and re-picked.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        mean: Mean of the values.\n        stddev: Standard deviation of the values.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.truncated_normal(shape, mean, stddev, dtype=dtype, seed=seed)",
        "begin_line": 4135,
        "end_line": 4157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ctc_label_dense_to_sparse#4167",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ctc_label_dense_to_sparse(labels, label_lengths)",
        "snippet": "def ctc_label_dense_to_sparse(labels, label_lengths):\n    \"\"\"Converts CTC labels from dense to sparse.\n\n    # Arguments\n        labels: dense CTC labels.\n        label_lengths: length of the labels.\n\n    # Returns\n        A sparse tensor representation of the labels.\n    \"\"\"\n    label_shape = tf.shape(labels)\n    num_batches_tns = tf.stack([label_shape[0]])\n    max_num_labels_tns = tf.stack([label_shape[1]])\n\n    def range_less_than(_, current_input):\n        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n            max_num_labels_tns, current_input)\n\n    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\n    dense_mask = functional_ops.scan(range_less_than, label_lengths,\n                                     initializer=init, parallel_iterations=1)\n    dense_mask = dense_mask[:, 0, :]\n\n    label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),\n                             label_shape)\n    label_ind = tf.boolean_mask(label_array, dense_mask)\n\n    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(label_shape[0]),\n                                                  max_num_labels_tns), reverse(label_shape, 0)))\n    batch_ind = tf.boolean_mask(batch_array, dense_mask)\n    indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n\n    vals_sparse = tf.gather_nd(labels, indices)\n\n    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))",
        "begin_line": 4167,
        "end_line": 4201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.range_less_than#4181",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.range_less_than(_, current_input)",
        "snippet": "    def range_less_than(_, current_input):\n        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n            max_num_labels_tns, current_input)",
        "begin_line": 4181,
        "end_line": 4183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ctc_batch_cost#4204",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)",
        "snippet": "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n    \"\"\"Runs CTC loss algorithm on each batch element.\n\n    # Arguments\n        y_true: tensor `(samples, max_string_length)`\n            containing the truth labels.\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_pred`.\n        label_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_true`.\n\n    # Returns\n        Tensor with shape (samples,1) containing the\n            CTC loss of each element.\n    \"\"\"\n    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\n    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n\n    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n\n    return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                       labels=sparse_labels,\n                                       sequence_length=input_length), 1)",
        "begin_line": 4204,
        "end_line": 4229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ctc_decode#4232",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1)",
        "snippet": "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n               top_paths=1):\n    \"\"\"Decodes the output of a softmax.\n\n    Can use either greedy search (also known as best path)\n    or a constrained dictionary search.\n\n    # Arguments\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, )` containing the sequence length for\n            each batch item in `y_pred`.\n        greedy: perform much faster best-path search if `true`.\n            This does not use a dictionary.\n        beam_width: if `greedy` is `false`: a beam search decoder will be used\n            with a beam of this width.\n        top_paths: if `greedy` is `false`,\n            how many of the most probable paths will be returned.\n\n    # Returns\n        Tuple:\n            List: if `greedy` is `true`, returns a list of one element that\n                contains the decoded sequence.\n                If `false`, returns the `top_paths` most probable\n                decoded sequences.\n                Important: blank labels are returned as `-1`.\n            Tensor `(top_paths, )` that contains\n                the log probability of each decoded sequence.\n    \"\"\"\n    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n    input_length = tf.to_int32(input_length)\n\n    if greedy:\n        (decoded, log_prob) = ctc.ctc_greedy_decoder(\n            inputs=y_pred,\n            sequence_length=input_length)\n    else:\n        (decoded, log_prob) = ctc.ctc_beam_search_decoder(\n            inputs=y_pred,\n            sequence_length=input_length, beam_width=beam_width,\n            top_paths=top_paths)\n\n    decoded_dense = [tf.sparse_to_dense(st.indices, st.dense_shape, st.values, default_value=-1)\n                     for st in decoded]\n    return (decoded_dense, log_prob)",
        "begin_line": 4232,
        "end_line": 4276,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0007429420505200594,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0007429420505200594,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.map_fn#4281",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.map_fn(fn, elems, name=None, dtype=None)",
        "snippet": "def map_fn(fn, elems, name=None, dtype=None):\n    \"\"\"Map the function fn over the elements elems and return the outputs.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems\n        elems: tensor\n        name: A string name for the map node in the graph\n        dtype: Output data type.\n\n    # Returns\n        Tensor with dtype `dtype`.\n    \"\"\"\n    return tf.map_fn(fn, elems, name=name, dtype=dtype)",
        "begin_line": 4281,
        "end_line": 4293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.foldl#4296",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.foldl(fn, elems, initializer=None, name=None)",
        "snippet": "def foldl(fn, elems, initializer=None, name=None):\n    \"\"\"Reduce elems using fn to combine them from left to right.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems and an\n            accumulator, for instance `lambda acc, x: acc + x`\n        elems: tensor\n        initializer: The first value used (`elems[0]` in case of None)\n        name: A string name for the foldl node in the graph\n\n    # Returns\n        Tensor with same type and shape as `initializer`.\n    \"\"\"\n    return tf.foldl(fn, elems, initializer=initializer, name=name)",
        "begin_line": 4296,
        "end_line": 4309,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.foldr#4312",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.foldr(fn, elems, initializer=None, name=None)",
        "snippet": "def foldr(fn, elems, initializer=None, name=None):\n    \"\"\"Reduce elems using fn to combine them from right to left.\n\n    # Arguments\n        fn: Callable that will be called upon each element in elems and an\n            accumulator, for instance `lambda acc, x: acc + x`\n        elems: tensor\n        initializer: The first value used (`elems[-1]` in case of None)\n        name: A string name for the foldr node in the graph\n\n    # Returns\n        Tensor with same type and shape as `initializer`.\n    \"\"\"\n    return tf.foldr(fn, elems, initializer=initializer, name=name)",
        "begin_line": 4312,
        "end_line": 4325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.local_conv1d#4328",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None)",
        "snippet": "def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n    \"\"\"Apply 1D conv with un-shared weights.\n\n    # Arguments\n        inputs: 3D tensor with shape: (batch_size, steps, input_dim)\n        kernel: the unshared weight for convolution,\n                with shape (output_length, feature_dim, filters)\n        kernel_size: a tuple of a single integer,\n                     specifying the length of the 1D convolution window\n        strides: a tuple of a single integer,\n                 specifying the stride length of the convolution\n        data_format: the data format, channels_first or channels_last\n\n    # Returns\n        the tensor after 1d conv with un-shared weights, with shape (batch_size, output_length, filters)\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    stride = strides[0]\n    kernel_shape = int_shape(kernel)\n    output_length, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_length):\n        slice_length = py_slice(i * stride,\n                                i * stride + kernel_size[0])\n        xs.append(reshape(inputs[:, slice_length, :],\n                          (1, -1, feature_dim)))\n    x_aggregate = concatenate(xs, axis=0)\n    # Shape: `(output_length, batch_size, filters)`.\n    output = batch_dot(x_aggregate, kernel)\n    return permute_dimensions(output, (1, 0, 2))",
        "begin_line": 4328,
        "end_line": 4363,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.local_conv2d#4366",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None)",
        "snippet": "def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None):\n    \"\"\"Apply 2D conv with un-shared weights.\n\n    # Arguments\n        inputs: 4D tensor with shape:\n                (batch_size, filters, new_rows, new_cols)\n                if data_format='channels_first'\n                or 4D tensor with shape:\n                (batch_size, new_rows, new_cols, filters)\n                if data_format='channels_last'.\n        kernel: the unshared weight for convolution,\n                with shape (output_items, feature_dim, filters)\n        kernel_size: a tuple of 2 integers, specifying the\n                     width and height of the 2D convolution window.\n        strides: a tuple of 2 integers, specifying the strides\n                 of the convolution along the width and height.\n        output_shape: a tuple with (output_row, output_col)\n        data_format: the data format, channels_first or channels_last\n\n    # Returns\n        A 4d tensor with shape:\n        (batch_size, filters, new_rows, new_cols)\n        if data_format='channels_first'\n        or 4D tensor with shape:\n        (batch_size, new_rows, new_cols, filters)\n        if data_format='channels_last'.\n\n    # Raises\n        ValueError: if `data_format` is neither\n                    `channels_last` or `channels_first`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    stride_row, stride_col = strides\n    output_row, output_col = output_shape\n    kernel_shape = int_shape(kernel)\n    _, feature_dim, filters = kernel_shape\n\n    xs = []\n    for i in range(output_row):\n        for j in range(output_col):\n            slice_row = py_slice(i * stride_row,\n                                 i * stride_row + kernel_size[0])\n            slice_col = py_slice(j * stride_col,\n                                 j * stride_col + kernel_size[1])\n            if data_format == 'channels_first':\n                xs.append(reshape(inputs[:, :, slice_row, slice_col],\n                                  (1, -1, feature_dim)))\n            else:\n                xs.append(reshape(inputs[:, slice_row, slice_col, :],\n                                  (1, -1, feature_dim)))\n\n    x_aggregate = concatenate(xs, axis=0)\n    output = batch_dot(x_aggregate, kernel)\n    output = reshape(output,\n                     (output_row, output_col, -1, filters))\n\n    if data_format == 'channels_first':\n        output = permute_dimensions(output, (2, 3, 0, 1))\n    else:\n        output = permute_dimensions(output, (2, 0, 1, 3))\n    return output",
        "begin_line": 4366,
        "end_line": 4427,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving._serialize_model#27",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving._serialize_model(model, f, include_optimizer=True)",
        "snippet": "def _serialize_model(model, f, include_optimizer=True):\n    \"\"\"Model serialization logic.\n\n    This method is used for both writing to HDF5 file/group,\n    as well as pickling. This is achieved via a\n    `keras.utils.hdf5_utls.H5Dict` object, which can wrap HDF5\n    files, groups and dicts with a common API.\n\n    # Arguments\n        model: Keras model instance to be serialized.\n        f: keras.utils.hdf5.HD5Dict instance.\n        include_optimizer: If True, serialize optimizer's state together.\n\n    \"\"\"\n    def get_json_type(obj):\n        \"\"\"Serialize any object to a JSON-serializable structure.\n\n        # Arguments\n            obj: the object to serialize\n\n        # Returns\n            JSON-serializable structure representing `obj`.\n\n        # Raises\n            TypeError: if `obj` cannot be serialized.\n        \"\"\"\n        # if obj is a serializable Keras class instance\n        # e.g. optimizer, layer\n        if hasattr(obj, 'get_config'):\n            return {'class_name': obj.__class__.__name__,\n                    'config': obj.get_config()}\n\n        # if obj is any numpy type\n        if type(obj).__module__ == np.__name__:\n            if isinstance(obj, np.ndarray):\n                return obj.tolist()\n            else:\n                return obj.item()\n\n        # misc functions (e.g. loss function)\n        if callable(obj):\n            return obj.__name__\n\n        # if obj is a python 'type'\n        if type(obj).__name__ == type.__name__:\n            return obj.__name__\n\n        raise TypeError('Not JSON Serializable: %s' % (obj,))\n\n    from .. import __version__ as keras_version\n\n    f['keras_version'] = str(keras_version).encode('utf8')\n    f['backend'] = K.backend().encode('utf8')\n\n    model_config = {}\n    model_config['class_name'] = model.__class__.__name__\n    model_config['config'] = model.get_config()\n    model_config = json.dumps(model_config, default=get_json_type)\n    model_config = model_config.encode('utf-8')\n    f['model_config'] = model_config\n\n    model_weights_group = f['model_weights']\n    model_layers = model.layers\n    model_weights_group['layer_names'] = [layer.name.encode('utf8') for layer in model_layers]\n    model_weights_group['backend'] = K.backend().encode('utf8')\n    model_weights_group['keras_version'] = str(keras_version).encode('utf8')\n    for layer in model_layers:\n        layer_group = model_weights_group[layer.name]\n        symbolic_weights = layer.weights\n        weight_values = K.batch_get_value(symbolic_weights)\n        weight_names = []\n        for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n            if hasattr(w, 'name') and w.name:\n                name = str(w.name)\n            else:\n                name = 'param_' + str(i)\n            if name in weight_names:\n                idx = 2\n                unique_name = name + '_1'\n                while unique_name in weight_names:\n                    unique_name = name + '_' + str(idx)\n                    idx += 1\n                name = unique_name\n            weight_names.append(name.encode('utf8'))\n        layer_group['weight_names'] = weight_names\n        for name, val in zip(weight_names, weight_values):\n            layer_group[name] = val\n    if include_optimizer and model.optimizer:\n        if isinstance(model.optimizer, optimizers.TFOptimizer):\n            warnings.warn(\n                'TensorFlow optimizers do not '\n                'make it possible to access '\n                'optimizer attributes or optimizer state '\n                'after instantiation. '\n                'As a result, we cannot save the optimizer '\n                'as part of the model save file.'\n                'You will have to compile your model again '\n                'after loading it. '\n                'Prefer using a Keras optimizer instead '\n                '(see keras.io/optimizers).')\n        else:\n            f['training_config'] = json.dumps({\n                'optimizer_config': {\n                    'class_name': model.optimizer.__class__.__name__,\n                    'config': model.optimizer.get_config()\n                },\n                'loss': model.loss,\n                'metrics': model.metrics,\n                'sample_weight_mode': model.sample_weight_mode,\n                'loss_weights': model.loss_weights,\n            }, default=get_json_type).encode('utf8')\n            symbolic_weights = getattr(model.optimizer, 'weights')\n            if symbolic_weights:\n                optimizer_weights_group = f['optimizer_weights']\n                weight_values = K.batch_get_value(symbolic_weights)\n                weight_names = []\n                for i, (w, val) in enumerate(zip(symbolic_weights,\n                                                 weight_values)):\n                    # Default values of symbolic_weights is /variable\n                    # for Theano and CNTK\n                    if K.backend() == 'theano' or K.backend() == 'cntk':\n                        if hasattr(w, 'name'):\n                            if w.name.split('/')[-1] == 'variable':\n                                name = str(w.name) + '_' + str(i)\n                            else:\n                                name = str(w.name)\n                        else:\n                            name = 'param_' + str(i)\n                    else:\n                        if hasattr(w, 'name') and w.name:\n                            name = str(w.name)\n                        else:\n                            name = 'param_' + str(i)\n                    if name in weight_names:\n                        idx = 2\n                        unique_name = name + '_1'\n                        while unique_name in weight_names:\n                            unique_name = name + '_' + str(idx)\n                            idx += 1\n                        name = unique_name\n                    weight_names.append(name.encode('utf8'))\n                optimizer_weights_group['weight_names'] = weight_names\n                for name, val in zip(weight_names, weight_values):\n                    optimizer_weights_group[name] = val",
        "begin_line": 27,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.get_json_type#41",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.get_json_type(obj)",
        "snippet": "    def get_json_type(obj):\n        \"\"\"Serialize any object to a JSON-serializable structure.\n\n        # Arguments\n            obj: the object to serialize\n\n        # Returns\n            JSON-serializable structure representing `obj`.\n\n        # Raises\n            TypeError: if `obj` cannot be serialized.\n        \"\"\"\n        # if obj is a serializable Keras class instance\n        # e.g. optimizer, layer\n        if hasattr(obj, 'get_config'):\n            return {'class_name': obj.__class__.__name__,\n                    'config': obj.get_config()}\n\n        # if obj is any numpy type\n        if type(obj).__module__ == np.__name__:\n            if isinstance(obj, np.ndarray):\n                return obj.tolist()\n            else:\n                return obj.item()\n\n        # misc functions (e.g. loss function)\n        if callable(obj):\n            return obj.__name__\n\n        # if obj is a python 'type'\n        if type(obj).__name__ == type.__name__:\n            return obj.__name__\n\n        raise TypeError('Not JSON Serializable: %s' % (obj,))",
        "begin_line": 41,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving._deserialize_model#173",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving._deserialize_model(f, custom_objects=None, compile=True)",
        "snippet": "def _deserialize_model(f, custom_objects=None, compile=True):\n    \"\"\"De-serializes a model serialized via _serialize_model\n\n    # Arguments\n        f: `keras.utils.hdf5_utils.HFDict` instance.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n        compile: Boolean, whether to compile the model\n            after loading.\n\n    # Returns\n        A Keras model instance. If an optimizer was found\n        as part of the saved model, the model is already\n        compiled. Otherwise, the model is uncompiled and\n        a warning will be displayed. When `compile` is set\n        to False, the compilation is omitted without any\n        warning.\n    \"\"\"\n    if not custom_objects:\n        custom_objects = {}\n\n    def convert_custom_objects(obj):\n        \"\"\"Handles custom object lookup.\n\n        # Arguments\n            obj: object, dict, or list.\n\n        # Returns\n            The same structure, where occurrences\n                of a custom object name have been replaced\n                with the custom object.\n        \"\"\"\n        if isinstance(obj, list):\n            deserialized = []\n            for value in obj:\n                deserialized.append(convert_custom_objects(value))\n            return deserialized\n        if isinstance(obj, dict):\n            deserialized = {}\n            for key, value in obj.items():\n                deserialized[key] = convert_custom_objects(value)\n            return deserialized\n        if obj in custom_objects:\n            return custom_objects[obj]\n        return obj\n\n    model_config = f['model_config']\n    if model_config is None:\n        raise ValueError('No model found in config.')\n    model_config = json.loads(model_config.decode('utf-8'))\n    model = model_from_config(model_config, custom_objects=custom_objects)\n    model_weights_group = f['model_weights']\n\n    if 'keras_version' in model_weights_group:\n        original_keras_version = model_weights_group['keras_version'].decode('utf8')\n    else:\n        original_keras_version = '1'\n    if 'backend' in model_weights_group:\n        original_backend = model_weights_group['backend'].decode('utf8')\n    else:\n        original_backend = None\n\n    layer_names = model_weights_group['layer_names']\n\n    layers = model.layers\n\n    filtered_layers = []\n    for layer in layers:\n        weights = layer.weights\n        if weights:\n            filtered_layers.append(layer)\n\n    filtered_layer_names = []\n    for name in layer_names:\n        layer_weights = model_weights_group[name]\n        weight_names = layer_weights['weight_names']\n        if weight_names:\n            filtered_layer_names.append(name)\n\n    layer_names = filtered_layer_names\n    if len(layer_names) != len(filtered_layers):\n        raise ValueError('You are trying to load a weight file'\n                         ' containing {} layers into a model with {} layers'\n                         .format(len(layer_names), len(filtered_layers))\n                         )\n\n    # We batch weight value assignments in a single backend call\n    # which provides a speedup in TensorFlow.\n    weight_value_tuples = []\n    for k, name in enumerate(layer_names):\n        layer_weights = model_weights_group[name]\n        weight_names = layer_weights['weight_names']\n        weight_values = [layer_weights[weight_name] for weight_name in weight_names]\n        layer = filtered_layers[k]\n        symbolic_weights = layer.weights\n        weight_values = preprocess_weights_for_loading(layer,\n                                                       weight_values,\n                                                       original_keras_version,\n                                                       original_backend,\n                                                       reshape=False)\n        if len(weight_values) != len(symbolic_weights):\n            raise ValueError('Layer #' + str(k) +\n                             ' (named \"' + layer.name +\n                             '\" in the current model) was found to '\n                             'correspond to layer ' + name +\n                             ' in the save file. '\n                             'However the new layer ' + layer.name +\n                             ' expects ' + str(len(symbolic_weights)) +\n                             ' weights, but the saved weights have ' +\n                             str(len(weight_values)) +\n                             ' elements.')\n        weight_value_tuples += zip(symbolic_weights, weight_values)\n    K.batch_set_value(weight_value_tuples)\n\n    if compile:\n        training_config = f.get('training_config')\n        if training_config is None:\n            warnings.warn('No training configuration found in save file: '\n                          'the model was *not* compiled. '\n                          'Compile it manually.')\n            return model\n        training_config = json.loads(training_config.decode('utf-8'))\n        optimizer_config = training_config['optimizer_config']\n        optimizer = optimizers.deserialize(optimizer_config,\n                                           custom_objects=custom_objects)\n\n        # Recover loss functions and metrics.\n        loss = convert_custom_objects(training_config['loss'])\n        metrics = convert_custom_objects(training_config['metrics'])\n        sample_weight_mode = training_config['sample_weight_mode']\n        loss_weights = training_config['loss_weights']\n\n        # Compile model.\n        model.compile(optimizer=optimizer,\n                      loss=loss,\n                      metrics=metrics,\n                      loss_weights=loss_weights,\n                      sample_weight_mode=sample_weight_mode)\n\n        # Set optimizer weights.\n        if 'optimizer_weights' in f:\n            # Build train function (to get weight updates).\n            model._make_train_function()\n            optimizer_weights_group = f['optimizer_weights']\n            optimizer_weight_names = [\n                n.decode('utf8') for n in\n                optimizer_weights_group['weight_names']]\n            optimizer_weight_values = [optimizer_weights_group[n] for n in\n                                       optimizer_weight_names]\n            try:\n                model.optimizer.set_weights(optimizer_weight_values)\n            except ValueError:\n                warnings.warn('Error in loading the saved optimizer '\n                              'state. As a result, your model is '\n                              'starting with a freshly initialized '\n                              'optimizer.')\n\n    return model",
        "begin_line": 173,
        "end_line": 331,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.convert_custom_objects#195",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.convert_custom_objects(obj)",
        "snippet": "    def convert_custom_objects(obj):\n        \"\"\"Handles custom object lookup.\n\n        # Arguments\n            obj: object, dict, or list.\n\n        # Returns\n            The same structure, where occurrences\n                of a custom object name have been replaced\n                with the custom object.\n        \"\"\"\n        if isinstance(obj, list):\n            deserialized = []\n            for value in obj:\n                deserialized.append(convert_custom_objects(value))\n            return deserialized\n        if isinstance(obj, dict):\n            deserialized = {}\n            for key, value in obj.items():\n                deserialized[key] = convert_custom_objects(value)\n            return deserialized\n        if obj in custom_objects:\n            return custom_objects[obj]\n        return obj",
        "begin_line": 195,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.save_model#334",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.save_model(model, filepath, overwrite=True, include_optimizer=True)",
        "snippet": "def save_model(model, filepath, overwrite=True, include_optimizer=True):\n    \"\"\"Save a model to a HDF5 file.\n\n    Note: Please also see\n    [How can I install HDF5 or h5py to save my models in Keras?](\n        /getting-started/faq/\n        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n    in the FAQ for instructions on how to install `h5py`.\n\n    The saved model contains:\n        - the model's configuration (topology)\n        - the model's weights\n        - the model's optimizer's state (if any)\n\n    Thus the saved model can be reinstantiated in\n    the exact same state, without any of the code\n    used for model definition or training.\n\n    # Arguments\n        model: Keras model instance to be saved.\n        filepath: one of the following:\n            - string, path where to save the model, or\n            - h5py.File or h5py.Group object where to save the model\n        overwrite: Whether we should overwrite any existing\n            model at the target location, or instead\n            ask the user with a manual prompt.\n        include_optimizer: If True, save optimizer's state together.\n\n    # Raises\n        ImportError: if h5py is not available.\n    \"\"\"\n    if h5py is None:\n        raise ImportError('`save_model` requires h5py.')\n\n    if not isinstance(filepath, h5py.Group):\n        # If file exists and should not be overwritten.\n        if not overwrite and os.path.isfile(filepath):\n            proceed = ask_to_proceed_with_overwrite(filepath)\n            if not proceed:\n                return\n        opened_new_file = True\n    else:\n        opened_new_file = False\n\n    f = h5dict(filepath, mode='w')\n\n    try:\n        _serialize_model(model, f, include_optimizer)\n    finally:\n        if opened_new_file:\n            f.close()",
        "begin_line": 334,
        "end_line": 384,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.load_model#387",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.load_model(filepath, custom_objects=None, compile=True)",
        "snippet": "def load_model(filepath, custom_objects=None, compile=True):\n    \"\"\"Loads a model saved via `save_model`.\n\n    # Arguments\n        filepath: one of the following:\n            - string, path to the saved model, or\n            - h5py.File or h5py.Group object from which to load the model\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n        compile: Boolean, whether to compile the model\n            after loading.\n\n    # Returns\n        A Keras model instance. If an optimizer was found\n        as part of the saved model, the model is already\n        compiled. Otherwise, the model is uncompiled and\n        a warning will be displayed. When `compile` is set\n        to False, the compilation is omitted without any\n        warning.\n\n    # Raises\n        ImportError: if h5py is not available.\n        ValueError: In case of an invalid savefile.\n    \"\"\"\n    if h5py is None:\n        raise ImportError('`load_model` requires h5py.')\n    model = None\n    opened_new_file = not isinstance(filepath, h5py.Group)\n    f = h5dict(filepath, 'r')\n    try:\n        model = _deserialize_model(f, custom_objects, compile)\n    finally:\n        if opened_new_file:\n            f.close()\n    return model",
        "begin_line": 387,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.pickle_model#425",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.pickle_model(model)",
        "snippet": "def pickle_model(model):\n    d = {}\n    f = h5dict(d)\n    _serialize_model(model, f)\n    return d",
        "begin_line": 425,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.unpickle_model#432",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.unpickle_model(state)",
        "snippet": "def unpickle_model(state):\n    f = h5dict(state, mode='r')\n    return _deserialize_model(f)",
        "begin_line": 432,
        "end_line": 434,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.model_from_config#437",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.model_from_config(config, custom_objects=None)",
        "snippet": "def model_from_config(config, custom_objects=None):\n    \"\"\"Instantiates a Keras model from its config.\n\n    # Arguments\n        config: Configuration dictionary.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    # Returns\n        A Keras model instance (uncompiled).\n\n    # Raises\n        TypeError: if `config` is not a dictionary.\n    \"\"\"\n    if isinstance(config, list):\n        raise TypeError('`model_from_config` expects a dictionary, '\n                        'not a list. Maybe you meant to use '\n                        '`Sequential.from_config(config)`?')\n    from ..layers import deserialize\n    return deserialize(config, custom_objects=custom_objects)",
        "begin_line": 437,
        "end_line": 457,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.391435011269723e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.model_from_yaml#460",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.model_from_yaml(yaml_string, custom_objects=None)",
        "snippet": "def model_from_yaml(yaml_string, custom_objects=None):\n    \"\"\"Parses a yaml model configuration file and returns a model instance.\n\n    # Arguments\n        yaml_string: YAML string encoding a model configuration.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    # Returns\n        A Keras model instance (uncompiled).\n    \"\"\"\n    config = yaml.load(yaml_string)\n    from ..layers import deserialize\n    return deserialize(config, custom_objects=custom_objects)",
        "begin_line": 460,
        "end_line": 474,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.1,
            "pseudo_dstar_susp": 0.0007451564828614009,
            "pseudo_tarantula_susp": 0.010638297872340425,
            "pseudo_op2_susp": 0.0007451564828614009,
            "pseudo_barinel_susp": 0.010638297872340425
        }
    },
    {
        "name": "keras.engine.saving.model_from_json#477",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.model_from_json(json_string, custom_objects=None)",
        "snippet": "def model_from_json(json_string, custom_objects=None):\n    \"\"\"Parses a JSON model configuration file and returns a model instance.\n\n    # Arguments\n        json_string: JSON string encoding a model configuration.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    # Returns\n        A Keras model instance (uncompiled).\n    \"\"\"\n    config = json.loads(json_string)\n    from ..layers import deserialize\n    return deserialize(config, custom_objects=custom_objects)",
        "begin_line": 477,
        "end_line": 491,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.saving.save_attributes_to_hdf5_group#494",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.save_attributes_to_hdf5_group(group, name, data)",
        "snippet": "def save_attributes_to_hdf5_group(group, name, data):\n    \"\"\"Saves attributes (data) of the specified name into the HDF5 group.\n\n    This method deals with an inherent problem of HDF5 file which is not\n    able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n\n    # Arguments\n        group: A pointer to a HDF5 group.\n        name: A name of the attributes to save.\n        data: Attributes data to store.\n    \"\"\"\n    # Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\n    # because in that case even chunking the array would not make the saving\n    # possible.\n    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n\n    # Expecting this to never be true.\n    if len(bad_attributes) > 0:\n        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n                           'file because they are larger than %d bytes: %s'\n                           % (HDF5_OBJECT_HEADER_LIMIT,\n                              ', '.join([x for x in bad_attributes])))\n\n    data_npy = np.asarray(data)\n\n    num_chunks = 1\n    chunked_data = np.array_split(data_npy, num_chunks)\n\n    # This will never loop forever thanks to the test above.\n    while any(map(lambda x: x.nbytes > HDF5_OBJECT_HEADER_LIMIT, chunked_data)):\n        num_chunks += 1\n        chunked_data = np.array_split(data_npy, num_chunks)\n\n    if num_chunks > 1:\n        for chunk_id, chunk_data in enumerate(chunked_data):\n            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n    else:\n        group.attrs[name] = data",
        "begin_line": 494,
        "end_line": 531,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003194888178913738,
            "pseudo_dstar_susp": 0.0007067137809187279,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.0007067137809187279,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.engine.saving.load_attributes_from_hdf5_group#534",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.load_attributes_from_hdf5_group(group, name)",
        "snippet": "def load_attributes_from_hdf5_group(group, name):\n    \"\"\"Loads attributes of the specified name from the HDF5 group.\n\n    This method deals with an inherent problem\n    of HDF5 file which is not able to store\n    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n\n    # Arguments\n        group: A pointer to a HDF5 group.\n        name: A name of the attributes to load.\n\n    # Returns\n        data: Attributes data.\n    \"\"\"\n    if name in group.attrs:\n        data = [n.decode('utf8') for n in group.attrs[name]]\n    else:\n        data = []\n        chunk_id = 0\n        while ('%s%d' % (name, chunk_id)) in group.attrs:\n            data.extend([n.decode('utf8')\n                         for n in group.attrs['%s%d' % (name, chunk_id)]])\n            chunk_id += 1\n    return data",
        "begin_line": 534,
        "end_line": 557,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003194888178913738,
            "pseudo_dstar_susp": 0.0007067137809187279,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.0007067137809187279,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.engine.saving.save_weights_to_hdf5_group#560",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.save_weights_to_hdf5_group(f, layers)",
        "snippet": "def save_weights_to_hdf5_group(f, layers):\n    from .. import __version__ as keras_version\n\n    save_attributes_to_hdf5_group(\n        f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n    f.attrs['backend'] = K.backend().encode('utf8')\n    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n\n    for layer in layers:\n        g = f.create_group(layer.name)\n        symbolic_weights = layer.weights\n        weight_values = K.batch_get_value(symbolic_weights)\n        weight_names = []\n        for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n            if hasattr(w, 'name') and w.name:\n                name = str(w.name)\n            else:\n                name = 'param_' + str(i)\n            weight_names.append(name.encode('utf8'))\n        save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n        for name, val in zip(weight_names, weight_values):\n            param_dset = g.create_dataset(name, val.shape,\n                                          dtype=val.dtype)\n            if not val.shape:\n                # scalar\n                param_dset[()] = val\n            else:\n                param_dset[:] = val",
        "begin_line": 560,
        "end_line": 587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003194888178913738,
            "pseudo_dstar_susp": 0.0007067137809187279,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.0007067137809187279,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.engine.saving.preprocess_weights_for_loading#590",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.preprocess_weights_for_loading(layer, weights, original_keras_version=None, original_backend=None, reshape=False)",
        "snippet": "def preprocess_weights_for_loading(layer, weights,\n                                   original_keras_version=None,\n                                   original_backend=None,\n                                   reshape=False):\n    \"\"\"Converts layers weights from Keras 1 format to Keras 2 and also weights of CuDNN layers in Keras 2.\n\n    # Arguments\n        layer: Layer instance.\n        weights: List of weights values (Numpy arrays).\n        original_keras_version: Keras version for the weights, as a string.\n        original_backend: Keras backend the weights were trained with,\n            as a string.\n        reshape: Reshape weights to fit the layer when the correct number\n            of values are present but the shape does not match.\n\n    # Returns\n        A list of weights values (Numpy arrays).\n    \"\"\"\n    def convert_nested_bidirectional(weights):\n        \"\"\"Converts layers nested in `Bidirectional` wrapper by `preprocess_weights_for_loading()`.\n\n        # Arguments\n            weights: List of weights values (Numpy arrays).\n        # Returns\n            A list of weights values (Numpy arrays).\n        \"\"\"\n        num_weights_per_layer = len(weights) // 2\n        forward_weights = preprocess_weights_for_loading(layer.forward_layer,\n                                                         weights[:num_weights_per_layer],\n                                                         original_keras_version,\n                                                         original_backend)\n        backward_weights = preprocess_weights_for_loading(layer.backward_layer,\n                                                          weights[num_weights_per_layer:],\n                                                          original_keras_version,\n                                                          original_backend)\n        return forward_weights + backward_weights\n\n    def convert_nested_time_distributed(weights):\n        \"\"\"Converts layers nested in `TimeDistributed` wrapper by `preprocess_weights_for_loading()`.\n\n        # Arguments\n            weights: List of weights values (Numpy arrays).\n        # Returns\n            A list of weights values (Numpy arrays).\n        \"\"\"\n        return preprocess_weights_for_loading(\n            layer.layer, weights, original_keras_version, original_backend)\n\n    def convert_nested_model(weights):\n        \"\"\"Converts layers nested in `Model` or `Sequential` by `preprocess_weights_for_loading()`.\n\n        # Arguments\n            weights: List of weights values (Numpy arrays).\n        # Returns\n            A list of weights values (Numpy arrays).\n        \"\"\"\n        new_weights = []\n        # trainable weights\n        for sublayer in layer.layers:\n            num_weights = len(sublayer.trainable_weights)\n            if num_weights > 0:\n                new_weights.extend(preprocess_weights_for_loading(\n                    layer=sublayer,\n                    weights=weights[:num_weights],\n                    original_keras_version=original_keras_version,\n                    original_backend=original_backend))\n                weights = weights[num_weights:]\n\n        # non-trainable weights\n        for sublayer in layer.layers:\n            num_weights = len([l for l in sublayer.weights\n                               if l not in sublayer.trainable_weights])\n            if num_weights > 0:\n                new_weights.extend(preprocess_weights_for_loading(\n                    layer=sublayer,\n                    weights=weights[:num_weights],\n                    original_keras_version=original_keras_version,\n                    original_backend=original_backend))\n                weights = weights[num_weights:]\n        return new_weights\n\n    # Convert layers nested in Bidirectional/TimeDistributed/Model/Sequential.\n    # Both transformation should be ran for both Keras 1->2 conversion\n    # and for conversion of CuDNN layers.\n    if layer.__class__.__name__ == 'Bidirectional':\n        weights = convert_nested_bidirectional(weights)\n    if layer.__class__.__name__ == 'TimeDistributed':\n        weights = convert_nested_time_distributed(weights)\n    elif layer.__class__.__name__ in ['Model', 'Sequential']:\n        weights = convert_nested_model(weights)\n\n    if original_keras_version == '1':\n        if layer.__class__.__name__ == 'TimeDistributed':\n            weights = preprocess_weights_for_loading(layer.layer,\n                                                     weights,\n                                                     original_keras_version,\n                                                     original_backend)\n\n        if layer.__class__.__name__ == 'Conv1D':\n            shape = weights[0].shape\n            # Handle Keras 1.1 format\n            if shape[:2] != (layer.kernel_size[0], 1) or shape[3] != layer.filters:\n                # Legacy shape:\n                # (filters, input_dim, filter_length, 1)\n                assert shape[0] == layer.filters and shape[2:] == (layer.kernel_size[0], 1)\n                weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n            weights[0] = weights[0][:, 0, :, :]\n\n        if layer.__class__.__name__ == 'Conv2D':\n            if layer.data_format == 'channels_first':\n                # old: (filters, stack_size, kernel_rows, kernel_cols)\n                # new: (kernel_rows, kernel_cols, stack_size, filters)\n                weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n\n        if layer.__class__.__name__ == 'Conv2DTranspose':\n            if layer.data_format == 'channels_last':\n                # old: (kernel_rows, kernel_cols, stack_size, filters)\n                # new: (kernel_rows, kernel_cols, filters, stack_size)\n                weights[0] = np.transpose(weights[0], (0, 1, 3, 2))\n            if layer.data_format == 'channels_first':\n                # old: (filters, stack_size, kernel_rows, kernel_cols)\n                # new: (kernel_rows, kernel_cols, filters, stack_size)\n                weights[0] = np.transpose(weights[0], (2, 3, 0, 1))\n\n        if layer.__class__.__name__ == 'Conv3D':\n            if layer.data_format == 'channels_first':\n                # old: (filters, stack_size, ...)\n                # new: (..., stack_size, filters)\n                weights[0] = np.transpose(weights[0], (2, 3, 4, 1, 0))\n\n        if layer.__class__.__name__ == 'GRU':\n            if len(weights) == 9:\n                kernel = np.concatenate([weights[0],\n                                         weights[3],\n                                         weights[6]], axis=-1)\n                recurrent_kernel = np.concatenate([weights[1],\n                                                   weights[4],\n                                                   weights[7]], axis=-1)\n                bias = np.concatenate([weights[2],\n                                       weights[5],\n                                       weights[8]], axis=-1)\n                weights = [kernel, recurrent_kernel, bias]\n\n        if layer.__class__.__name__ == 'LSTM':\n            if len(weights) == 12:\n                # old: i, c, f, o\n                # new: i, f, c, o\n                kernel = np.concatenate([weights[0],\n                                         weights[6],\n                                         weights[3],\n                                         weights[9]], axis=-1)\n                recurrent_kernel = np.concatenate([weights[1],\n                                                   weights[7],\n                                                   weights[4],\n                                                   weights[10]], axis=-1)\n                bias = np.concatenate([weights[2],\n                                       weights[8],\n                                       weights[5],\n                                       weights[11]], axis=-1)\n                weights = [kernel, recurrent_kernel, bias]\n\n        if layer.__class__.__name__ == 'ConvLSTM2D':\n            if len(weights) == 12:\n                kernel = np.concatenate([weights[0],\n                                         weights[6],\n                                         weights[3],\n                                         weights[9]], axis=-1)\n                recurrent_kernel = np.concatenate([weights[1],\n                                                   weights[7],\n                                                   weights[4],\n                                                   weights[10]], axis=-1)\n                bias = np.concatenate([weights[2],\n                                       weights[8],\n                                       weights[5],\n                                       weights[11]], axis=-1)\n                if layer.data_format == 'channels_first':\n                    # old: (filters, stack_size, kernel_rows, kernel_cols)\n                    # new: (kernel_rows, kernel_cols, stack_size, filters)\n                    kernel = np.transpose(kernel, (2, 3, 1, 0))\n                    recurrent_kernel = np.transpose(recurrent_kernel,\n                                                    (2, 3, 1, 0))\n                weights = [kernel, recurrent_kernel, bias]\n\n    conv_layers = ['Conv1D',\n                   'Conv2D',\n                   'Conv3D',\n                   'Conv2DTranspose',\n                   'ConvLSTM2D']\n    if layer.__class__.__name__ in conv_layers:\n        layer_weights_shape = K.int_shape(layer.weights[0])\n        if _need_convert_kernel(original_backend):\n            weights[0] = conv_utils.convert_kernel(weights[0])\n            if layer.__class__.__name__ == 'ConvLSTM2D':\n                weights[1] = conv_utils.convert_kernel(weights[1])\n        if reshape and layer_weights_shape != weights[0].shape:\n            if weights[0].size != np.prod(layer_weights_shape):\n                raise ValueError('Weights must be of equal size to ' +\n                                 'apply a reshape operation. ' +\n                                 'Layer ' + layer.name +\n                                 '\\'s weights have shape ' +\n                                 str(layer_weights_shape) + ' and size ' +\n                                 str(np.prod(layer_weights_shape)) + '. ' +\n                                 'The weights for loading have shape ' +\n                                 str(weights[0].shape) + ' and size ' +\n                                 str(weights[0].size) + '. ')\n            weights[0] = np.reshape(weights[0], layer_weights_shape)\n        elif layer_weights_shape != weights[0].shape:\n            weights[0] = np.transpose(weights[0], (3, 2, 0, 1))\n            if layer.__class__.__name__ == 'ConvLSTM2D':\n                weights[1] = np.transpose(weights[1], (3, 2, 0, 1))\n\n    # convert CuDNN layers\n    weights = _convert_rnn_weights(layer, weights)\n\n    return weights",
        "begin_line": 590,
        "end_line": 804,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003497726477789437,
            "pseudo_dstar_susp": 0.00031725888324873094,
            "pseudo_tarantula_susp": 0.0005446623093681918,
            "pseudo_op2_susp": 0.00031725888324873094,
            "pseudo_barinel_susp": 0.0005446623093681918
        }
    },
    {
        "name": "keras.engine.saving.convert_nested_bidirectional#608",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.convert_nested_bidirectional(weights)",
        "snippet": "    def convert_nested_bidirectional(weights):\n        \"\"\"Converts layers nested in `Bidirectional` wrapper by `preprocess_weights_for_loading()`.\n\n        # Arguments\n            weights: List of weights values (Numpy arrays).\n        # Returns\n            A list of weights values (Numpy arrays).\n        \"\"\"\n        num_weights_per_layer = len(weights) // 2\n        forward_weights = preprocess_weights_for_loading(layer.forward_layer,\n                                                         weights[:num_weights_per_layer],\n                                                         original_keras_version,\n                                                         original_backend)\n        backward_weights = preprocess_weights_for_loading(layer.backward_layer,\n                                                          weights[num_weights_per_layer:],\n                                                          original_keras_version,\n                                                          original_backend)\n        return forward_weights + backward_weights",
        "begin_line": 608,
        "end_line": 625,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034698126301179735,
            "pseudo_dstar_susp": 0.0003154574132492114,
            "pseudo_tarantula_susp": 0.0005313496280552603,
            "pseudo_op2_susp": 0.0003154574132492114,
            "pseudo_barinel_susp": 0.0005313496280552603
        }
    },
    {
        "name": "keras.engine.saving.convert_nested_time_distributed#627",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.convert_nested_time_distributed(weights)",
        "snippet": "    def convert_nested_time_distributed(weights):\n        \"\"\"Converts layers nested in `TimeDistributed` wrapper by `preprocess_weights_for_loading()`.\n\n        # Arguments\n            weights: List of weights values (Numpy arrays).\n        # Returns\n            A list of weights values (Numpy arrays).\n        \"\"\"\n        return preprocess_weights_for_loading(\n            layer.layer, weights, original_keras_version, original_backend)",
        "begin_line": 627,
        "end_line": 636,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003454231433506045,
            "pseudo_dstar_susp": 0.000315059861373661,
            "pseudo_tarantula_susp": 0.000481000481000481,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.000481000481000481
        }
    },
    {
        "name": "keras.engine.saving.convert_nested_model#638",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.convert_nested_model(weights)",
        "snippet": "    def convert_nested_model(weights):\n        \"\"\"Converts layers nested in `Model` or `Sequential` by `preprocess_weights_for_loading()`.\n\n        # Arguments\n            weights: List of weights values (Numpy arrays).\n        # Returns\n            A list of weights values (Numpy arrays).\n        \"\"\"\n        new_weights = []\n        # trainable weights\n        for sublayer in layer.layers:\n            num_weights = len(sublayer.trainable_weights)\n            if num_weights > 0:\n                new_weights.extend(preprocess_weights_for_loading(\n                    layer=sublayer,\n                    weights=weights[:num_weights],\n                    original_keras_version=original_keras_version,\n                    original_backend=original_backend))\n                weights = weights[num_weights:]\n\n        # non-trainable weights\n        for sublayer in layer.layers:\n            num_weights = len([l for l in sublayer.weights\n                               if l not in sublayer.trainable_weights])\n            if num_weights > 0:\n                new_weights.extend(preprocess_weights_for_loading(\n                    layer=sublayer,\n                    weights=weights[:num_weights],\n                    original_keras_version=original_keras_version,\n                    original_backend=original_backend))\n                weights = weights[num_weights:]\n        return new_weights",
        "begin_line": 638,
        "end_line": 669,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003444712366517396,
            "pseudo_dstar_susp": 0.0003147623544224111,
            "pseudo_tarantula_susp": 0.00047551117451260106,
            "pseudo_op2_susp": 0.0003147623544224111,
            "pseudo_barinel_susp": 0.00047551117451260106
        }
    },
    {
        "name": "keras.engine.saving._convert_rnn_weights#807",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving._convert_rnn_weights(layer, weights)",
        "snippet": "def _convert_rnn_weights(layer, weights):\n    \"\"\"Converts weights for RNN layers between native and CuDNN format.\n\n    Input kernels for each gate are transposed and converted between Fortran\n    and C layout, recurrent kernels are transposed. For LSTM biases are summed/\n    split in half, for GRU biases are reshaped.\n\n    Weights can be converted in both directions between `LSTM` and`CuDNNSLTM`\n    and between `CuDNNGRU` and `GRU(reset_after=True)`. Default `GRU` is not\n    compatible with `CuDNNGRU`.\n\n    For missing biases in `LSTM`/`GRU` (`use_bias=False`),\n    no conversion is made.\n\n    # Arguments\n        layer: Target layer instance.\n        weights: List of source weights values (input kernels, recurrent\n            kernels, [biases]) (Numpy arrays).\n\n    # Returns\n        A list of converted weights values (Numpy arrays).\n\n    # Raises\n        ValueError: for incompatible GRU layer/weights or incompatible biases\n    \"\"\"\n\n    def transform_kernels(kernels, func, n_gates):\n        \"\"\"Transforms kernel for each gate separately using given function.\n\n        # Arguments\n            kernels: Stacked array of kernels for individual gates.\n            func: Function applied to kernel of each gate.\n            n_gates: Number of gates (4 for LSTM, 3 for GRU).\n        # Returns\n            Stacked array of transformed kernels.\n        \"\"\"\n        return np.hstack([func(k) for k in np.hsplit(kernels, n_gates)])\n\n    def transpose_input(from_cudnn):\n        \"\"\"Makes a function that transforms input kernels from/to CuDNN format.\n\n        It keeps the shape, but changes between the layout (Fortran/C). Eg.:\n\n        ```\n        Keras                 CuDNN\n        [[0, 1, 2],  <--->  [[0, 2, 4],\n         [3, 4, 5]]          [1, 3, 5]]\n        ```\n\n        It can be passed to `transform_kernels()`.\n\n        # Arguments\n            from_cudnn: `True` if source weights are in CuDNN format, `False`\n                if they're in plain Keras format.\n        # Returns\n            Function that converts input kernel to the other format.\n        \"\"\"\n        order = 'F' if from_cudnn else 'C'\n\n        def transform(kernel):\n            return kernel.T.reshape(kernel.shape, order=order)\n\n        return transform\n\n    target_class = layer.__class__.__name__\n\n    # convert the weights between CuDNNLSTM and LSTM\n    if target_class in ['LSTM', 'CuDNNLSTM'] and len(weights) == 3:\n        # determine if we're loading a CuDNNLSTM layer\n        # from the number of bias weights:\n        # CuDNNLSTM has (units * 8) weights; while LSTM has (units * 4)\n        # if there's no bias weight in the file, skip this conversion\n        units = weights[1].shape[0]\n        bias_shape = weights[2].shape\n        n_gates = 4\n\n        if bias_shape == (2 * units * n_gates,):\n            source = 'CuDNNLSTM'\n        elif bias_shape == (units * n_gates,):\n            source = 'LSTM'\n        else:\n            raise ValueError('Invalid bias shape: ' + str(bias_shape))\n\n        def convert_weights(weights, from_cudnn=True):\n            # transpose (and reshape) input and recurrent kernels\n            kernels = transform_kernels(weights[0],\n                                        transpose_input(from_cudnn),\n                                        n_gates)\n            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n            if from_cudnn:\n                # merge input and recurrent biases into a single set\n                biases = np.sum(np.split(weights[2], 2, axis=0), axis=0)\n            else:\n                # Split single set of biases evenly to two sets. The way of\n                # splitting doesn't matter as long as the two sets sum is kept.\n                biases = np.tile(0.5 * weights[2], 2)\n            return [kernels, recurrent_kernels, biases]\n\n        if source != target_class:\n            weights = convert_weights(weights, from_cudnn=source == 'CuDNNLSTM')\n\n    # convert the weights between CuDNNGRU and GRU(reset_after=True)\n    if target_class in ['GRU', 'CuDNNGRU'] and len(weights) == 3:\n        # We can determine the source of the weights from the shape of the bias.\n        # If there is no bias we skip the conversion since CuDNNGRU always has biases.\n\n        units = weights[1].shape[0]\n        bias_shape = weights[2].shape\n        n_gates = 3\n\n        def convert_weights(weights, from_cudnn=True):\n            kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n            biases = np.array(weights[2]).reshape((2, -1) if from_cudnn else -1)\n            return [kernels, recurrent_kernels, biases]\n\n        if bias_shape == (2 * units * n_gates,):\n            source = 'CuDNNGRU'\n        elif bias_shape == (2, units * n_gates):\n            source = 'GRU(reset_after=True)'\n        elif bias_shape == (units * n_gates,):\n            source = 'GRU(reset_after=False)'\n        else:\n            raise ValueError('Invalid bias shape: ' + str(bias_shape))\n\n        if target_class == 'CuDNNGRU':\n            target = 'CuDNNGRU'\n        elif layer.reset_after:\n            target = 'GRU(reset_after=True)'\n        else:\n            target = 'GRU(reset_after=False)'\n\n        # only convert between different types\n        if source != target:\n            types = (source, target)\n            if 'GRU(reset_after=False)' in types:\n                raise ValueError('%s is not compatible with %s' % types)\n            if source == 'CuDNNGRU':\n                weights = convert_weights(weights, from_cudnn=True)\n            elif source == 'GRU(reset_after=True)':\n                weights = convert_weights(weights, from_cudnn=False)\n\n    return weights",
        "begin_line": 807,
        "end_line": 949,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003497726477789437,
            "pseudo_dstar_susp": 0.00031725888324873094,
            "pseudo_tarantula_susp": 0.0005446623093681918,
            "pseudo_op2_susp": 0.00031725888324873094,
            "pseudo_barinel_susp": 0.0005446623093681918
        }
    },
    {
        "name": "keras.engine.saving.transform_kernels#833",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.transform_kernels(kernels, func, n_gates)",
        "snippet": "    def transform_kernels(kernels, func, n_gates):\n        \"\"\"Transforms kernel for each gate separately using given function.\n\n        # Arguments\n            kernels: Stacked array of kernels for individual gates.\n            func: Function applied to kernel of each gate.\n            n_gates: Number of gates (4 for LSTM, 3 for GRU).\n        # Returns\n            Stacked array of transformed kernels.\n        \"\"\"\n        return np.hstack([func(k) for k in np.hsplit(kernels, n_gates)])",
        "begin_line": 833,
        "end_line": 843,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003497726477789437,
            "pseudo_dstar_susp": 0.00031725888324873094,
            "pseudo_tarantula_susp": 0.0005446623093681918,
            "pseudo_op2_susp": 0.00031725888324873094,
            "pseudo_barinel_susp": 0.0005446623093681918
        }
    },
    {
        "name": "keras.engine.saving.transpose_input#845",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.transpose_input(from_cudnn)",
        "snippet": "    def transpose_input(from_cudnn):\n        \"\"\"Makes a function that transforms input kernels from/to CuDNN format.\n\n        It keeps the shape, but changes between the layout (Fortran/C). Eg.:\n\n        ```\n        Keras                 CuDNN\n        [[0, 1, 2],  <--->  [[0, 2, 4],\n         [3, 4, 5]]          [1, 3, 5]]\n        ```\n\n        It can be passed to `transform_kernels()`.\n\n        # Arguments\n            from_cudnn: `True` if source weights are in CuDNN format, `False`\n                if they're in plain Keras format.\n        # Returns\n            Function that converts input kernel to the other format.\n        \"\"\"\n        order = 'F' if from_cudnn else 'C'\n\n        def transform(kernel):\n            return kernel.T.reshape(kernel.shape, order=order)\n\n        return transform",
        "begin_line": 845,
        "end_line": 869,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003497726477789437,
            "pseudo_dstar_susp": 0.00031725888324873094,
            "pseudo_tarantula_susp": 0.0005446623093681918,
            "pseudo_op2_susp": 0.00031725888324873094,
            "pseudo_barinel_susp": 0.0005446623093681918
        }
    },
    {
        "name": "keras.engine.saving.convert_weights#890",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.convert_weights(weights, from_cudnn=True)",
        "snippet": "        def convert_weights(weights, from_cudnn=True):\n            # transpose (and reshape) input and recurrent kernels\n            kernels = transform_kernels(weights[0],\n                                        transpose_input(from_cudnn),\n                                        n_gates)\n            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n            if from_cudnn:\n                # merge input and recurrent biases into a single set\n                biases = np.sum(np.split(weights[2], 2, axis=0), axis=0)\n            else:\n                # Split single set of biases evenly to two sets. The way of\n                # splitting doesn't matter as long as the two sets sum is kept.\n                biases = np.tile(0.5 * weights[2], 2)\n            return [kernels, recurrent_kernels, biases]",
        "begin_line": 890,
        "end_line": 903,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.convert_weights#917",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.convert_weights(weights, from_cudnn=True)",
        "snippet": "        def convert_weights(weights, from_cudnn=True):\n            kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n            biases = np.array(weights[2]).reshape((2, -1) if from_cudnn else -1)\n            return [kernels, recurrent_kernels, biases]",
        "begin_line": 917,
        "end_line": 921,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving._need_convert_kernel#952",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving._need_convert_kernel(original_backend)",
        "snippet": "def _need_convert_kernel(original_backend):\n    \"\"\"Checks if conversion on kernel matrices is required during weight loading.\n\n    The convolution operation is implemented differently in different backends.\n    While TH implements convolution, TF and CNTK implement the correlation operation.\n    So the channel axis needs to be flipped when we're loading TF weights onto a TH model,\n    or vice versa. However, there's no conversion required between TF and CNTK.\n\n    # Arguments\n        original_backend: Keras backend the weights were trained with, as a string.\n\n    # Returns\n        `True` if conversion on kernel matrices is required, otherwise `False`.\n    \"\"\"\n    if original_backend is None:\n        # backend information not available\n        return False\n    uses_correlation = {'tensorflow': True,\n                        'theano': False,\n                        'cntk': True}\n    if original_backend not in uses_correlation:\n        # By default, do not convert the kernels if the original backend is unknown\n        return False\n    if K.backend() in uses_correlation:\n        current_uses_correlation = uses_correlation[K.backend()]\n    else:\n        # Assume unknown backends use correlation\n        current_uses_correlation = True\n    return uses_correlation[original_backend] != current_uses_correlation",
        "begin_line": 952,
        "end_line": 980,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.saving.load_weights_from_hdf5_group#983",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.load_weights_from_hdf5_group(f, layers, reshape=False)",
        "snippet": "def load_weights_from_hdf5_group(f, layers, reshape=False):\n    \"\"\"Implements topological (order-based) weight loading.\n\n    # Arguments\n        f: A pointer to a HDF5 group.\n        layers: a list of target layers.\n        reshape: Reshape weights to fit the layer when the correct number\n            of values are present but the shape does not match.\n\n    # Raises\n        ValueError: in case of mismatch between provided layers\n            and weights file.\n    \"\"\"\n    if 'keras_version' in f.attrs:\n        original_keras_version = f.attrs['keras_version'].decode('utf8')\n    else:\n        original_keras_version = '1'\n    if 'backend' in f.attrs:\n        original_backend = f.attrs['backend'].decode('utf8')\n    else:\n        original_backend = None\n\n    filtered_layers = []\n    for layer in layers:\n        weights = layer.weights\n        if weights:\n            filtered_layers.append(layer)\n\n    layer_names = load_attributes_from_hdf5_group(f, 'layer_names')\n    filtered_layer_names = []\n    for name in layer_names:\n        g = f[name]\n        weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n        if weight_names:\n            filtered_layer_names.append(name)\n    layer_names = filtered_layer_names\n    if len(layer_names) != len(filtered_layers):\n        raise ValueError('You are trying to load a weight file '\n                         'containing ' + str(len(layer_names)) +\n                         ' layers into a model with ' +\n                         str(len(filtered_layers)) + ' layers.')\n\n    # We batch weight value assignments in a single backend call\n    # which provides a speedup in TensorFlow.\n    weight_value_tuples = []\n    for k, name in enumerate(layer_names):\n        g = f[name]\n        weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n        weight_values = [np.asarray(g[weight_name]) for weight_name in weight_names]\n        layer = filtered_layers[k]\n        symbolic_weights = layer.weights\n        weight_values = preprocess_weights_for_loading(layer,\n                                                       weight_values,\n                                                       original_keras_version,\n                                                       original_backend,\n                                                       reshape=reshape)\n        if len(weight_values) != len(symbolic_weights):\n            raise ValueError('Layer #' + str(k) +\n                             ' (named \"' + layer.name +\n                             '\" in the current model) was found to '\n                             'correspond to layer ' + name +\n                             ' in the save file. '\n                             'However the new layer ' + layer.name +\n                             ' expects ' + str(len(symbolic_weights)) +\n                             ' weights, but the saved weights have ' +\n                             str(len(weight_values)) +\n                             ' elements.')\n        weight_value_tuples += zip(symbolic_weights, weight_values)\n    K.batch_set_value(weight_value_tuples)",
        "begin_line": 983,
        "end_line": 1051,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0007246376811594203,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0007246376811594203,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.saving.load_weights_from_hdf5_group_by_name#1054",
        "src_path": "keras/engine/saving.py",
        "class_name": "keras.engine.saving",
        "signature": "keras.engine.saving.load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False, reshape=False)",
        "snippet": "def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n                                         reshape=False):\n    \"\"\"Implements name-based weight loading.\n\n    (instead of topological weight loading).\n\n    Layers that have no matching name are skipped.\n\n    # Arguments\n        f: A pointer to a HDF5 group.\n        layers: A list of target layers.\n        skip_mismatch: Boolean, whether to skip loading of layers\n            where there is a mismatch in the number of weights,\n            or a mismatch in the shape of the weights.\n        reshape: Reshape weights to fit the layer when the correct number\n            of values are present but the shape does not match.\n\n    # Raises\n        ValueError: in case of mismatch between provided layers\n            and weights file and skip_mismatch=False.\n    \"\"\"\n    if 'keras_version' in f.attrs:\n        original_keras_version = f.attrs['keras_version'].decode('utf8')\n    else:\n        original_keras_version = '1'\n    if 'backend' in f.attrs:\n        original_backend = f.attrs['backend'].decode('utf8')\n    else:\n        original_backend = None\n\n    # New file format.\n    layer_names = load_attributes_from_hdf5_group(f, 'layer_names')\n\n    # Reverse index of layer name to list of layers with name.\n    index = {}\n    for layer in layers:\n        if layer.name:\n            index.setdefault(layer.name, []).append(layer)\n\n    # We batch weight value assignments in a single backend call\n    # which provides a speedup in TensorFlow.\n    weight_value_tuples = []\n    for k, name in enumerate(layer_names):\n        g = f[name]\n        weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n        weight_values = [np.asarray(g[weight_name]) for weight_name in weight_names]\n\n        for layer in index.get(name, []):\n            symbolic_weights = layer.weights\n            weight_values = preprocess_weights_for_loading(\n                layer,\n                weight_values,\n                original_keras_version,\n                original_backend,\n                reshape=reshape)\n            if len(weight_values) != len(symbolic_weights):\n                if skip_mismatch:\n                    warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n                                  ' due to mismatch in number of weights' +\n                                  ' ({} vs {}).'.format(len(symbolic_weights), len(weight_values)))\n                    continue\n                else:\n                    raise ValueError('Layer #' + str(k) +\n                                     ' (named \"' + layer.name +\n                                     '\") expects ' +\n                                     str(len(symbolic_weights)) +\n                                     ' weight(s), but the saved weights' +\n                                     ' have ' + str(len(weight_values)) +\n                                     ' element(s).')\n            # Set values.\n            for i in range(len(weight_values)):\n                if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n                    if skip_mismatch:\n                        warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n                                      ' due to mismatch in shape' +\n                                      ' ({} vs {}).'.format(\n                                          symbolic_weights[i].shape,\n                                          weight_values[i].shape))\n                        continue\n                    else:\n                        raise ValueError('Layer #' + str(k) +\n                                         ' (named \"' + layer.name +\n                                         '\"), weight ' +\n                                         str(symbolic_weights[i]) +\n                                         ' has shape {}'.format(K.int_shape(symbolic_weights[i])) +\n                                         ', but the saved weight has shape ' +\n                                         str(weight_values[i].shape) + '.')\n                else:\n                    weight_value_tuples.append((symbolic_weights[i],\n                                                weight_values[i]))\n\n    K.batch_set_value(weight_value_tuples)",
        "begin_line": 1054,
        "end_line": 1145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.conv_utils.normalize_tuple#12",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.normalize_tuple(value, n, name)",
        "snippet": "def normalize_tuple(value, n, name):\n    \"\"\"Transforms a single int or iterable of ints into an int tuple.\n\n    # Arguments\n        value: The value to validate and convert. Could be an int, or any iterable\n          of ints.\n        n: The size of the tuple to be returned.\n        name: The name of the argument being validated, e.g. `strides` or\n          `kernel_size`. This is only used to format error messages.\n\n    # Returns\n        A tuple of n integers.\n\n    # Raises\n        ValueError: If something else than an int/long or iterable thereof was\n        passed.\n    \"\"\"\n    if isinstance(value, int):\n        return (value,) * n\n    else:\n        try:\n            value_tuple = tuple(value)\n        except TypeError:\n            raise ValueError('The `' + name + '` argument must be a tuple of ' +\n                             str(n) + ' integers. Received: ' + str(value))\n        if len(value_tuple) != n:\n            raise ValueError('The `' + name + '` argument must be a tuple of ' +\n                             str(n) + ' integers. Received: ' + str(value))\n        for single_value in value_tuple:\n            try:\n                int(single_value)\n            except ValueError:\n                raise ValueError('The `' + name + '` argument must be a tuple of ' +\n                                 str(n) + ' integers. Received: ' + str(value) + ' '\n                                 'including element ' + str(single_value) + ' of '\n                                 'type ' + str(type(single_value)))\n    return value_tuple",
        "begin_line": 12,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034698126301179735,
            "pseudo_dstar_susp": 0.0005906674542232723,
            "pseudo_tarantula_susp": 0.0003215434083601286,
            "pseudo_op2_susp": 0.0005906674542232723,
            "pseudo_barinel_susp": 0.0003215434083601286
        }
    },
    {
        "name": "keras.utils.conv_utils.normalize_padding#51",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.normalize_padding(value)",
        "snippet": "def normalize_padding(value):\n    padding = value.lower()\n    allowed = {'valid', 'same', 'causal'}\n    if K.backend() == 'theano':\n        allowed.add('full')\n    if padding not in allowed:\n        raise ValueError('The `padding` argument must be one of \"valid\", \"same\" '\n                         '(or \"causal\" for Conv1D). Received: ' + str(padding))\n    return padding",
        "begin_line": 51,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00035323207347227127,
            "pseudo_dstar_susp": 0.0005963029218843172,
            "pseudo_tarantula_susp": 0.0003245699448231094,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0003245699448231094
        }
    },
    {
        "name": "keras.utils.conv_utils.convert_kernel#62",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.convert_kernel(kernel)",
        "snippet": "def convert_kernel(kernel):\n    \"\"\"Converts a Numpy kernel matrix from Theano format to TensorFlow format.\n\n    Also works reciprocally, since the transformation is its own inverse.\n\n    # Arguments\n        kernel: Numpy array (3D, 4D or 5D).\n\n    # Returns\n        The converted kernel.\n\n    # Raises\n        ValueError: in case of invalid kernel shape or invalid data_format.\n    \"\"\"\n    kernel = np.asarray(kernel)\n    if not 3 <= kernel.ndim <= 5:\n        raise ValueError('Invalid kernel shape:', kernel.shape)\n    slices = [slice(None, None, -1) for _ in range(kernel.ndim)]\n    no_flip = (slice(None, None), slice(None, None))\n    slices[-2:] = no_flip\n    return np.copy(kernel[slices])",
        "begin_line": 62,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.conv_utils.conv_output_length#85",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.conv_output_length(input_length, filter_size, padding, stride, dilation=1)",
        "snippet": "def conv_output_length(input_length, filter_size,\n                       padding, stride, dilation=1):\n    \"\"\"Determines output length of a convolution given input length.\n\n    # Arguments\n        input_length: integer.\n        filter_size: integer.\n        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n        stride: integer.\n        dilation: dilation rate, integer.\n\n    # Returns\n        The output length (integer).\n    \"\"\"\n    if input_length is None:\n        return None\n    assert padding in {'same', 'valid', 'full', 'causal'}\n    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n    if padding == 'same':\n        output_length = input_length\n    elif padding == 'valid':\n        output_length = input_length - dilated_filter_size + 1\n    elif padding == 'causal':\n        output_length = input_length\n    elif padding == 'full':\n        output_length = input_length + dilated_filter_size - 1\n    return (output_length + stride - 1) // stride",
        "begin_line": 85,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0006222775357809583,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0006222775357809583,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.utils.conv_utils.conv_input_length#114",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.conv_input_length(output_length, filter_size, padding, stride)",
        "snippet": "def conv_input_length(output_length, filter_size, padding, stride):\n    \"\"\"Determines input length of a convolution given output length.\n\n    # Arguments\n        output_length: integer.\n        filter_size: integer.\n        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n        stride: integer.\n\n    # Returns\n        The input length (integer).\n    \"\"\"\n    if output_length is None:\n        return None\n    assert padding in {'same', 'valid', 'full'}\n    if padding == 'same':\n        pad = filter_size // 2\n    elif padding == 'valid':\n        pad = 0\n    elif padding == 'full':\n        pad = filter_size - 1\n    return (output_length - 1) * stride - 2 * pad + filter_size",
        "begin_line": 114,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.conv_utils.deconv_length#138",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.deconv_length(dim_size, stride_size, kernel_size, padding, output_padding, dilation=1)",
        "snippet": "def deconv_length(dim_size, stride_size, kernel_size, padding,\n                  output_padding, dilation=1):\n    \"\"\"Determines output length of a transposed convolution given input length.\n\n    # Arguments\n        dim_size: Integer, the input length.\n        stride_size: Integer, the stride along the dimension of `dim_size`.\n        kernel_size: Integer, the kernel size along the dimension of\n            `dim_size`.\n        padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n        output_padding: Integer, amount of padding along the output dimension,\n            Can be set to `None` in which case the output length is inferred.\n        dilation: dilation rate, integer.\n\n    # Returns\n        The output length (integer).\n    \"\"\"\n    assert padding in {'same', 'valid', 'full'}\n    if dim_size is None:\n        return None\n\n    # Get the dilated kernel size\n    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n\n    # Infer length if output padding is None, else compute the exact length\n    if output_padding is None:\n        if padding == 'valid':\n            dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)\n        elif padding == 'full':\n            dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)\n        elif padding == 'same':\n            dim_size = dim_size * stride_size\n    else:\n        if padding == 'same':\n            pad = kernel_size // 2\n        elif padding == 'valid':\n            pad = 0\n        elif padding == 'full':\n            pad = kernel_size - 1\n\n        dim_size = ((dim_size - 1) * stride_size + kernel_size - 2 * pad +\n                    output_padding)\n\n    return dim_size",
        "begin_line": 138,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.get_file#123",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None)",
        "snippet": "def get_file(fname,\n             origin,\n             untar=False,\n             md5_hash=None,\n             file_hash=None,\n             cache_subdir='datasets',\n             hash_algorithm='auto',\n             extract=False,\n             archive_format='auto',\n             cache_dir=None):\n    \"\"\"Downloads a file from a URL if it not already in the cache.\n\n    By default the file at the url `origin` is downloaded to the\n    cache_dir `~/.keras`, placed in the cache_subdir `datasets`,\n    and given the filename `fname`. The final location of a file\n    `example.txt` would therefore be `~/.keras/datasets/example.txt`.\n\n    Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.\n    Passing a hash will verify the file after download. The command line\n    programs `shasum` and `sha256sum` can compute the hash.\n\n    # Arguments\n        fname: Name of the file. If an absolute path `/path/to/file.txt` is\n            specified the file will be saved at that location.\n        origin: Original URL of the file.\n        untar: Deprecated in favor of 'extract'.\n            boolean, whether the file should be decompressed\n        md5_hash: Deprecated in favor of 'file_hash'.\n            md5 hash of the file for verification\n        file_hash: The expected hash string of the file after download.\n            The sha256 and md5 hash algorithms are both supported.\n        cache_subdir: Subdirectory under the Keras cache dir where the file is\n            saved. If an absolute path `/path/to/folder` is\n            specified the file will be saved at that location.\n        hash_algorithm: Select the hash algorithm to verify the file.\n            options are 'md5', 'sha256', and 'auto'.\n            The default 'auto' detects the hash algorithm in use.\n        extract: True tries extracting the file as an Archive, like tar or zip.\n        archive_format: Archive format to try for extracting the file.\n            Options are 'auto', 'tar', 'zip', and None.\n            'tar' includes tar, tar.gz, and tar.bz files.\n            The default 'auto' is ['tar', 'zip'].\n            None or an empty list will return no matches found.\n        cache_dir: Location to store cached files, when None it\n            defaults to the [Keras Directory](/faq/#where-is-the-keras-configuration-filed-stored).\n\n    # Returns\n        Path to the downloaded file\n    \"\"\"  # noqa\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser('~'), '.keras')\n    if md5_hash is not None and file_hash is None:\n        file_hash = md5_hash\n        hash_algorithm = 'md5'\n    datadir_base = os.path.expanduser(cache_dir)\n    if not os.access(datadir_base, os.W_OK):\n        datadir_base = os.path.join('/tmp', '.keras')\n    datadir = os.path.join(datadir_base, cache_subdir)\n    if not os.path.exists(datadir):\n        os.makedirs(datadir)\n\n    if untar:\n        untar_fpath = os.path.join(datadir, fname)\n        fpath = untar_fpath + '.tar.gz'\n    else:\n        fpath = os.path.join(datadir, fname)\n\n    download = False\n    if os.path.exists(fpath):\n        # File found; verify integrity if a hash was provided.\n        if file_hash is not None:\n            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n                print('A local file was found, but it seems to be '\n                      'incomplete or outdated because the ' + hash_algorithm +\n                      ' file hash does not match the original value of ' +\n                      file_hash + ' so we will re-download the data.')\n                download = True\n    else:\n        download = True\n\n    if download:\n        print('Downloading data from', origin)\n\n        class ProgressTracker(object):\n            # Maintain progbar for the lifetime of download.\n            # This design was chosen for Python 2.7 compatibility.\n            progbar = None\n\n        def dl_progress(count, block_size, total_size):\n            if ProgressTracker.progbar is None:\n                if total_size is -1:\n                    total_size = None\n                ProgressTracker.progbar = Progbar(total_size)\n            else:\n                ProgressTracker.progbar.update(count * block_size)\n\n        error_msg = 'URL fetch failure on {}: {} -- {}'\n        try:\n            try:\n                urlretrieve(origin, fpath, dl_progress)\n            except HTTPError as e:\n                raise Exception(error_msg.format(origin, e.code, e.msg))\n            except URLError as e:\n                raise Exception(error_msg.format(origin, e.errno, e.reason))\n        except (Exception, KeyboardInterrupt):\n            if os.path.exists(fpath):\n                os.remove(fpath)\n            raise\n        ProgressTracker.progbar = None\n\n    if untar:\n        if not os.path.exists(untar_fpath):\n            _extract_archive(fpath, datadir, archive_format='tar')\n        return untar_fpath\n\n    if extract:\n        _extract_archive(fpath, datadir, archive_format)\n\n    return fpath",
        "begin_line": 123,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils._hash_file#244",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils._hash_file(fpath, algorithm='sha256', chunk_size=65535)",
        "snippet": "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n    \"\"\"Calculates a file sha256 or md5 hash.\n\n    # Example\n\n    ```python\n        >>> from keras.data_utils import _hash_file\n        >>> _hash_file('/path/to/file.zip')\n        'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n    ```\n\n    # Arguments\n        fpath: path to the file being validated\n        algorithm: hash algorithm, one of 'auto', 'sha256', or 'md5'.\n            The default 'auto' detects the hash algorithm in use.\n        chunk_size: Bytes to read at a time, important for large files.\n\n    # Returns\n        The file hash\n    \"\"\"\n    if (algorithm is 'sha256') or (algorithm is 'auto' and len(hash) is 64):\n        hasher = hashlib.sha256()\n    else:\n        hasher = hashlib.md5()\n\n    with open(fpath, 'rb') as fpath_file:\n        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n            hasher.update(chunk)\n\n    return hasher.hexdigest()",
        "begin_line": 244,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.validate_file#276",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535)",
        "snippet": "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n    \"\"\"Validates a file against a sha256 or md5 hash.\n\n    # Arguments\n        fpath: path to the file being validated\n        file_hash:  The expected hash string of the file.\n            The sha256 and md5 hash algorithms are both supported.\n        algorithm: Hash algorithm, one of 'auto', 'sha256', or 'md5'.\n            The default 'auto' detects the hash algorithm in use.\n        chunk_size: Bytes to read at a time, important for large files.\n\n    # Returns\n        Whether the file is valid\n    \"\"\"\n    if ((algorithm is 'sha256') or\n            (algorithm is 'auto' and len(file_hash) is 64)):\n        hasher = 'sha256'\n    else:\n        hasher = 'md5'\n\n    if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n        return True\n    else:\n        return False",
        "begin_line": 276,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.get_index#387",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.get_index(uid, i)",
        "snippet": "def get_index(uid, i):\n    \"\"\"Get the value from the Sequence `uid` at index `i`.\n\n    To allow multiple Sequences to be used at the same time, we use `uid` to\n    get a specific one. A single Sequence would cause the validation to\n    overwrite the training Sequence.\n\n    # Arguments\n        uid: int, Sequence identifier\n        i: index\n\n    # Returns\n        The value at index `i`.\n    \"\"\"\n    return _SHARED_SEQUENCES[uid][i]",
        "begin_line": 387,
        "end_line": 401,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.SequenceEnqueuer.__init__#425",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.SequenceEnqueuer",
        "signature": "keras.utils.data_utils.SequenceEnqueuer.__init__(self, sequence, use_multiprocessing=False)",
        "snippet": "    def __init__(self, sequence,\n                 use_multiprocessing=False):\n        self.sequence = sequence\n        self.use_multiprocessing = use_multiprocessing\n\n        global _SEQUENCE_COUNTER\n        if _SEQUENCE_COUNTER is None:\n            try:\n                _SEQUENCE_COUNTER = mp.Value('i', 0)\n            except OSError:\n                # In this case the OS does not allow us to use\n                # multiprocessing. We resort to an int\n                # for enqueuer indexing.\n                _SEQUENCE_COUNTER = 0\n\n        if isinstance(_SEQUENCE_COUNTER, int):\n            self.uid = _SEQUENCE_COUNTER\n            _SEQUENCE_COUNTER += 1\n        else:\n            # Doing Multiprocessing.Value += x is not process-safe.\n            with _SEQUENCE_COUNTER.get_lock():\n                self.uid = _SEQUENCE_COUNTER.value\n                _SEQUENCE_COUNTER.value += 1\n\n        self.workers = 0\n        self.executor_fn = None\n        self.queue = None\n        self.run_thread = None\n        self.stop_signal = None",
        "begin_line": 425,
        "end_line": 453,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.SequenceEnqueuer.is_running#455",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.SequenceEnqueuer",
        "signature": "keras.utils.data_utils.SequenceEnqueuer.is_running(self)",
        "snippet": "    def is_running(self):\n        return self.stop_signal is not None and not self.stop_signal.is_set()",
        "begin_line": 455,
        "end_line": 456,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.SequenceEnqueuer.start#458",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.SequenceEnqueuer",
        "signature": "keras.utils.data_utils.SequenceEnqueuer.start(self, workers=1, max_queue_size=10)",
        "snippet": "    def start(self, workers=1, max_queue_size=10):\n        \"\"\"Start the handler's workers.\n\n        # Arguments\n            workers: number of worker threads\n            max_queue_size: queue size\n                (when full, workers could block on `put()`)\n        \"\"\"\n        if self.use_multiprocessing:\n            self.executor_fn = self._get_executor_init(workers)\n        else:\n            # We do not need the init since it's threads.\n            self.executor_fn = lambda _: ThreadPool(workers)\n        self.workers = workers\n        self.queue = queue.Queue(max_queue_size)\n        self.stop_signal = threading.Event()\n        self.run_thread = threading.Thread(target=self._run)\n        self.run_thread.daemon = True\n        self.run_thread.start()",
        "begin_line": 458,
        "end_line": 476,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007776049766718507,
            "pseudo_dstar_susp": 0.0006680026720106881,
            "pseudo_tarantula_susp": 0.001053740779768177,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.001053740779768177
        }
    },
    {
        "name": "keras.utils.data_utils.SequenceEnqueuer._send_sequence#478",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.SequenceEnqueuer",
        "signature": "keras.utils.data_utils.SequenceEnqueuer._send_sequence(self)",
        "snippet": "    def _send_sequence(self):\n        \"\"\"Send current Iterable to all workers.\"\"\"\n        # For new processes that may spawn\n        _SHARED_SEQUENCES[self.uid] = self.sequence",
        "begin_line": 478,
        "end_line": 481,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.SequenceEnqueuer.stop#483",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.SequenceEnqueuer",
        "signature": "keras.utils.data_utils.SequenceEnqueuer.stop(self, timeout=None)",
        "snippet": "    def stop(self, timeout=None):\n        \"\"\"Stops running threads and wait for them to exit, if necessary.\n\n        Should be called by the same thread which called `start()`.\n\n        # Arguments\n            timeout: maximum time to wait on `thread.join()`\n        \"\"\"\n        self.stop_signal.set()\n        with self.queue.mutex:\n            self.queue.queue.clear()\n            self.queue.unfinished_tasks = 0\n            self.queue.not_full.notify()\n        self.run_thread.join(timeout)\n        _SHARED_SEQUENCES[self.uid] = None",
        "begin_line": 483,
        "end_line": 497,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.__init__#536",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.__init__(self, sequence, use_multiprocessing=False, shuffle=False)",
        "snippet": "    def __init__(self, sequence, use_multiprocessing=False, shuffle=False):\n        super(OrderedEnqueuer, self).__init__(sequence, use_multiprocessing)\n        self.shuffle = shuffle",
        "begin_line": 536,
        "end_line": 538,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer._get_executor_init#540",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer._get_executor_init(self, workers)",
        "snippet": "    def _get_executor_init(self, workers):\n        \"\"\"Get the Pool initializer for multiprocessing.\n\n        # Returns\n            Function, a Function to initialize the pool\n        \"\"\"\n        return lambda seqs: mp.Pool(workers,\n                                    initializer=init_pool,\n                                    initargs=(seqs,))",
        "begin_line": 540,
        "end_line": 548,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer._wait_queue#550",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer._wait_queue(self)",
        "snippet": "    def _wait_queue(self):\n        \"\"\"Wait for the queue to be empty.\"\"\"\n        while True:\n            time.sleep(0.1)\n            if self.queue.unfinished_tasks == 0 or self.stop_signal.is_set():\n                return",
        "begin_line": 550,
        "end_line": 555,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer._run#557",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer._run(self)",
        "snippet": "    def _run(self):\n        \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n        sequence = list(range(len(self.sequence)))\n        self._send_sequence()  # Share the initial sequence\n        while True:\n            if self.shuffle:\n                random.shuffle(sequence)\n\n            with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n                for i in sequence:\n                    if self.stop_signal.is_set():\n                        return\n                    self.queue.put(\n                        executor.apply_async(get_index, (self.uid, i)), block=True)\n\n                # Done with the current epoch, waiting for the final batches\n                self._wait_queue()\n\n                if self.stop_signal.is_set():\n                    # We're done\n                    return\n\n            # Call the internal on epoch end.\n            self.sequence.on_epoch_end()\n            self._send_sequence()  # Update the pool",
        "begin_line": 557,
        "end_line": 581,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.get#583",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.get(self)",
        "snippet": "    def get(self):\n        \"\"\"Creates a generator to extract data from the queue.\n\n        Skip the data if it is `None`.\n\n        # Yields\n            The next element in the queue, i.e. a tuple\n            `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n        \"\"\"\n        try:\n            while self.is_running():\n                inputs = self.queue.get(block=True).get()\n                self.queue.task_done()\n                if inputs is not None:\n                    yield inputs\n        except Exception as e:\n            self.stop()\n            six.reraise(*sys.exc_info())",
        "begin_line": 583,
        "end_line": 601,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.init_pool_generator#604",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.init_pool_generator(gens, random_seed=None)",
        "snippet": "def init_pool_generator(gens, random_seed=None):\n    global _SHARED_SEQUENCES\n    _SHARED_SEQUENCES = gens\n\n    if random_seed is not None:\n        ident = mp.current_process().ident\n        np.random.seed(random_seed + ident)",
        "begin_line": 604,
        "end_line": 610,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.next_sample#613",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.next_sample(uid)",
        "snippet": "def next_sample(uid):\n    \"\"\"Get the next value from the generator `uid`.\n\n    To allow multiple generators to be used at the same time, we use `uid` to\n    get a specific one. A single generator would cause the validation to\n    overwrite the training generator.\n\n    # Arguments\n        uid: int, generator identifier\n\n    # Returns\n        The next value of generator `uid`.\n    \"\"\"\n    return six.next(_SHARED_SEQUENCES[uid])",
        "begin_line": 613,
        "end_line": 626,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.__init__#645",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.__init__(self, sequence, use_multiprocessing=False, wait_time=None, random_seed=None)",
        "snippet": "    def __init__(self, sequence, use_multiprocessing=False, wait_time=None,\n                 random_seed=None):\n        super(GeneratorEnqueuer, self).__init__(sequence, use_multiprocessing)\n        self.random_seed = random_seed\n        if wait_time is not None:\n            warnings.warn('`wait_time` is not used anymore.',\n                          DeprecationWarning)",
        "begin_line": 645,
        "end_line": 651,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer._get_executor_init#653",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer._get_executor_init(self, workers)",
        "snippet": "    def _get_executor_init(self, workers):\n        \"\"\"Get the Pool initializer for multiprocessing.\n\n        # Returns\n            Function, a Function to initialize the pool\n        \"\"\"\n        return lambda seqs: mp.Pool(workers,\n                                    initializer=init_pool_generator,\n                                    initargs=(seqs, self.random_seed))",
        "begin_line": 653,
        "end_line": 661,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer._run#663",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer._run(self)",
        "snippet": "    def _run(self):\n        \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n        self._send_sequence()  # Share the initial generator\n        with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n            while True:\n                if self.stop_signal.is_set():\n                    return\n                self.queue.put(\n                    executor.apply_async(next_sample, (self.uid,)), block=True)",
        "begin_line": 663,
        "end_line": 671,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.get#673",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.get(self)",
        "snippet": "    def get(self):\n        \"\"\"Creates a generator to extract data from the queue.\n\n        Skip the data if it is `None`.\n\n        # Yields\n            The next element in the queue, i.e. a tuple\n            `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n        \"\"\"\n        try:\n            while self.is_running():\n                inputs = self.queue.get(block=True).get()\n                self.queue.task_done()\n                if inputs is not None:\n                    yield inputs\n        except StopIteration:\n            # Special case for finite generators\n            last_ones = []\n            while self.queue.qsize() > 0:\n                last_ones.append(self.queue.get(block=True))\n            # Wait for them to complete\n            list(map(lambda f: f.wait(), last_ones))\n            # Keep the good ones\n            last_ones = [future.get() for future in last_ones if future.successful()]\n            for inputs in last_ones:\n                if inputs is not None:\n                    yield inputs\n        except Exception as e:\n            self.stop()\n            if 'generator already executing' in str(e):\n                raise RuntimeError(\n                    \"Your generator is NOT thread-safe.\"\n                    \"Keras requires a thread-safe generator when\"\n                    \"`use_multiprocessing=False, workers > 1`.\"\n                    \"For more information see issue #1638.\")\n            six.reraise(*sys.exc_info())",
        "begin_line": 673,
        "end_line": 709,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.backend.common.epsilon#13",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.epsilon()",
        "snippet": "def epsilon():\n    \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n\n    # Returns\n        A float.\n\n    # Example\n    ```python\n        >>> keras.backend.epsilon()\n        1e-07\n    ```\n    \"\"\"\n    return _EPSILON",
        "begin_line": 13,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004723665564478035,
            "pseudo_dstar_susp": 0.0009025270758122744,
            "pseudo_tarantula_susp": 0.00046490004649000463,
            "pseudo_op2_susp": 0.0009025270758122744,
            "pseudo_barinel_susp": 0.00046490004649000463
        }
    },
    {
        "name": "keras.backend.common.floatx#48",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.floatx()",
        "snippet": "def floatx():\n    \"\"\"Returns the default float type, as a string.\n    (e.g. 'float16', 'float32', 'float64').\n\n    # Returns\n        String, the current default float type.\n\n    # Example\n    ```python\n        >>> keras.backend.floatx()\n        'float32'\n    ```\n    \"\"\"\n    return _FLOATX",
        "begin_line": 48,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002012072434607646,
            "pseudo_dstar_susp": 0.04,
            "pseudo_tarantula_susp": 0.0004655493482309125,
            "pseudo_op2_susp": 0.04,
            "pseudo_barinel_susp": 0.0004655493482309125
        }
    },
    {
        "name": "keras.backend.common.set_floatx#64",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.set_floatx(floatx)",
        "snippet": "def set_floatx(floatx):\n    \"\"\"Sets the default float type.\n\n    # Arguments\n        floatx: String, 'float16', 'float32', or 'float64'.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.floatx()\n        'float32'\n        >>> K.set_floatx('float16')\n        >>> K.floatx()\n        'float16'\n    ```\n    \"\"\"\n    global _FLOATX\n    if floatx not in {'float16', 'float32', 'float64'}:\n        raise ValueError('Unknown floatx type: ' + str(floatx))\n    _FLOATX = str(floatx)",
        "begin_line": 64,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.common.cast_to_floatx#86",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.cast_to_floatx(x)",
        "snippet": "def cast_to_floatx(x):\n    \"\"\"Cast a Numpy array to the default Keras float type.\n\n    # Arguments\n        x: Numpy array.\n\n    # Returns\n        The same Numpy array, cast to its new type.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.floatx()\n        'float32'\n        >>> arr = numpy.array([1.0, 2.0], dtype='float64')\n        >>> arr.dtype\n        dtype('float64')\n        >>> new_arr = K.cast_to_floatx(arr)\n        >>> new_arr\n        array([ 1.,  2.], dtype=float32)\n        >>> new_arr.dtype\n        dtype('float32')\n    ```\n    \"\"\"\n    return np.asarray(x, dtype=_FLOATX)",
        "begin_line": 86,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.003331232556046e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.common.image_data_format#113",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.image_data_format()",
        "snippet": "def image_data_format():\n    \"\"\"Returns the default image data format convention ('channels_first' or 'channels_last').\n\n    # Returns\n        A string, either `'channels_first'` or `'channels_last'`\n\n    # Example\n    ```python\n        >>> keras.backend.image_data_format()\n        'channels_first'\n    ```\n    \"\"\"\n    return _IMAGE_DATA_FORMAT",
        "begin_line": 113,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034698126301179735,
            "pseudo_dstar_susp": 0.0005906674542232723,
            "pseudo_tarantula_susp": 0.0003215434083601286,
            "pseudo_op2_susp": 0.0005906674542232723,
            "pseudo_barinel_susp": 0.0003215434083601286
        }
    },
    {
        "name": "keras.backend.common.set_image_data_format#128",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.set_image_data_format(data_format)",
        "snippet": "def set_image_data_format(data_format):\n    \"\"\"Sets the value of the data format convention.\n\n    # Arguments\n        data_format: string. `'channels_first'` or `'channels_last'`.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.image_data_format()\n        'channels_first'\n        >>> K.set_image_data_format('channels_last')\n        >>> K.image_data_format()\n        'channels_last'\n    ```\n    \"\"\"\n    global _IMAGE_DATA_FORMAT\n    if data_format not in {'channels_last', 'channels_first'}:\n        raise ValueError('Unknown data_format:', data_format)\n    _IMAGE_DATA_FORMAT = str(data_format)",
        "begin_line": 128,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.backend.common.normalize_data_format#150",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.normalize_data_format(value)",
        "snippet": "def normalize_data_format(value):\n    \"\"\"Checks that the value correspond to a valid data format.\n\n    # Arguments\n        value: String or None. `'channels_first'` or `'channels_last'`.\n\n    # Returns\n        A string, either `'channels_first'` or `'channels_last'`\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.normalize_data_format(None)\n        'channels_first'\n        >>> K.normalize_data_format('channels_last')\n        'channels_last'\n    ```\n\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    \"\"\"\n    if value is None:\n        value = image_data_format()\n    data_format = value.lower()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('The `data_format` argument must be one of '\n                         '\"channels_first\", \"channels_last\". Received: ' +\n                         str(value))\n    return data_format",
        "begin_line": 150,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001443001443001443,
            "pseudo_dstar_susp": 0.022222222222222223,
            "pseudo_tarantula_susp": 0.00046511627906976747,
            "pseudo_op2_susp": 0.022222222222222223,
            "pseudo_barinel_susp": 0.00046511627906976747
        }
    },
    {
        "name": "keras.layers.pooling._Pooling1D.__init__#19",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling1D",
        "signature": "keras.layers.pooling._Pooling1D.__init__(self, pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)",
        "snippet": "    def __init__(self, pool_size=2, strides=None,\n                 padding='valid', data_format='channels_last', **kwargs):\n        super(_Pooling1D, self).__init__(**kwargs)\n        if strides is None:\n            strides = pool_size\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')\n        self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=3)",
        "begin_line": 19,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling1D.compute_output_shape#30",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling1D",
        "signature": "keras.layers.pooling._Pooling1D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            steps = input_shape[2]\n            features = input_shape[1]\n        else:\n            steps = input_shape[1]\n            features = input_shape[2]\n        length = conv_utils.conv_output_length(steps,\n                                               self.pool_size[0],\n                                               self.padding,\n                                               self.strides[0])\n        if self.data_format == 'channels_first':\n            return (input_shape[0], features, length)\n        else:\n            return (input_shape[0], length, features)",
        "begin_line": 30,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling1D.call#50",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling1D",
        "signature": "keras.layers.pooling._Pooling1D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        dummy_axis = 2 if self.data_format == 'channels_last' else 3\n        inputs = K.expand_dims(inputs, dummy_axis)   # add dummy last dimension\n        output = self._pooling_function(inputs=inputs,\n                                        pool_size=self.pool_size + (1,),\n                                        strides=self.strides + (1,),\n                                        padding=self.padding,\n                                        data_format=self.data_format)\n        return K.squeeze(output, dummy_axis)  # remove dummy last dimension",
        "begin_line": 50,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling1D.get_config#60",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling1D",
        "signature": "keras.layers.pooling._Pooling1D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'strides': self.strides,\n                  'pool_size': self.pool_size,\n                  'padding': self.padding,\n                  'data_format': self.data_format}\n        base_config = super(_Pooling1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 60,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.784735812133073e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.MaxPooling1D.__init__#104",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.MaxPooling1D",
        "signature": "keras.layers.pooling.MaxPooling1D.__init__(self, pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)",
        "snippet": "    def __init__(self, pool_size=2, strides=None,\n                 padding='valid', data_format='channels_last', **kwargs):\n        super(MaxPooling1D, self).__init__(pool_size, strides,\n                                           padding, data_format,\n                                           **kwargs)",
        "begin_line": 104,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.MaxPooling1D._pooling_function#110",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.MaxPooling1D",
        "signature": "keras.layers.pooling.MaxPooling1D._pooling_function(self, inputs, pool_size, strides, padding, data_format)",
        "snippet": "    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format):\n        output = K.pool2d(inputs, pool_size, strides,\n                          padding, data_format, pool_mode='max')\n        return output",
        "begin_line": 110,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.AveragePooling1D.__init__#152",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.AveragePooling1D",
        "signature": "keras.layers.pooling.AveragePooling1D.__init__(self, pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)",
        "snippet": "    def __init__(self, pool_size=2, strides=None,\n                 padding='valid', data_format='channels_last', **kwargs):\n        super(AveragePooling1D, self).__init__(pool_size, strides,\n                                               padding, data_format,\n                                               **kwargs)",
        "begin_line": 152,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.AveragePooling1D._pooling_function#158",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.AveragePooling1D",
        "signature": "keras.layers.pooling.AveragePooling1D._pooling_function(self, inputs, pool_size, strides, padding, data_format)",
        "snippet": "    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format):\n        output = K.pool2d(inputs, pool_size, strides,\n                          padding, data_format, pool_mode='avg')\n        return output",
        "begin_line": 158,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling2D.__init__#169",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling2D",
        "signature": "keras.layers.pooling._Pooling2D.__init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)",
        "snippet": "    def __init__(self, pool_size=(2, 2), strides=None, padding='valid',\n                 data_format=None, **kwargs):\n        super(_Pooling2D, self).__init__(**kwargs)\n        if strides is None:\n            strides = pool_size\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=4)",
        "begin_line": 169,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004784688995215311,
            "pseudo_dstar_susp": 0.0003850596842510589,
            "pseudo_tarantula_susp": 0.0011695906432748538,
            "pseudo_op2_susp": 0.0003850596842510589,
            "pseudo_barinel_susp": 0.0011695906432748538
        }
    },
    {
        "name": "keras.layers.pooling._Pooling2D.compute_output_shape#180",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling2D",
        "signature": "keras.layers.pooling._Pooling2D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            rows = input_shape[2]\n            cols = input_shape[3]\n        elif self.data_format == 'channels_last':\n            rows = input_shape[1]\n            cols = input_shape[2]\n        rows = conv_utils.conv_output_length(rows, self.pool_size[0],\n                                             self.padding, self.strides[0])\n        cols = conv_utils.conv_output_length(cols, self.pool_size[1],\n                                             self.padding, self.strides[1])\n        if self.data_format == 'channels_first':\n            return (input_shape[0], input_shape[1], rows, cols)\n        elif self.data_format == 'channels_last':\n            return (input_shape[0], rows, cols, input_shape[3])",
        "begin_line": 180,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0004139072847682119,
            "pseudo_tarantula_susp": 0.0015600624024961,
            "pseudo_op2_susp": 0.0004139072847682119,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.layers.pooling._Pooling2D.call#200",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling2D",
        "signature": "keras.layers.pooling._Pooling2D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = self._pooling_function(inputs=inputs,\n                                        pool_size=self.pool_size,\n                                        strides=self.strides,\n                                        padding=self.padding,\n                                        data_format=self.data_format)\n        return output",
        "begin_line": 200,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.00040064102564102563,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00040064102564102563,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.layers.pooling._Pooling2D.get_config#208",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling2D",
        "signature": "keras.layers.pooling._Pooling2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'pool_size': self.pool_size,\n                  'padding': self.padding,\n                  'strides': self.strides,\n                  'data_format': self.data_format}\n        base_config = super(_Pooling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 208,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.MaxPooling2D.__init__#259",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.MaxPooling2D",
        "signature": "keras.layers.pooling.MaxPooling2D.__init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)",
        "snippet": "    def __init__(self, pool_size=(2, 2), strides=None, padding='valid',\n                 data_format=None, **kwargs):\n        super(MaxPooling2D, self).__init__(pool_size, strides, padding,\n                                           data_format, **kwargs)",
        "begin_line": 259,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.00041946308724832214,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.00041946308724832214,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.layers.pooling.MaxPooling2D._pooling_function#264",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.MaxPooling2D",
        "signature": "keras.layers.pooling.MaxPooling2D._pooling_function(self, inputs, pool_size, strides, padding, data_format)",
        "snippet": "    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format):\n        output = K.pool2d(inputs, pool_size, strides,\n                          padding, data_format,\n                          pool_mode='max')\n        return output",
        "begin_line": 264,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.layers.pooling.AveragePooling2D.__init__#314",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.AveragePooling2D",
        "signature": "keras.layers.pooling.AveragePooling2D.__init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)",
        "snippet": "    def __init__(self, pool_size=(2, 2), strides=None, padding='valid',\n                 data_format=None, **kwargs):\n        super(AveragePooling2D, self).__init__(pool_size, strides, padding,\n                                               data_format, **kwargs)",
        "begin_line": 314,
        "end_line": 317,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.AveragePooling2D._pooling_function#319",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.AveragePooling2D",
        "signature": "keras.layers.pooling.AveragePooling2D._pooling_function(self, inputs, pool_size, strides, padding, data_format)",
        "snippet": "    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format):\n        output = K.pool2d(inputs, pool_size, strides,\n                          padding, data_format, pool_mode='avg')\n        return output",
        "begin_line": 319,
        "end_line": 323,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling3D.__init__#330",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling3D",
        "signature": "keras.layers.pooling._Pooling3D.__init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)",
        "snippet": "    def __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid',\n                 data_format=None, **kwargs):\n        super(_Pooling3D, self).__init__(**kwargs)\n        if strides is None:\n            strides = pool_size\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')\n        self.strides = conv_utils.normalize_tuple(strides, 3, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=5)",
        "begin_line": 330,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling3D.compute_output_shape#341",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling3D",
        "signature": "keras.layers.pooling._Pooling3D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            len_dim1 = input_shape[2]\n            len_dim2 = input_shape[3]\n            len_dim3 = input_shape[4]\n        elif self.data_format == 'channels_last':\n            len_dim1 = input_shape[1]\n            len_dim2 = input_shape[2]\n            len_dim3 = input_shape[3]\n        len_dim1 = conv_utils.conv_output_length(len_dim1, self.pool_size[0],\n                                                 self.padding, self.strides[0])\n        len_dim2 = conv_utils.conv_output_length(len_dim2, self.pool_size[1],\n                                                 self.padding, self.strides[1])\n        len_dim3 = conv_utils.conv_output_length(len_dim3, self.pool_size[2],\n                                                 self.padding, self.strides[2])\n        if self.data_format == 'channels_first':\n            return (input_shape[0],\n                    input_shape[1],\n                    len_dim1, len_dim2, len_dim3)\n        elif self.data_format == 'channels_last':\n            return (input_shape[0],\n                    len_dim1, len_dim2, len_dim3,\n                    input_shape[4])",
        "begin_line": 341,
        "end_line": 363,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling3D.call#369",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling3D",
        "signature": "keras.layers.pooling._Pooling3D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = self._pooling_function(inputs=inputs,\n                                        pool_size=self.pool_size,\n                                        strides=self.strides,\n                                        padding=self.padding,\n                                        data_format=self.data_format)\n        return output",
        "begin_line": 369,
        "end_line": 375,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._Pooling3D.get_config#377",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._Pooling3D",
        "signature": "keras.layers.pooling._Pooling3D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'pool_size': self.pool_size,\n                  'padding': self.padding,\n                  'strides': self.strides,\n                  'data_format': self.data_format}\n        base_config = super(_Pooling3D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 377,
        "end_line": 383,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.MaxPooling3D.__init__#424",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.MaxPooling3D",
        "signature": "keras.layers.pooling.MaxPooling3D.__init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)",
        "snippet": "    def __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid',\n                 data_format=None, **kwargs):\n        super(MaxPooling3D, self).__init__(pool_size, strides, padding,\n                                           data_format, **kwargs)",
        "begin_line": 424,
        "end_line": 427,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.MaxPooling3D._pooling_function#429",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.MaxPooling3D",
        "signature": "keras.layers.pooling.MaxPooling3D._pooling_function(self, inputs, pool_size, strides, padding, data_format)",
        "snippet": "    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format):\n        output = K.pool3d(inputs, pool_size, strides,\n                          padding, data_format, pool_mode='max')\n        return output",
        "begin_line": 429,
        "end_line": 433,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.AveragePooling3D.__init__#474",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.AveragePooling3D",
        "signature": "keras.layers.pooling.AveragePooling3D.__init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)",
        "snippet": "    def __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid',\n                 data_format=None, **kwargs):\n        super(AveragePooling3D, self).__init__(pool_size, strides, padding,\n                                               data_format, **kwargs)",
        "begin_line": 474,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.AveragePooling3D._pooling_function#479",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.AveragePooling3D",
        "signature": "keras.layers.pooling.AveragePooling3D._pooling_function(self, inputs, pool_size, strides, padding, data_format)",
        "snippet": "    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format):\n        output = K.pool3d(inputs, pool_size, strides,\n                          padding, data_format,\n                          pool_mode='avg')\n        return output",
        "begin_line": 479,
        "end_line": 484,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling1D.__init__#491",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling1D",
        "signature": "keras.layers.pooling._GlobalPooling1D.__init__(self, data_format='channels_last', **kwargs)",
        "snippet": "    def __init__(self, data_format='channels_last', **kwargs):\n        super(_GlobalPooling1D, self).__init__(**kwargs)\n        self.input_spec = InputSpec(ndim=3)\n        self.data_format = K.normalize_data_format(data_format)",
        "begin_line": 491,
        "end_line": 494,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling1D.compute_output_shape#496",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling1D",
        "signature": "keras.layers.pooling._GlobalPooling1D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            return (input_shape[0], input_shape[1])\n        else:\n            return (input_shape[0], input_shape[2])",
        "begin_line": 496,
        "end_line": 500,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling1D.get_config#505",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling1D",
        "signature": "keras.layers.pooling._GlobalPooling1D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'data_format': self.data_format}\n        base_config = super(_GlobalPooling1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 505,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalAveragePooling1D.__init__#536",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalAveragePooling1D",
        "signature": "keras.layers.pooling.GlobalAveragePooling1D.__init__(self, data_format='channels_last', **kwargs)",
        "snippet": "    def __init__(self, data_format='channels_last', **kwargs):\n        super(GlobalAveragePooling1D, self).__init__(data_format,\n                                                     **kwargs)\n        self.supports_masking = True",
        "begin_line": 536,
        "end_line": 539,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalAveragePooling1D.call#541",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalAveragePooling1D",
        "signature": "keras.layers.pooling.GlobalAveragePooling1D.call(self, inputs, mask=None)",
        "snippet": "    def call(self, inputs, mask=None):\n        steps_axis = 1 if self.data_format == 'channels_last' else 2\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            input_shape = K.int_shape(inputs)\n            broadcast_shape = [-1, input_shape[steps_axis], 1]\n            mask = K.reshape(mask, broadcast_shape)\n            inputs *= mask\n            return K.sum(inputs, axis=steps_axis) / K.sum(mask, axis=steps_axis)\n        else:\n            return K.mean(inputs, axis=steps_axis)",
        "begin_line": 541,
        "end_line": 551,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalAveragePooling1D.compute_mask#553",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalAveragePooling1D",
        "signature": "keras.layers.pooling.GlobalAveragePooling1D.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        return None",
        "begin_line": 553,
        "end_line": 554,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalMaxPooling1D.call#582",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalMaxPooling1D",
        "signature": "keras.layers.pooling.GlobalMaxPooling1D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        steps_axis = 1 if self.data_format == 'channels_last' else 2\n        return K.max(inputs, axis=steps_axis)",
        "begin_line": 582,
        "end_line": 584,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling2D.__init__#592",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling2D",
        "signature": "keras.layers.pooling._GlobalPooling2D.__init__(self, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, data_format=None, **kwargs):\n        super(_GlobalPooling2D, self).__init__(**kwargs)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=4)",
        "begin_line": 592,
        "end_line": 595,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004928536224741252,
            "pseudo_dstar_susp": 0.000388651379712398,
            "pseudo_tarantula_susp": 0.0012121212121212121,
            "pseudo_op2_susp": 0.000388651379712398,
            "pseudo_barinel_susp": 0.0012121212121212121
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling2D.compute_output_shape#597",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling2D",
        "signature": "keras.layers.pooling._GlobalPooling2D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_last':\n            return (input_shape[0], input_shape[3])\n        else:\n            return (input_shape[0], input_shape[1])",
        "begin_line": 597,
        "end_line": 601,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling2D.get_config#606",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling2D",
        "signature": "keras.layers.pooling._GlobalPooling2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'data_format': self.data_format}\n        base_config = super(_GlobalPooling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 606,
        "end_line": 609,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalAveragePooling2D.call#640",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalAveragePooling2D",
        "signature": "keras.layers.pooling.GlobalAveragePooling2D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return K.mean(inputs, axis=[1, 2])\n        else:\n            return K.mean(inputs, axis=[2, 3])",
        "begin_line": 640,
        "end_line": 644,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0004510599909788002,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.layers.pooling.GlobalMaxPooling2D.call#675",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalMaxPooling2D",
        "signature": "keras.layers.pooling.GlobalMaxPooling2D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return K.max(inputs, axis=[1, 2])\n        else:\n            return K.max(inputs, axis=[2, 3])",
        "begin_line": 675,
        "end_line": 679,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling3D.__init__#687",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling3D",
        "signature": "keras.layers.pooling._GlobalPooling3D.__init__(self, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, data_format=None, **kwargs):\n        super(_GlobalPooling3D, self).__init__(**kwargs)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=5)",
        "begin_line": 687,
        "end_line": 690,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling3D.compute_output_shape#692",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling3D",
        "signature": "keras.layers.pooling._GlobalPooling3D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_last':\n            return (input_shape[0], input_shape[4])\n        else:\n            return (input_shape[0], input_shape[1])",
        "begin_line": 692,
        "end_line": 696,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling._GlobalPooling3D.get_config#701",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling._GlobalPooling3D",
        "signature": "keras.layers.pooling._GlobalPooling3D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'data_format': self.data_format}\n        base_config = super(_GlobalPooling3D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 701,
        "end_line": 704,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalAveragePooling3D.call#735",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalAveragePooling3D",
        "signature": "keras.layers.pooling.GlobalAveragePooling3D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return K.mean(inputs, axis=[1, 2, 3])\n        else:\n            return K.mean(inputs, axis=[2, 3, 4])",
        "begin_line": 735,
        "end_line": 739,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.pooling.GlobalMaxPooling3D.call#770",
        "src_path": "keras/layers/pooling.py",
        "class_name": "keras.layers.pooling.GlobalMaxPooling3D",
        "signature": "keras.layers.pooling.GlobalMaxPooling3D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return K.max(inputs, axis=[1, 2, 3])\n        else:\n            return K.max(inputs, axis=[2, 3, 4])",
        "begin_line": 770,
        "end_line": 774,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.__init__#47",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.__init__(self, cells, **kwargs)",
        "snippet": "    def __init__(self, cells, **kwargs):\n        for cell in cells:\n            if not hasattr(cell, 'call'):\n                raise ValueError('All cells must have a `call` method. '\n                                 'received cells:', cells)\n            if not hasattr(cell, 'state_size'):\n                raise ValueError('All cells must have a '\n                                 '`state_size` attribute. '\n                                 'received cells:', cells)\n        self.cells = cells\n        # reverse_state_order determines whether the state size will be in a\n        # reverse order of the cells' state. User might want to set this to True\n        # to keep the existing behavior. This is only useful when use\n        # `RNN(return_state=True)` since the state will be returned as the same\n        # order of state_size.\n        self.reverse_state_order = kwargs.pop('reverse_state_order', False)\n        if self.reverse_state_order:\n            warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '\n                          'will soon be deprecated. Please update the code to '\n                          'work with the natural order of states if you '\n                          'reply on the RNN states, '\n                          'eg `RNN(return_state=True)`.')\n        super(StackedRNNCells, self).__init__(**kwargs)",
        "begin_line": 47,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.state_size#72",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.state_size(self)",
        "snippet": "    def state_size(self):\n        # States are a flat list of the individual cell state size.\n        # e.g. states of a 2-layer LSTM would be `[h1, c1, h2, c2]`.\n        # (assuming one LSTM has states [h, c])\n        # In the case of reverse_state_order=True, the state_size will be\n        # `[h2, c2, h1, c1]`.\n        state_size = []\n        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n            if hasattr(cell.state_size, '__len__'):\n                state_size += list(cell.state_size)\n            else:\n                state_size.append(cell.state_size)\n        return tuple(state_size)",
        "begin_line": 72,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.output_size#87",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.output_size(self)",
        "snippet": "    def output_size(self):\n        if getattr(self.cells[-1], 'output_size', None) is not None:\n            return self.cells[-1].output_size\n        if hasattr(self.cells[-1].state_size, '__len__'):\n            return self.cells[-1].state_size[0]\n        else:\n            return self.cells[-1].state_size",
        "begin_line": 87,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.call#95",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.call(self, inputs, states, constants=None, **kwargs)",
        "snippet": "    def call(self, inputs, states, constants=None, **kwargs):\n        # Recover per-cell states.\n        nested_states = []\n        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n            if hasattr(cell.state_size, '__len__'):\n                nested_states.append(states[:len(cell.state_size)])\n                states = states[len(cell.state_size):]\n            else:\n                nested_states.append([states[0]])\n                states = states[1:]\n        if self.reverse_state_order:\n            nested_states = nested_states[::-1]\n\n        # Call the cells in order and store the returned states.\n        new_nested_states = []\n        for cell, states in zip(self.cells, nested_states):\n            if has_arg(cell.call, 'constants'):\n                inputs, states = cell.call(inputs, states,\n                                           constants=constants,\n                                           **kwargs)\n            else:\n                inputs, states = cell.call(inputs, states, **kwargs)\n            new_nested_states.append(states)\n\n        # Format the new states as a flat list\n        # in reverse cell order.\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states",
        "begin_line": 95,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.build#128",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            constants_shape = input_shape[1:]\n            input_shape = input_shape[0]\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                if has_arg(cell.call, 'constants'):\n                    cell.build([input_shape] + constants_shape)\n                else:\n                    cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                output_dim = cell.state_size[0]\n            else:\n                output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n        self.built = True",
        "begin_line": 128,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.get_config#147",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.get_config(self)",
        "snippet": "    def get_config(self):\n        cells = []\n        for cell in self.cells:\n            cells.append({'class_name': cell.__class__.__name__,\n                          'config': cell.get_config()})\n        config = {'cells': cells}\n        base_config = super(StackedRNNCells, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 147,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.from_config#157",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        from . import deserialize as deserialize_layer\n        cells = []\n        for cell_config in config.pop('cells'):\n            cells.append(deserialize_layer(cell_config,\n                                           custom_objects=custom_objects))\n        return cls(cells, **config)",
        "begin_line": 157,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.trainable_weights#166",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        if not self.trainable:\n            return []\n        weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                weights += cell.trainable_weights\n        return weights",
        "begin_line": 166,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.non_trainable_weights#176",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.non_trainable_weights(self)",
        "snippet": "    def non_trainable_weights(self):\n        weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                weights += cell.non_trainable_weights\n        if not self.trainable:\n            trainable_weights = []\n            for cell in self.cells:\n                if isinstance(cell, Layer):\n                    trainable_weights += cell.trainable_weights\n            return trainable_weights + weights\n        return weights",
        "begin_line": 176,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.losses#219",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.losses(self)",
        "snippet": "    def losses(self):\n        losses = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                cell_losses = cell.losses\n                losses += cell_losses\n        return losses",
        "begin_line": 219,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.StackedRNNCells.get_losses_for#227",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.StackedRNNCells",
        "signature": "keras.layers.recurrent.StackedRNNCells.get_losses_for(self, inputs=None)",
        "snippet": "    def get_losses_for(self, inputs=None):\n        losses = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                cell_losses = cell.get_losses_for(inputs)\n                losses += cell_losses\n        return losses",
        "begin_line": 227,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.__init__#390",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.__init__(self, cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
        "snippet": "    def __init__(self, cell,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if isinstance(cell, (list, tuple)):\n            cell = StackedRNNCells(cell)\n        if not hasattr(cell, 'call'):\n            raise ValueError('`cell` should have a `call` method. '\n                             'The RNN was passed:', cell)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('The RNN cell should have '\n                             'an attribute `state_size` '\n                             '(tuple of integers, '\n                             'one integer per RNN state).')\n        super(RNN, self).__init__(**kwargs)\n        self.cell = cell\n        self.return_sequences = return_sequences\n        self.return_state = return_state\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.state_spec = None\n        self._states = None\n        self.constants_spec = None\n        self._num_constants = None",
        "begin_line": 390,
        "end_line": 420,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.states#423",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.states(self)",
        "snippet": "    def states(self):\n        if self._states is None:\n            if isinstance(self.cell.state_size, int):\n                num_states = 1\n            else:\n                num_states = len(self.cell.state_size)\n            return [None for _ in range(num_states)]\n        return self._states",
        "begin_line": 423,
        "end_line": 430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.states#433",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.states(self, states)",
        "snippet": "    def states(self, states):\n        self._states = states",
        "begin_line": 433,
        "end_line": 434,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.compute_output_shape#436",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        if hasattr(self.cell.state_size, '__len__'):\n            state_size = self.cell.state_size\n        else:\n            state_size = [self.cell.state_size]\n\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n\n        if self.return_sequences:\n            output_shape = (input_shape[0], input_shape[1], output_dim)\n        else:\n            output_shape = (input_shape[0], output_dim)\n\n        if self.return_state:\n            state_shape = [(input_shape[0], dim) for dim in state_size]\n            return [output_shape] + state_shape\n        else:\n            return output_shape",
        "begin_line": 436,
        "end_line": 459,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.compute_mask#461",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.compute_mask(self, inputs, mask)",
        "snippet": "    def compute_mask(self, inputs, mask):\n        if isinstance(mask, list):\n            mask = mask[0]\n        output_mask = mask if self.return_sequences else None\n        if self.return_state:\n            state_mask = [None for _ in self.states]\n            return [output_mask] + state_mask\n        else:\n            return output_mask",
        "begin_line": 461,
        "end_line": 469,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.build#471",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        # Note input_shape will be list of shapes of initial states and\n        # constants if these are passed in __call__.\n        if self._num_constants is not None:\n            constants_shape = input_shape[-self._num_constants:]\n        else:\n            constants_shape = None\n\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        input_dim = input_shape[-1]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n\n        # allow cell (if layer) to build before we set or validate state_spec\n        if isinstance(self.cell, Layer):\n            step_input_shape = (input_shape[0],) + input_shape[2:]\n            if constants_shape is not None:\n                self.cell.build([step_input_shape] + constants_shape)\n            else:\n                self.cell.build(step_input_shape)\n\n        # set or validate state_spec\n        if hasattr(self.cell.state_size, '__len__'):\n            state_size = list(self.cell.state_size)\n        else:\n            state_size = [self.cell.state_size]\n\n        if self.state_spec is not None:\n            # initial_state was passed in call, check compatibility\n            if [spec.shape[-1] for spec in self.state_spec] != state_size:\n                raise ValueError(\n                    'An `initial_state` was passed that is not compatible with '\n                    '`cell.state_size`. Received `state_spec`={}; '\n                    'however `cell.state_size` is '\n                    '{}'.format(self.state_spec, self.cell.state_size))\n        else:\n            self.state_spec = [InputSpec(shape=(None, dim))\n                               for dim in state_size]\n        if self.stateful:\n            self.reset_states()\n        self.built = True",
        "begin_line": 471,
        "end_line": 513,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.get_initial_state#515",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.get_initial_state(self, inputs)",
        "snippet": "    def get_initial_state(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        if hasattr(self.cell.state_size, '__len__'):\n            return [K.tile(initial_state, [1, dim])\n                    for dim in self.cell.state_size]\n        else:\n            return [K.tile(initial_state, [1, self.cell.state_size])]",
        "begin_line": 515,
        "end_line": 524,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.180207472688883e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.__call__#526",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.__call__(self, inputs, initial_state=None, constants=None, **kwargs)",
        "snippet": "    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n        inputs, initial_state, constants = _standardize_args(\n            inputs, initial_state, constants, self._num_constants)\n\n        if initial_state is None and constants is None:\n            return super(RNN, self).__call__(inputs, **kwargs)\n\n        # If any of `initial_state` or `constants` are specified and are Keras\n        # tensors, then add them to the inputs and temporarily modify the\n        # input_spec to include them.\n\n        additional_inputs = []\n        additional_specs = []\n        if initial_state is not None:\n            kwargs['initial_state'] = initial_state\n            additional_inputs += initial_state\n            self.state_spec = [InputSpec(shape=K.int_shape(state))\n                               for state in initial_state]\n            additional_specs += self.state_spec\n        if constants is not None:\n            kwargs['constants'] = constants\n            additional_inputs += constants\n            self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                                   for constant in constants]\n            self._num_constants = len(constants)\n            additional_specs += self.constants_spec\n        # at this point additional_inputs cannot be empty\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n        for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                raise ValueError('The initial state or constants of an RNN'\n                                 ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state and constants\n            full_input = [inputs] + additional_inputs\n            full_input_spec = self.input_spec + additional_specs\n            # Perform the call with temporarily replaced input_spec\n            original_input_spec = self.input_spec\n            self.input_spec = full_input_spec\n            output = super(RNN, self).__call__(full_input, **kwargs)\n            self.input_spec = original_input_spec\n            return output\n        else:\n            return super(RNN, self).__call__(inputs, **kwargs)",
        "begin_line": 526,
        "end_line": 573,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.call#575",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.call(self, inputs, mask=None, training=None, initial_state=None, constants=None)",
        "snippet": "    def call(self,\n             inputs,\n             mask=None,\n             training=None,\n             initial_state=None,\n             constants=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            # get initial_state from full input spec\n            # as they could be copied to multiple GPU.\n            if self._num_constants is None:\n                initial_state = inputs[1:]\n            else:\n                initial_state = inputs[1:-self._num_constants]\n            if len(initial_state) == 0:\n                initial_state = None\n            inputs = inputs[0]\n        if initial_state is not None:\n            pass\n        elif self.stateful:\n            initial_state = self.states\n        else:\n            initial_state = self.get_initial_state(inputs)\n\n        if isinstance(mask, list):\n            mask = mask[0]\n\n        if len(initial_state) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_state)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        timesteps = input_shape[1]\n        if self.unroll and timesteps in [None, 1]:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined or equal to 1. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n\n        kwargs = {}\n        if has_arg(self.cell.call, 'training'):\n            kwargs['training'] = training\n\n        if constants:\n            if not has_arg(self.cell.call, 'constants'):\n                raise ValueError('RNN cell does not support constants')\n\n            def step(inputs, states):\n                constants = states[-self._num_constants:]\n                states = states[:-self._num_constants]\n                return self.cell.call(inputs, states, constants=constants,\n                                      **kwargs)\n        else:\n            def step(inputs, states):\n                return self.cell.call(inputs, states, **kwargs)\n\n        last_output, outputs, states = K.rnn(step,\n                                             inputs,\n                                             initial_state,\n                                             constants=constants,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             unroll=self.unroll,\n                                             input_length=timesteps)\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        if self.return_sequences:\n            output = outputs\n        else:\n            output = last_output\n\n        # Properly set learning phase\n        if getattr(last_output, '_uses_learning_phase', False):\n            output._uses_learning_phase = True\n            for state in states:\n                state._uses_learning_phase = True\n\n        if self.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            return [output] + states\n        else:\n            return output",
        "begin_line": 575,
        "end_line": 673,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.step#632",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.step(inputs, states)",
        "snippet": "            def step(inputs, states):\n                constants = states[-self._num_constants:]\n                states = states[:-self._num_constants]\n                return self.cell.call(inputs, states, constants=constants,\n                                      **kwargs)",
        "begin_line": 632,
        "end_line": 636,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.step#638",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.step(inputs, states)",
        "snippet": "            def step(inputs, states):\n                return self.cell.call(inputs, states, **kwargs)",
        "begin_line": 638,
        "end_line": 639,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.reset_states#675",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.reset_states(self, states=None)",
        "snippet": "    def reset_states(self, states=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        batch_size = self.input_spec[0].shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the batch size by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        # initialize state if None\n        if self.states[0] is None:\n            if hasattr(self.cell.state_size, '__len__'):\n                self.states = [K.zeros((batch_size, dim))\n                               for dim in self.cell.state_size]\n            else:\n                self.states = [K.zeros((batch_size, self.cell.state_size))]\n        elif states is None:\n            if hasattr(self.cell.state_size, '__len__'):\n                for state, dim in zip(self.states, self.cell.state_size):\n                    K.set_value(state, np.zeros((batch_size, dim)))\n            else:\n                K.set_value(self.states[0],\n                            np.zeros((batch_size, self.cell.state_size)))\n        else:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            if len(states) != len(self.states):\n                raise ValueError('Layer ' + self.name + ' expects ' +\n                                 str(len(self.states)) + ' states, '\n                                 'but it received ' + str(len(states)) +\n                                 ' state values. Input received: ' +\n                                 str(states))\n            for index, (value, state) in enumerate(zip(states, self.states)):\n                if hasattr(self.cell.state_size, '__len__'):\n                    dim = self.cell.state_size[index]\n                else:\n                    dim = self.cell.state_size\n                if value.shape != (batch_size, dim):\n                    raise ValueError('State ' + str(index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected shape=' +\n                                     str((batch_size, dim)) +\n                                     ', found shape=' + str(value.shape))\n                # TODO: consider batch calls to `set_value`.\n                K.set_value(state, value)",
        "begin_line": 675,
        "end_line": 725,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.get_config#727",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'return_state': self.return_state,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll}\n        if self._num_constants is not None:\n            config['num_constants'] = self._num_constants\n\n        cell_config = self.cell.get_config()\n        config['cell'] = {'class_name': self.cell.__class__.__name__,\n                          'config': cell_config}\n        base_config = super(RNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 727,
        "end_line": 740,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.from_config#743",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        from . import deserialize as deserialize_layer\n        cell = deserialize_layer(config.pop('cell'),\n                                 custom_objects=custom_objects)\n        num_constants = config.pop('num_constants', None)\n        layer = cls(cell, **config)\n        layer._num_constants = num_constants\n        return layer",
        "begin_line": 743,
        "end_line": 750,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.trainable_weights#753",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        if not self.trainable:\n            return []\n        if isinstance(self.cell, Layer):\n            return self.cell.trainable_weights\n        return []",
        "begin_line": 753,
        "end_line": 758,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.non_trainable_weights#761",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.non_trainable_weights(self)",
        "snippet": "    def non_trainable_weights(self):\n        if isinstance(self.cell, Layer):\n            if not self.trainable:\n                return self.cell.weights\n            return self.cell.non_trainable_weights\n        return []",
        "begin_line": 761,
        "end_line": 766,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.losses#769",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.losses(self)",
        "snippet": "    def losses(self):\n        layer_losses = super(RNN, self).losses\n        if isinstance(self.cell, Layer):\n            return self.cell.losses + layer_losses\n        return layer_losses",
        "begin_line": 769,
        "end_line": 773,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.RNN.get_losses_for#775",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.RNN",
        "signature": "keras.layers.recurrent.RNN.get_losses_for(self, inputs=None)",
        "snippet": "    def get_losses_for(self, inputs=None):\n        if isinstance(self.cell, Layer):\n            cell_losses = self.cell.get_losses_for(inputs)\n            return cell_losses + super(RNN, self).get_losses_for(inputs)\n        return super(RNN, self).get_losses_for(inputs)",
        "begin_line": 775,
        "end_line": 779,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNNCell.__init__#826",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNNCell",
        "signature": "keras.layers.recurrent.SimpleRNNCell.__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNNCell, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_size = self.units\n        self.output_size = self.units\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None",
        "begin_line": 826,
        "end_line": 863,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.208951100469657e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNNCell.build#865",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNNCell",
        "signature": "keras.layers.recurrent.SimpleRNNCell.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True",
        "begin_line": 865,
        "end_line": 885,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNNCell.call#887",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNNCell",
        "signature": "keras.layers.recurrent.SimpleRNNCell.call(self, inputs, states, training=None)",
        "snippet": "    def call(self, inputs, states, training=None):\n        prev_output = states[0]\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                K.ones_like(inputs),\n                self.dropout,\n                training=training)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_dropout_mask is None):\n            self._recurrent_dropout_mask = _generate_dropout_mask(\n                K.ones_like(prev_output),\n                self.recurrent_dropout,\n                training=training)\n\n        dp_mask = self._dropout_mask\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        if dp_mask is not None:\n            h = K.dot(inputs * dp_mask, self.kernel)\n        else:\n            h = K.dot(inputs, self.kernel)\n        if self.bias is not None:\n            h = K.bias_add(h, self.bias)\n\n        if rec_dp_mask is not None:\n            prev_output *= rec_dp_mask\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                output._uses_learning_phase = True\n        return output, [output]",
        "begin_line": 887,
        "end_line": 921,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNNCell.get_config#923",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNNCell",
        "signature": "keras.layers.recurrent.SimpleRNNCell.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNNCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 923,
        "end_line": 939,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010583130489998942,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.__init__#1006",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.__init__(self, units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if 'implementation' in kwargs:\n            kwargs.pop('implementation')\n            warnings.warn('The `implementation` argument '\n                          'in `SimpleRNN` has been deprecated. '\n                          'Please remove it from your layer call.')\n        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = SimpleRNNCell(units,\n                             activation=activation,\n                             use_bias=use_bias,\n                             kernel_initializer=kernel_initializer,\n                             recurrent_initializer=recurrent_initializer,\n                             bias_initializer=bias_initializer,\n                             kernel_regularizer=kernel_regularizer,\n                             recurrent_regularizer=recurrent_regularizer,\n                             bias_regularizer=bias_regularizer,\n                             kernel_constraint=kernel_constraint,\n                             recurrent_constraint=recurrent_constraint,\n                             bias_constraint=bias_constraint,\n                             dropout=dropout,\n                             recurrent_dropout=recurrent_dropout)\n        super(SimpleRNN, self).__init__(cell,\n                                        return_sequences=return_sequences,\n                                        return_state=return_state,\n                                        go_backwards=go_backwards,\n                                        stateful=stateful,\n                                        unroll=unroll,\n                                        **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)",
        "begin_line": 1006,
        "end_line": 1062,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.call#1064",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.call(self, inputs, mask=None, training=None, initial_state=None)",
        "snippet": "    def call(self, inputs, mask=None, training=None, initial_state=None):\n        self.cell._dropout_mask = None\n        self.cell._recurrent_dropout_mask = None\n        return super(SimpleRNN, self).call(inputs,\n                                           mask=mask,\n                                           training=training,\n                                           initial_state=initial_state)",
        "begin_line": 1064,
        "end_line": 1070,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.51746454744456e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.units#1073",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.units(self)",
        "snippet": "    def units(self):\n        return self.cell.units",
        "begin_line": 1073,
        "end_line": 1074,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.activation#1077",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.activation(self)",
        "snippet": "    def activation(self):\n        return self.cell.activation",
        "begin_line": 1077,
        "end_line": 1078,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.use_bias#1081",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.use_bias(self)",
        "snippet": "    def use_bias(self):\n        return self.cell.use_bias",
        "begin_line": 1081,
        "end_line": 1082,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.kernel_initializer#1085",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.kernel_initializer(self)",
        "snippet": "    def kernel_initializer(self):\n        return self.cell.kernel_initializer",
        "begin_line": 1085,
        "end_line": 1086,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.recurrent_initializer#1089",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.recurrent_initializer(self)",
        "snippet": "    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer",
        "begin_line": 1089,
        "end_line": 1090,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.bias_initializer#1093",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.bias_initializer(self)",
        "snippet": "    def bias_initializer(self):\n        return self.cell.bias_initializer",
        "begin_line": 1093,
        "end_line": 1094,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.kernel_regularizer#1097",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.kernel_regularizer(self)",
        "snippet": "    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer",
        "begin_line": 1097,
        "end_line": 1098,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.recurrent_regularizer#1101",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.recurrent_regularizer(self)",
        "snippet": "    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer",
        "begin_line": 1101,
        "end_line": 1102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.bias_regularizer#1105",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.bias_regularizer(self)",
        "snippet": "    def bias_regularizer(self):\n        return self.cell.bias_regularizer",
        "begin_line": 1105,
        "end_line": 1106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.kernel_constraint#1109",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.kernel_constraint(self)",
        "snippet": "    def kernel_constraint(self):\n        return self.cell.kernel_constraint",
        "begin_line": 1109,
        "end_line": 1110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.recurrent_constraint#1113",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.recurrent_constraint(self)",
        "snippet": "    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint",
        "begin_line": 1113,
        "end_line": 1114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.bias_constraint#1117",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.bias_constraint(self)",
        "snippet": "    def bias_constraint(self):\n        return self.cell.bias_constraint",
        "begin_line": 1117,
        "end_line": 1118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.dropout#1121",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.dropout(self)",
        "snippet": "    def dropout(self):\n        return self.cell.dropout",
        "begin_line": 1121,
        "end_line": 1122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.recurrent_dropout#1125",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.recurrent_dropout(self)",
        "snippet": "    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout",
        "begin_line": 1125,
        "end_line": 1126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.get_config#1128",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 1128,
        "end_line": 1146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.SimpleRNN.from_config#1149",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.SimpleRNN",
        "signature": "keras.layers.recurrent.SimpleRNN.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        if 'implementation' in config:\n            config.pop('implementation')\n        return cls(**config)",
        "begin_line": 1149,
        "end_line": 1152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRUCell.__init__#1214",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRUCell",
        "signature": "keras.layers.recurrent.GRUCell.__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 reset_after=False,\n                 **kwargs):\n        super(GRUCell, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.reset_after = reset_after\n        self.state_size = self.units\n        self.output_size = self.units\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None",
        "begin_line": 1214,
        "end_line": 1257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.30925339787749e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRUCell.build#1259",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRUCell",
        "signature": "keras.layers.recurrent.GRUCell.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if not self.reset_after:\n                bias_shape = (3 * self.units,)\n            else:\n                # separate biases for input and recurrent kernels\n                # Note: the shape is intentionally different from CuDNNGRU biases\n                # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n                # when loading and converting saved weights.\n                bias_shape = (2, 3 * self.units)\n            self.bias = self.add_weight(shape=bias_shape,\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if not self.reset_after:\n                self.input_bias, self.recurrent_bias = self.bias, None\n            else:\n                # NOTE: need to flatten, since slicing in CNTK gives 2D array\n                self.input_bias = K.flatten(self.bias[0])\n                self.recurrent_bias = K.flatten(self.bias[1])\n        else:\n            self.bias = None\n\n        # update gate\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        # reset gate\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        # new gate\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            # bias for inputs\n            self.input_bias_z = self.input_bias[:self.units]\n            self.input_bias_r = self.input_bias[self.units: self.units * 2]\n            self.input_bias_h = self.input_bias[self.units * 2:]\n            # bias for hidden state - just for compatibility with CuDNN\n            if self.reset_after:\n                self.recurrent_bias_z = self.recurrent_bias[:self.units]\n                self.recurrent_bias_r = self.recurrent_bias[self.units: self.units * 2]\n                self.recurrent_bias_h = self.recurrent_bias[self.units * 2:]\n        else:\n            self.input_bias_z = None\n            self.input_bias_r = None\n            self.input_bias_h = None\n            if self.reset_after:\n                self.recurrent_bias_z = None\n                self.recurrent_bias_r = None\n                self.recurrent_bias_h = None\n        self.built = True",
        "begin_line": 1259,
        "end_line": 1326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRUCell.call#1328",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRUCell",
        "signature": "keras.layers.recurrent.GRUCell.call(self, inputs, states, training=None)",
        "snippet": "    def call(self, inputs, states, training=None):\n        h_tm1 = states[0]  # previous memory\n\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                K.ones_like(inputs),\n                self.dropout,\n                training=training,\n                count=3)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_dropout_mask is None):\n            self._recurrent_dropout_mask = _generate_dropout_mask(\n                K.ones_like(h_tm1),\n                self.recurrent_dropout,\n                training=training,\n                count=3)\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        if self.implementation == 1:\n            if 0. < self.dropout < 1.:\n                inputs_z = inputs * dp_mask[0]\n                inputs_r = inputs * dp_mask[1]\n                inputs_h = inputs * dp_mask[2]\n            else:\n                inputs_z = inputs\n                inputs_r = inputs\n                inputs_h = inputs\n\n            x_z = K.dot(inputs_z, self.kernel_z)\n            x_r = K.dot(inputs_r, self.kernel_r)\n            x_h = K.dot(inputs_h, self.kernel_h)\n            if self.use_bias:\n                x_z = K.bias_add(x_z, self.input_bias_z)\n                x_r = K.bias_add(x_r, self.input_bias_r)\n                x_h = K.bias_add(x_h, self.input_bias_h)\n\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1_z = h_tm1 * rec_dp_mask[0]\n                h_tm1_r = h_tm1 * rec_dp_mask[1]\n                h_tm1_h = h_tm1 * rec_dp_mask[2]\n            else:\n                h_tm1_z = h_tm1\n                h_tm1_r = h_tm1\n                h_tm1_h = h_tm1\n\n            recurrent_z = K.dot(h_tm1_z, self.recurrent_kernel_z)\n            recurrent_r = K.dot(h_tm1_r, self.recurrent_kernel_r)\n            if self.reset_after and self.use_bias:\n                recurrent_z = K.bias_add(recurrent_z, self.recurrent_bias_z)\n                recurrent_r = K.bias_add(recurrent_r, self.recurrent_bias_r)\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            # reset gate applied after/before matrix multiplication\n            if self.reset_after:\n                recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n                if self.use_bias:\n                    recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n                recurrent_h = r * recurrent_h\n            else:\n                recurrent_h = K.dot(r * h_tm1_h, self.recurrent_kernel_h)\n\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if 0. < self.dropout < 1.:\n                inputs *= dp_mask[0]\n\n            # inputs projected by all gate matrices at once\n            matrix_x = K.dot(inputs, self.kernel)\n            if self.use_bias:\n                # biases: bias_z_i, bias_r_i, bias_h_i\n                matrix_x = K.bias_add(matrix_x, self.input_bias)\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            x_h = matrix_x[:, 2 * self.units:]\n\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1 *= rec_dp_mask[0]\n\n            if self.reset_after:\n                # hidden state projected by all gate matrices at once\n                matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n                if self.use_bias:\n                    matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n            else:\n                # hidden state projected separately for update/reset and new\n                matrix_inner = K.dot(h_tm1,\n                                     self.recurrent_kernel[:, :2 * self.units])\n\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            if self.reset_after:\n                recurrent_h = r * matrix_inner[:, 2 * self.units:]\n            else:\n                recurrent_h = K.dot(r * h_tm1,\n                                    self.recurrent_kernel[:, 2 * self.units:])\n\n            hh = self.activation(x_h + recurrent_h)\n\n        # previous and candidate state mixed by update gate\n        h = z * h_tm1 + (1 - z) * hh\n\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n\n        return h, [h]",
        "begin_line": 1328,
        "end_line": 1443,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRUCell.get_config#1445",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRUCell",
        "signature": "keras.layers.recurrent.GRUCell.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation,\n                  'reset_after': self.reset_after}\n        base_config = super(GRUCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 1445,
        "end_line": 1464,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.__init__#1561",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 reset_after=False,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=1`.'\n                          'Please update your layer call.')\n        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = GRUCell(units,\n                       activation=activation,\n                       recurrent_activation=recurrent_activation,\n                       use_bias=use_bias,\n                       kernel_initializer=kernel_initializer,\n                       recurrent_initializer=recurrent_initializer,\n                       bias_initializer=bias_initializer,\n                       kernel_regularizer=kernel_regularizer,\n                       recurrent_regularizer=recurrent_regularizer,\n                       bias_regularizer=bias_regularizer,\n                       kernel_constraint=kernel_constraint,\n                       recurrent_constraint=recurrent_constraint,\n                       bias_constraint=bias_constraint,\n                       dropout=dropout,\n                       recurrent_dropout=recurrent_dropout,\n                       implementation=implementation,\n                       reset_after=reset_after)\n        super(GRU, self).__init__(cell,\n                                  return_sequences=return_sequences,\n                                  return_state=return_state,\n                                  go_backwards=go_backwards,\n                                  stateful=stateful,\n                                  unroll=unroll,\n                                  **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)",
        "begin_line": 1561,
        "end_line": 1622,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.46163307786924e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.call#1624",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.call(self, inputs, mask=None, training=None, initial_state=None)",
        "snippet": "    def call(self, inputs, mask=None, training=None, initial_state=None):\n        self.cell._dropout_mask = None\n        self.cell._recurrent_dropout_mask = None\n        return super(GRU, self).call(inputs,\n                                     mask=mask,\n                                     training=training,\n                                     initial_state=initial_state)",
        "begin_line": 1624,
        "end_line": 1630,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.602458229306702e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.units#1633",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.units(self)",
        "snippet": "    def units(self):\n        return self.cell.units",
        "begin_line": 1633,
        "end_line": 1634,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.activation#1637",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.activation(self)",
        "snippet": "    def activation(self):\n        return self.cell.activation",
        "begin_line": 1637,
        "end_line": 1638,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.recurrent_activation#1641",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.recurrent_activation(self)",
        "snippet": "    def recurrent_activation(self):\n        return self.cell.recurrent_activation",
        "begin_line": 1641,
        "end_line": 1642,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.use_bias#1645",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.use_bias(self)",
        "snippet": "    def use_bias(self):\n        return self.cell.use_bias",
        "begin_line": 1645,
        "end_line": 1646,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.kernel_initializer#1649",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.kernel_initializer(self)",
        "snippet": "    def kernel_initializer(self):\n        return self.cell.kernel_initializer",
        "begin_line": 1649,
        "end_line": 1650,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.recurrent_initializer#1653",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.recurrent_initializer(self)",
        "snippet": "    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer",
        "begin_line": 1653,
        "end_line": 1654,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.bias_initializer#1657",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.bias_initializer(self)",
        "snippet": "    def bias_initializer(self):\n        return self.cell.bias_initializer",
        "begin_line": 1657,
        "end_line": 1658,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.kernel_regularizer#1661",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.kernel_regularizer(self)",
        "snippet": "    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer",
        "begin_line": 1661,
        "end_line": 1662,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.recurrent_regularizer#1665",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.recurrent_regularizer(self)",
        "snippet": "    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer",
        "begin_line": 1665,
        "end_line": 1666,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.bias_regularizer#1669",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.bias_regularizer(self)",
        "snippet": "    def bias_regularizer(self):\n        return self.cell.bias_regularizer",
        "begin_line": 1669,
        "end_line": 1670,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.kernel_constraint#1673",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.kernel_constraint(self)",
        "snippet": "    def kernel_constraint(self):\n        return self.cell.kernel_constraint",
        "begin_line": 1673,
        "end_line": 1674,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.recurrent_constraint#1677",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.recurrent_constraint(self)",
        "snippet": "    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint",
        "begin_line": 1677,
        "end_line": 1678,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.bias_constraint#1681",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.bias_constraint(self)",
        "snippet": "    def bias_constraint(self):\n        return self.cell.bias_constraint",
        "begin_line": 1681,
        "end_line": 1682,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.dropout#1685",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.dropout(self)",
        "snippet": "    def dropout(self):\n        return self.cell.dropout",
        "begin_line": 1685,
        "end_line": 1686,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.recurrent_dropout#1689",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.recurrent_dropout(self)",
        "snippet": "    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout",
        "begin_line": 1689,
        "end_line": 1690,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.implementation#1693",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.implementation(self)",
        "snippet": "    def implementation(self):\n        return self.cell.implementation",
        "begin_line": 1693,
        "end_line": 1694,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.reset_after#1697",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.reset_after(self)",
        "snippet": "    def reset_after(self):\n        return self.cell.reset_after",
        "begin_line": 1697,
        "end_line": 1698,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.get_config#1700",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation,\n                  'reset_after': self.reset_after}\n        base_config = super(GRU, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 1700,
        "end_line": 1721,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.GRU.from_config#1724",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.GRU",
        "signature": "keras.layers.recurrent.GRU.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 1\n        return cls(**config)",
        "begin_line": 1724,
        "end_line": 1727,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTMCell.__init__#1790",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTMCell",
        "signature": "keras.layers.recurrent.LSTMCell.__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 **kwargs):\n        super(LSTMCell, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.state_size = (self.units, self.units)\n        self.output_size = self.units\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None",
        "begin_line": 1790,
        "end_line": 1833,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.045680687471733e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTMCell.build#1835",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTMCell",
        "signature": "keras.layers.recurrent.LSTMCell.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(_, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True",
        "begin_line": 1835,
        "end_line": 1887,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTMCell.bias_initializer#1851",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTMCell",
        "signature": "keras.layers.recurrent.LSTMCell.bias_initializer(_, *args, **kwargs)",
        "snippet": "                def bias_initializer(_, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])",
        "begin_line": 1851,
        "end_line": 1856,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.095870474804439e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTMCell.call#1889",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTMCell",
        "signature": "keras.layers.recurrent.LSTMCell.call(self, inputs, states, training=None)",
        "snippet": "    def call(self, inputs, states, training=None):\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                K.ones_like(inputs),\n                self.dropout,\n                training=training,\n                count=4)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_dropout_mask is None):\n            self._recurrent_dropout_mask = _generate_dropout_mask(\n                K.ones_like(states[0]),\n                self.recurrent_dropout,\n                training=training,\n                count=4)\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        h_tm1 = states[0]  # previous memory state\n        c_tm1 = states[1]  # previous carry state\n\n        if self.implementation == 1:\n            if 0 < self.dropout < 1.:\n                inputs_i = inputs * dp_mask[0]\n                inputs_f = inputs * dp_mask[1]\n                inputs_c = inputs * dp_mask[2]\n                inputs_o = inputs * dp_mask[3]\n            else:\n                inputs_i = inputs\n                inputs_f = inputs\n                inputs_c = inputs\n                inputs_o = inputs\n            x_i = K.dot(inputs_i, self.kernel_i)\n            x_f = K.dot(inputs_f, self.kernel_f)\n            x_c = K.dot(inputs_c, self.kernel_c)\n            x_o = K.dot(inputs_o, self.kernel_o)\n            if self.use_bias:\n                x_i = K.bias_add(x_i, self.bias_i)\n                x_f = K.bias_add(x_f, self.bias_f)\n                x_c = K.bias_add(x_c, self.bias_c)\n                x_o = K.bias_add(x_o, self.bias_o)\n\n            if 0 < self.recurrent_dropout < 1.:\n                h_tm1_i = h_tm1 * rec_dp_mask[0]\n                h_tm1_f = h_tm1 * rec_dp_mask[1]\n                h_tm1_c = h_tm1 * rec_dp_mask[2]\n                h_tm1_o = h_tm1 * rec_dp_mask[3]\n            else:\n                h_tm1_i = h_tm1\n                h_tm1_f = h_tm1\n                h_tm1_c = h_tm1\n                h_tm1_o = h_tm1\n            i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n                                                      self.recurrent_kernel_o))\n        else:\n            if 0. < self.dropout < 1.:\n                inputs *= dp_mask[0]\n            z = K.dot(inputs, self.kernel)\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1 *= rec_dp_mask[0]\n            z += K.dot(h_tm1, self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, [h, c]",
        "begin_line": 1889,
        "end_line": 1975,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTMCell.get_config#1977",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTMCell",
        "signature": "keras.layers.recurrent.LSTMCell.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(LSTMCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 1977,
        "end_line": 1996,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.__init__#2085",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.__init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=1`.'\n                          'Please update your layer call.')\n        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = LSTMCell(units,\n                        activation=activation,\n                        recurrent_activation=recurrent_activation,\n                        use_bias=use_bias,\n                        kernel_initializer=kernel_initializer,\n                        recurrent_initializer=recurrent_initializer,\n                        unit_forget_bias=unit_forget_bias,\n                        bias_initializer=bias_initializer,\n                        kernel_regularizer=kernel_regularizer,\n                        recurrent_regularizer=recurrent_regularizer,\n                        bias_regularizer=bias_regularizer,\n                        kernel_constraint=kernel_constraint,\n                        recurrent_constraint=recurrent_constraint,\n                        bias_constraint=bias_constraint,\n                        dropout=dropout,\n                        recurrent_dropout=recurrent_dropout,\n                        implementation=implementation)\n        super(LSTM, self).__init__(cell,\n                                   return_sequences=return_sequences,\n                                   return_state=return_state,\n                                   go_backwards=go_backwards,\n                                   stateful=stateful,\n                                   unroll=unroll,\n                                   **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)",
        "begin_line": 2085,
        "end_line": 2146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.119927040583675e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.call#2148",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.call(self, inputs, mask=None, training=None, initial_state=None)",
        "snippet": "    def call(self, inputs, mask=None, training=None, initial_state=None):\n        self.cell._dropout_mask = None\n        self.cell._recurrent_dropout_mask = None\n        return super(LSTM, self).call(inputs,\n                                      mask=mask,\n                                      training=training,\n                                      initial_state=initial_state)",
        "begin_line": 2148,
        "end_line": 2154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.180207472688883e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.units#2157",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.units(self)",
        "snippet": "    def units(self):\n        return self.cell.units",
        "begin_line": 2157,
        "end_line": 2158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.activation#2161",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.activation(self)",
        "snippet": "    def activation(self):\n        return self.cell.activation",
        "begin_line": 2161,
        "end_line": 2162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.recurrent_activation#2165",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.recurrent_activation(self)",
        "snippet": "    def recurrent_activation(self):\n        return self.cell.recurrent_activation",
        "begin_line": 2165,
        "end_line": 2166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.use_bias#2169",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.use_bias(self)",
        "snippet": "    def use_bias(self):\n        return self.cell.use_bias",
        "begin_line": 2169,
        "end_line": 2170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.kernel_initializer#2173",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.kernel_initializer(self)",
        "snippet": "    def kernel_initializer(self):\n        return self.cell.kernel_initializer",
        "begin_line": 2173,
        "end_line": 2174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.recurrent_initializer#2177",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.recurrent_initializer(self)",
        "snippet": "    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer",
        "begin_line": 2177,
        "end_line": 2178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.bias_initializer#2181",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.bias_initializer(self)",
        "snippet": "    def bias_initializer(self):\n        return self.cell.bias_initializer",
        "begin_line": 2181,
        "end_line": 2182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.unit_forget_bias#2185",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.unit_forget_bias(self)",
        "snippet": "    def unit_forget_bias(self):\n        return self.cell.unit_forget_bias",
        "begin_line": 2185,
        "end_line": 2186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.kernel_regularizer#2189",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.kernel_regularizer(self)",
        "snippet": "    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer",
        "begin_line": 2189,
        "end_line": 2190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.recurrent_regularizer#2193",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.recurrent_regularizer(self)",
        "snippet": "    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer",
        "begin_line": 2193,
        "end_line": 2194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.bias_regularizer#2197",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.bias_regularizer(self)",
        "snippet": "    def bias_regularizer(self):\n        return self.cell.bias_regularizer",
        "begin_line": 2197,
        "end_line": 2198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.kernel_constraint#2201",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.kernel_constraint(self)",
        "snippet": "    def kernel_constraint(self):\n        return self.cell.kernel_constraint",
        "begin_line": 2201,
        "end_line": 2202,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.recurrent_constraint#2205",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.recurrent_constraint(self)",
        "snippet": "    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint",
        "begin_line": 2205,
        "end_line": 2206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.bias_constraint#2209",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.bias_constraint(self)",
        "snippet": "    def bias_constraint(self):\n        return self.cell.bias_constraint",
        "begin_line": 2209,
        "end_line": 2210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.dropout#2213",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.dropout(self)",
        "snippet": "    def dropout(self):\n        return self.cell.dropout",
        "begin_line": 2213,
        "end_line": 2214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.recurrent_dropout#2217",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.recurrent_dropout(self)",
        "snippet": "    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout",
        "begin_line": 2217,
        "end_line": 2218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.implementation#2221",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.implementation(self)",
        "snippet": "    def implementation(self):\n        return self.cell.implementation",
        "begin_line": 2221,
        "end_line": 2222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.get_config#2224",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(LSTM, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 2224,
        "end_line": 2245,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.LSTM.from_config#2248",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent.LSTM",
        "signature": "keras.layers.recurrent.LSTM.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 1\n        return cls(**config)",
        "begin_line": 2248,
        "end_line": 2251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000100999899000101,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent._generate_dropout_mask#2254",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent",
        "signature": "keras.layers.recurrent._generate_dropout_mask(ones, rate, training=None, count=1)",
        "snippet": "def _generate_dropout_mask(ones, rate, training=None, count=1):\n    def dropped_inputs():\n        return K.dropout(ones, rate)\n\n    if count > 1:\n        return [K.in_train_phase(\n            dropped_inputs,\n            ones,\n            training=training) for _ in range(count)]\n    return K.in_train_phase(\n        dropped_inputs,\n        ones,\n        training=training)",
        "begin_line": 2254,
        "end_line": 2266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.dropped_inputs#2255",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent",
        "signature": "keras.layers.recurrent.dropped_inputs()",
        "snippet": "    def dropped_inputs():\n        return K.dropout(ones, rate)",
        "begin_line": 2255,
        "end_line": 2256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.943323058566172e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent._standardize_args#2269",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent",
        "signature": "keras.layers.recurrent._standardize_args(inputs, initial_state, constants, num_constants)",
        "snippet": "def _standardize_args(inputs, initial_state, constants, num_constants):\n    \"\"\"Standardize `__call__` to a single list of tensor inputs.\n\n    When running a model loaded from file, the input tensors\n    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n    of `inputs` instead of by the dedicated keyword arguments. This method\n    makes sure the arguments are separated and that `initial_state` and\n    `constants` are lists of tensors (or None).\n\n    # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n\n    # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n    \"\"\"\n    if isinstance(inputs, list):\n        assert initial_state is None and constants is None\n        if num_constants is not None:\n            constants = inputs[-num_constants:]\n            inputs = inputs[:-num_constants]\n        if len(inputs) > 1:\n            initial_state = inputs[1:]\n        inputs = inputs[0]\n\n    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]\n\n    initial_state = to_list_or_none(initial_state)\n    constants = to_list_or_none(constants)\n\n    return inputs, initial_state, constants",
        "begin_line": 2269,
        "end_line": 2307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.recurrent.to_list_or_none#2297",
        "src_path": "keras/layers/recurrent.py",
        "class_name": "keras.layers.recurrent",
        "signature": "keras.layers.recurrent.to_list_or_none(x)",
        "snippet": "    def to_list_or_none(x):\n        if x is None or isinstance(x, list):\n            return x\n        if isinstance(x, tuple):\n            return list(x)\n        return [x]",
        "begin_line": 2297,
        "end_line": 2302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge._Merge.__init__#20",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge._Merge",
        "signature": "keras.layers.merge._Merge.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        super(_Merge, self).__init__(**kwargs)\n        self.supports_masking = True",
        "begin_line": 20,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009970089730807576,
            "pseudo_dstar_susp": 0.0006747638326585695,
            "pseudo_tarantula_susp": 0.0011111111111111111,
            "pseudo_op2_susp": 0.0006747638326585695,
            "pseudo_barinel_susp": 0.0011111111111111111
        }
    },
    {
        "name": "keras.layers.merge._Merge._compute_elemwise_op_output_shape#27",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge._Merge",
        "signature": "keras.layers.merge._Merge._compute_elemwise_op_output_shape(self, shape1, shape2)",
        "snippet": "    def _compute_elemwise_op_output_shape(self, shape1, shape2):\n        \"\"\"Computes the shape of the resultant of an elementwise operation.\n\n        # Arguments\n            shape1: tuple or None. Shape of the first tensor\n            shape2: tuple or None. Shape of the second tensor\n\n        # Returns\n            expected output shape when an element-wise operation is\n            carried out on 2 tensors with shapes shape1 and shape2.\n            tuple or None.\n\n        # Raises\n            ValueError: if shape1 and shape2 are not compatible for\n                element-wise operations.\n        \"\"\"\n        if None in [shape1, shape2]:\n            return None\n        elif len(shape1) < len(shape2):\n            return self._compute_elemwise_op_output_shape(shape2, shape1)\n        elif not shape2:\n            return shape1\n        output_shape = list(shape1[:-len(shape2)])\n        for i, j in zip(shape1[-len(shape2):], shape2):\n            if i is None or j is None:\n                output_shape.append(None)\n            elif i == 1:\n                output_shape.append(j)\n            elif j == 1:\n                output_shape.append(i)\n            else:\n                if i != j:\n                    raise ValueError('Operands could not be broadcast '\n                                     'together with shapes ' +\n                                     str(shape1) + ' ' + str(shape2))\n                output_shape.append(i)\n        return tuple(output_shape)",
        "begin_line": 27,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.layers.merge._Merge.build#65",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge._Merge",
        "signature": "keras.layers.merge._Merge.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        # Used purely for shape validation.\n        if not isinstance(input_shape, list):\n            raise ValueError('A merge layer should be called '\n                             'on a list of inputs.')\n        if len(input_shape) < 2:\n            raise ValueError('A merge layer should be called '\n                             'on a list of at least 2 inputs. '\n                             'Got ' + str(len(input_shape)) + ' inputs.')\n        batch_sizes = [s[0] for s in input_shape if s is not None]\n        batch_sizes = set(batch_sizes)\n        batch_sizes -= set([None])\n        if len(batch_sizes) > 1:\n            raise ValueError('Can not merge tensors with different '\n                             'batch sizes. Got tensors with shapes : ' +\n                             str(input_shape))\n        if input_shape[0] is None:\n            output_shape = None\n        else:\n            output_shape = input_shape[0][1:]\n        for i in range(1, len(input_shape)):\n            if input_shape[i] is None:\n                shape = None\n            else:\n                shape = input_shape[i][1:]\n            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n        # If the inputs have different ranks, we have to reshape them\n        # to make them broadcastable.\n        if None not in input_shape and len(set(map(len, input_shape))) == 1:\n            self._reshape_required = False\n        else:\n            self._reshape_required = True",
        "begin_line": 65,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0006640106241699867,
            "pseudo_tarantula_susp": 0.001053740779768177,
            "pseudo_op2_susp": 0.0006640106241699867,
            "pseudo_barinel_susp": 0.001053740779768177
        }
    },
    {
        "name": "keras.layers.merge._Merge.call#98",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge._Merge",
        "signature": "keras.layers.merge._Merge.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if not isinstance(inputs, list):\n            raise ValueError('A merge layer should be called '\n                             'on a list of inputs.')\n        if self._reshape_required:\n            reshaped_inputs = []\n            input_ndims = list(map(K.ndim, inputs))\n            if None not in input_ndims:\n                # If ranks of all inputs are available,\n                # we simply expand each of them at axis=1\n                # until all of them have the same rank.\n                max_ndim = max(input_ndims)\n                for x in inputs:\n                    x_ndim = K.ndim(x)\n                    for _ in range(max_ndim - x_ndim):\n                        x = K.expand_dims(x, 1)\n                    reshaped_inputs.append(x)\n                return self._merge_function(reshaped_inputs)\n            else:\n                # Transpose all inputs so that batch size is the last dimension.\n                # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)\n                transposed = False\n                for x in inputs:\n                    x_ndim = K.ndim(x)\n                    if x_ndim is None:\n                        x_shape = K.shape(x)\n                        batch_size = x_shape[0]\n                        new_shape = K.concatenate([x_shape[1:], K.expand_dims(batch_size)])\n                        x_transposed = K.reshape(x, K.stack([batch_size, K.prod(x_shape[1:])]))\n                        x_transposed = K.permute_dimensions(x_transposed, (1, 0))\n                        x_transposed = K.reshape(x_transposed, new_shape)\n                        reshaped_inputs.append(x_transposed)\n                        transposed = True\n                    elif x_ndim > 1:\n                        dims = list(range(1, x_ndim)) + [0]\n                        reshaped_inputs.append(K.permute_dimensions(x, dims))\n                        transposed = True\n                    else:\n                        # We don't transpose inputs if they are 1D vectors or scalars.\n                        reshaped_inputs.append(x)\n                y = self._merge_function(reshaped_inputs)\n                y_ndim = K.ndim(y)\n                if transposed:\n                    # If inputs have been transposed, we have to transpose the output too.\n                    if y_ndim is None:\n                        y_shape = K.shape(y)\n                        y_ndim = K.shape(y_shape)[0]\n                        batch_size = y_shape[y_ndim - 1]\n                        new_shape = K.concatenate([K.expand_dims(batch_size), y_shape[:y_ndim - 1]])\n                        y = K.reshape(y, (-1, batch_size))\n                        y = K.permute_dimensions(y, (1, 0))\n                        y = K.reshape(y, new_shape)\n                    elif y_ndim > 1:\n                        dims = [y_ndim - 1] + list(range(y_ndim - 1))\n                        y = K.permute_dimensions(y, dims)\n                return y\n        else:\n            return self._merge_function(inputs)",
        "begin_line": 98,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010362694300518134,
            "pseudo_dstar_susp": 0.0006793478260869565,
            "pseudo_tarantula_susp": 0.0011299435028248588,
            "pseudo_op2_susp": 0.0006793478260869565,
            "pseudo_barinel_susp": 0.0011299435028248588
        }
    },
    {
        "name": "keras.layers.merge._Merge.compute_output_shape#157",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge._Merge",
        "signature": "keras.layers.merge._Merge.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if input_shape[0] is None:\n            output_shape = None\n        else:\n            output_shape = input_shape[0][1:]\n        for i in range(1, len(input_shape)):\n            if input_shape[i] is None:\n                shape = None\n            else:\n                shape = input_shape[i][1:]\n            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n        batch_sizes = [s[0] for s in input_shape if s is not None]\n        batch_sizes = set(batch_sizes)\n        batch_sizes -= set([None])\n        if len(batch_sizes) == 1:\n            output_shape = (list(batch_sizes)[0],) + output_shape\n        else:\n            output_shape = (None,) + output_shape\n        return output_shape",
        "begin_line": 157,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0006640106241699867,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.0006640106241699867,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.layers.merge._Merge.compute_mask#177",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge._Merge",
        "signature": "keras.layers.merge._Merge.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        if mask is None:\n            return None\n        if not isinstance(mask, list):\n            raise ValueError('`mask` should be a list.')\n        if not isinstance(inputs, list):\n            raise ValueError('`inputs` should be a list.')\n        if len(mask) != len(inputs):\n            raise ValueError('The lists `inputs` and `mask` '\n                             'should have the same length.')\n        if all([m is None for m in mask]):\n            return None\n        masks = [K.expand_dims(m, 0) for m in mask if m is not None]\n        return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)",
        "begin_line": 177,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0006640106241699867,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.0006640106241699867,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.layers.merge.Add._merge_function#216",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Add",
        "signature": "keras.layers.merge.Add._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output += inputs[i]\n        return output",
        "begin_line": 216,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.00041946308724832214,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.00041946308724832214,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.layers.merge.Subtract.build#247",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Subtract",
        "signature": "keras.layers.merge.Subtract.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        super(Subtract, self).build(input_shape)\n        if len(input_shape) != 2:\n            raise ValueError('A `Subtract` layer should be called '\n                             'on exactly 2 inputs')",
        "begin_line": 247,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Subtract._merge_function#253",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Subtract",
        "signature": "keras.layers.merge.Subtract._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        if len(inputs) != 2:\n            raise ValueError('A `Subtract` layer should be called '\n                             'on exactly 2 inputs')\n        return inputs[0] - inputs[1]",
        "begin_line": 253,
        "end_line": 257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Multiply._merge_function#268",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Multiply",
        "signature": "keras.layers.merge.Multiply._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output *= inputs[i]\n        return output",
        "begin_line": 268,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Average._merge_function#283",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Average",
        "signature": "keras.layers.merge.Average._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output += inputs[i]\n        return output / len(inputs)",
        "begin_line": 283,
        "end_line": 287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Maximum._merge_function#298",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Maximum",
        "signature": "keras.layers.merge.Maximum._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output = K.maximum(output, inputs[i])\n        return output",
        "begin_line": 298,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Minimum._merge_function#313",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Minimum",
        "signature": "keras.layers.merge.Minimum._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output = K.minimum(output, inputs[i])\n        return output",
        "begin_line": 313,
        "end_line": 317,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Concatenate.__init__#332",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Concatenate",
        "signature": "keras.layers.merge.Concatenate.__init__(self, axis=-1, **kwargs)",
        "snippet": "    def __init__(self, axis=-1, **kwargs):\n        super(Concatenate, self).__init__(**kwargs)\n        self.axis = axis\n        self.supports_masking = True\n        self._reshape_required = False",
        "begin_line": 332,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.00040064102564102563,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00040064102564102563,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.layers.merge.Concatenate.build#338",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Concatenate",
        "signature": "keras.layers.merge.Concatenate.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        # Used purely for shape validation.\n        if not isinstance(input_shape, list) or len(input_shape) < 2:\n            raise ValueError('A `Concatenate` layer should be called '\n                             'on a list of at least 2 inputs')\n        if all([shape is None for shape in input_shape]):\n            return\n        reduced_inputs_shapes = [list(shape) for shape in input_shape]\n        shape_set = set()\n        for i in range(len(reduced_inputs_shapes)):\n            del reduced_inputs_shapes[i][self.axis]\n            shape_set.add(tuple(reduced_inputs_shapes[i]))\n        if len(shape_set) > 1:\n            raise ValueError('A `Concatenate` layer requires '\n                             'inputs with matching shapes '\n                             'except for the concat axis. '\n                             'Got inputs shapes: %s' % (input_shape))",
        "begin_line": 338,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0006844626967830253,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0006844626967830253,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.layers.merge.Concatenate._merge_function#356",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Concatenate",
        "signature": "keras.layers.merge.Concatenate._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        return K.concatenate(inputs, axis=self.axis)",
        "begin_line": 356,
        "end_line": 357,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.00040064102564102563,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00040064102564102563,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.layers.merge.Concatenate.compute_output_shape#359",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Concatenate",
        "signature": "keras.layers.merge.Concatenate.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list):\n            raise ValueError('A `Concatenate` layer should be called '\n                             'on a list of inputs.')\n        input_shapes = input_shape\n        output_shape = list(input_shapes[0])\n        for shape in input_shapes[1:]:\n            if output_shape[self.axis] is None or shape[self.axis] is None:\n                output_shape[self.axis] = None\n                break\n            output_shape[self.axis] += shape[self.axis]\n        return tuple(output_shape)",
        "begin_line": 359,
        "end_line": 370,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.00040064102564102563,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00040064102564102563,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.layers.merge.Concatenate.compute_mask#372",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Concatenate",
        "signature": "keras.layers.merge.Concatenate.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        if mask is None:\n            return None\n        if not isinstance(mask, list):\n            raise ValueError('`mask` should be a list.')\n        if not isinstance(inputs, list):\n            raise ValueError('`inputs` should be a list.')\n        if len(mask) != len(inputs):\n            raise ValueError('The lists `inputs` and `mask` '\n                             'should have the same length.')\n        if all([m is None for m in mask]):\n            return None\n        # Make a list of masks while making sure\n        # the dimensionality of each mask\n        # is the same as the corresponding input.\n        masks = []\n        for input_i, mask_i in zip(inputs, mask):\n            if mask_i is None:\n                # Input is unmasked. Append all 1s to masks,\n                masks.append(K.ones_like(input_i, dtype='bool'))\n            elif K.ndim(mask_i) < K.ndim(input_i):\n                # Mask is smaller than the input, expand it\n                masks.append(K.expand_dims(mask_i))\n            else:\n                masks.append(mask_i)\n        concatenated = K.concatenate(masks, axis=self.axis)\n        return K.all(concatenated, axis=-1, keepdims=False)",
        "begin_line": 372,
        "end_line": 398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0006844626967830253,
            "pseudo_tarantula_susp": 0.0015600624024961,
            "pseudo_op2_susp": 0.0006844626967830253,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.layers.merge.Concatenate.get_config#400",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Concatenate",
        "signature": "keras.layers.merge.Concatenate.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'axis': self.axis,\n        }\n        base_config = super(Concatenate, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 400,
        "end_line": 405,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.00047214353163361664,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.00047214353163361664,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.layers.merge.Dot.__init__#426",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Dot",
        "signature": "keras.layers.merge.Dot.__init__(self, axes, normalize=False, **kwargs)",
        "snippet": "    def __init__(self, axes, normalize=False, **kwargs):\n        super(Dot, self).__init__(**kwargs)\n        if not isinstance(axes, int):\n            if not isinstance(axes, (list, tuple)):\n                raise TypeError('Invalid type for `axes` - '\n                                'should be a list or an int.')\n            if len(axes) != 2:\n                raise ValueError('Invalid format for `axes` - '\n                                 'should contain two elements.')\n            if not isinstance(axes[0], int) or not isinstance(axes[1], int):\n                raise ValueError('Invalid format for `axes` - '\n                                 'list elements should be \"int\".')\n        self.axes = axes\n        self.normalize = normalize\n        self.supports_masking = True\n        self._reshape_required = False",
        "begin_line": 426,
        "end_line": 441,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Dot.build#443",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Dot",
        "signature": "keras.layers.merge.Dot.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        # Used purely for shape validation.\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `Dot` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = input_shape[0]\n        shape2 = input_shape[1]\n        if shape1 is None or shape2 is None:\n            return\n        if isinstance(self.axes, int):\n            if self.axes < 0:\n                axes = [self.axes % len(shape1), self.axes % len(shape2)]\n            else:\n                axes = [self.axes] * 2\n        else:\n            axes = self.axes\n        if shape1[axes[0]] != shape2[axes[1]]:\n            raise ValueError(\n                'Dimension incompatibility '\n                '%s != %s. ' % (shape1[axes[0]], shape2[axes[1]]) +\n                'Layer shapes: %s, %s' % (shape1, shape2))",
        "begin_line": 443,
        "end_line": 463,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Dot._merge_function#465",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Dot",
        "signature": "keras.layers.merge.Dot._merge_function(self, inputs)",
        "snippet": "    def _merge_function(self, inputs):\n        if len(inputs) != 2:\n            raise ValueError('A `Dot` layer should be called '\n                             'on exactly 2 inputs')\n        x1 = inputs[0]\n        x2 = inputs[1]\n        if isinstance(self.axes, int):\n            if self.axes < 0:\n                axes = [self.axes % K.ndim(x1), self.axes % K.ndim(x2)]\n            else:\n                axes = [self.axes] * 2\n        else:\n            axes = []\n            for i in range(len(self.axes)):\n                if self.axes[i] < 0:\n                    axes.append(self.axes[i] % K.ndim(inputs[i]))\n                else:\n                    axes.append(self.axes[i])\n        if self.normalize:\n            x1 = K.l2_normalize(x1, axis=axes[0])\n            x2 = K.l2_normalize(x2, axis=axes[1])\n        output = K.batch_dot(x1, x2, axes)\n        return output",
        "begin_line": 465,
        "end_line": 487,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Dot.compute_output_shape#489",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Dot",
        "signature": "keras.layers.merge.Dot.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `Dot` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n        if isinstance(self.axes, int):\n            if self.axes < 0:\n                axes = [self.axes % len(shape1), self.axes % len(shape2)]\n            else:\n                axes = [self.axes] * 2\n        else:\n            axes = self.axes\n        shape1.pop(axes[0])\n        shape2.pop(axes[1])\n        shape2.pop(0)\n        output_shape = shape1 + shape2\n        if len(output_shape) == 1:\n            output_shape += [1]\n        return tuple(output_shape)",
        "begin_line": 489,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.Dot.compute_mask#510",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge.Dot",
        "signature": "keras.layers.merge.Dot.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        return None",
        "begin_line": 510,
        "end_line": 511,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.add#522",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.add(inputs, **kwargs)",
        "snippet": "def add(inputs, **kwargs):\n    \"\"\"Functional interface to the `Add` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the sum of the inputs.\n\n    # Examples\n\n    ```python\n        import keras\n\n        input1 = keras.layers.Input(shape=(16,))\n        x1 = keras.layers.Dense(8, activation='relu')(input1)\n        input2 = keras.layers.Input(shape=(32,))\n        x2 = keras.layers.Dense(8, activation='relu')(input2)\n        added = keras.layers.add([x1, x2])\n\n        out = keras.layers.Dense(4)(added)\n        model = keras.models.Model(inputs=[input1, input2], outputs=out)\n    ```\n    \"\"\"\n    return Add(**kwargs)(inputs)",
        "begin_line": 522,
        "end_line": 547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.00041946308724832214,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.00041946308724832214,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.layers.merge.subtract#550",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.subtract(inputs, **kwargs)",
        "snippet": "def subtract(inputs, **kwargs):\n    \"\"\"Functional interface to the `Subtract` layer.\n\n    # Arguments\n        inputs: A list of input tensors (exactly 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the difference of the inputs.\n\n    # Examples\n\n    ```python\n        import keras\n\n        input1 = keras.layers.Input(shape=(16,))\n        x1 = keras.layers.Dense(8, activation='relu')(input1)\n        input2 = keras.layers.Input(shape=(32,))\n        x2 = keras.layers.Dense(8, activation='relu')(input2)\n        subtracted = keras.layers.subtract([x1, x2])\n\n        out = keras.layers.Dense(4)(subtracted)\n        model = keras.models.Model(inputs=[input1, input2], outputs=out)\n    ```\n    \"\"\"\n    return Subtract(**kwargs)(inputs)",
        "begin_line": 550,
        "end_line": 575,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.multiply#578",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.multiply(inputs, **kwargs)",
        "snippet": "def multiply(inputs, **kwargs):\n    \"\"\"Functional interface to the `Multiply` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the element-wise product of the inputs.\n    \"\"\"\n    return Multiply(**kwargs)(inputs)",
        "begin_line": 578,
        "end_line": 588,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.average#591",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.average(inputs, **kwargs)",
        "snippet": "def average(inputs, **kwargs):\n    \"\"\"Functional interface to the `Average` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the average of the inputs.\n    \"\"\"\n    return Average(**kwargs)(inputs)",
        "begin_line": 591,
        "end_line": 601,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.maximum#604",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.maximum(inputs, **kwargs)",
        "snippet": "def maximum(inputs, **kwargs):\n    \"\"\"Functional interface to the `Maximum` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the element-wise maximum of the inputs.\n    \"\"\"\n    return Maximum(**kwargs)(inputs)",
        "begin_line": 604,
        "end_line": 614,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.minimum#617",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.minimum(inputs, **kwargs)",
        "snippet": "def minimum(inputs, **kwargs):\n    \"\"\"Functional interface to the `Minimum` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the element-wise minimum of the inputs.\n    \"\"\"\n    return Minimum(**kwargs)(inputs)",
        "begin_line": 617,
        "end_line": 627,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.merge.concatenate#630",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.concatenate(inputs, axis=-1, **kwargs)",
        "snippet": "def concatenate(inputs, axis=-1, **kwargs):\n    \"\"\"Functional interface to the `Concatenate` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        axis: Concatenation axis.\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the concatenation of the inputs alongside axis `axis`.\n    \"\"\"\n    return Concatenate(axis=axis, **kwargs)(inputs)",
        "begin_line": 630,
        "end_line": 641,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.00041946308724832214,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.00041946308724832214,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.layers.merge.dot#644",
        "src_path": "keras/layers/merge.py",
        "class_name": "keras.layers.merge",
        "signature": "keras.layers.merge.dot(inputs, axes, normalize=False, **kwargs)",
        "snippet": "def dot(inputs, axes, normalize=False, **kwargs):\n    \"\"\"Functional interface to the `Dot` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        axes: Integer or tuple of integers,\n            axis or axes along which to take the dot product.\n        normalize: Whether to L2-normalize samples along the\n            dot product axis before taking the dot product.\n            If set to True, then the output of the dot product\n            is the cosine proximity between the two samples.\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the dot product of the samples from the inputs.\n    \"\"\"\n    return Dot(axes=axes, normalize=normalize, **kwargs)(inputs)",
        "begin_line": 644,
        "end_line": 660,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.__init__#88",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.__init__(self, rank, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, rank,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=1,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(_Conv, self).__init__(**kwargs)\n        self.rank = rank\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(ndim=self.rank + 2)",
        "begin_line": 88,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003721622627465575,
            "pseudo_dstar_susp": 0.0006064281382656155,
            "pseudo_tarantula_susp": 0.0004351610095735422,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0004351610095735422
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.build#124",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True",
        "begin_line": 124,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.0006640106241699867,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.0006640106241699867,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.call#153",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.rank == 1:\n            outputs = K.conv1d(\n                inputs,\n                self.kernel,\n                strides=self.strides[0],\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate[0])\n        if self.rank == 2:\n            outputs = K.conv2d(\n                inputs,\n                self.kernel,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n        if self.rank == 3:\n            outputs = K.conv3d(\n                inputs,\n                self.kernel,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs",
        "begin_line": 153,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001451378809869376,
            "pseudo_dstar_susp": 0.0006816632583503749,
            "pseudo_tarantula_susp": 0.0012360939431396785,
            "pseudo_op2_susp": 0.0006816632583503749,
            "pseudo_barinel_susp": 0.0012360939431396785
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.compute_output_shape#189",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_last':\n            space = input_shape[1:-1]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n        if self.data_format == 'channels_first':\n            space = input_shape[2:]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0], self.filters) + tuple(new_space)",
        "begin_line": 189,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045433893684688776,
            "pseudo_dstar_susp": 0.0006293266205160479,
            "pseudo_tarantula_susp": 0.0006333122229259025,
            "pseudo_op2_susp": 0.0006293266205160479,
            "pseudo_barinel_susp": 0.0006333122229259025
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.get_config#215",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'rank': self.rank,\n            'filters': self.filters,\n            'kernel_size': self.kernel_size,\n            'strides': self.strides,\n            'padding': self.padding,\n            'data_format': self.data_format,\n            'dilation_rate': self.dilation_rate,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(_Conv, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 215,
        "end_line": 235,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.946144211844694e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv1D.__init__#317",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv1D",
        "signature": "keras.layers.convolutional.Conv1D.__init__(self, filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format='channels_last',\n                 dilation_rate=1,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        if padding == 'causal':\n            if data_format != 'channels_last':\n                raise ValueError('When using causal padding in `Conv1D`, '\n                                 '`data_format` must be \"channels_last\" '\n                                 '(temporal data).')\n        super(Conv1D, self).__init__(\n            rank=1,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)",
        "begin_line": 317,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv1D.get_config#357",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv1D",
        "signature": "keras.layers.convolutional.Conv1D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(Conv1D, self).get_config()\n        config.pop('rank')\n        return config",
        "begin_line": 357,
        "end_line": 360,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010583130489998942,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2D.__init__#453",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2D",
        "signature": "keras.layers.convolutional.Conv2D.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(Conv2D, self).__init__(\n            rank=2,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)",
        "begin_line": 453,
        "end_line": 486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00048590864917395527,
            "pseudo_dstar_susp": 0.0006406149903907751,
            "pseudo_tarantula_susp": 0.0006939625260235947,
            "pseudo_op2_susp": 0.0006406149903907751,
            "pseudo_barinel_susp": 0.0006939625260235947
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2D.get_config#488",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2D",
        "signature": "keras.layers.convolutional.Conv2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(Conv2D, self).get_config()\n        config.pop('rank')\n        return config",
        "begin_line": 488,
        "end_line": 491,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.30925339787749e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3D.__init__#581",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3D",
        "signature": "keras.layers.convolutional.Conv3D.__init__(self, filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1, 1),\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(Conv3D, self).__init__(\n            rank=3,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)",
        "begin_line": 581,
        "end_line": 614,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.602458229306702e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3D.get_config#616",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3D",
        "signature": "keras.layers.convolutional.Conv3D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(Conv3D, self).get_config()\n        config.pop('rank')\n        return config",
        "begin_line": 616,
        "end_line": 619,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.654373431164318e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2DTranspose.__init__#728",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2DTranspose",
        "signature": "keras.layers.convolutional.Conv2DTranspose.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 output_padding=None,\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(Conv2DTranspose, self).__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)\n\n        self.output_padding = output_padding\n        if self.output_padding is not None:\n            self.output_padding = conv_utils.normalize_tuple(\n                self.output_padding, 2, 'output_padding')\n            for stride, out_pad in zip(self.strides, self.output_padding):\n                if out_pad >= stride:\n                    raise ValueError('Stride ' + str(self.strides) + ' must be '\n                                     'greater than output padding ' +\n                                     str(self.output_padding))",
        "begin_line": 728,
        "end_line": 771,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2DTranspose.build#773",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2DTranspose",
        "signature": "keras.layers.convolutional.Conv2DTranspose.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if len(input_shape) != 4:\n            raise ValueError('Inputs should have rank ' +\n                             str(4) +\n                             '; Received input shape:', str(input_shape))\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (self.filters, input_dim)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n        self.built = True",
        "begin_line": 773,
        "end_line": 803,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2DTranspose.call#805",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2DTranspose",
        "signature": "keras.layers.convolutional.Conv2DTranspose.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        input_shape = K.shape(inputs)\n        batch_size = input_shape[0]\n        if self.data_format == 'channels_first':\n            h_axis, w_axis = 2, 3\n        else:\n            h_axis, w_axis = 1, 2\n\n        height, width = input_shape[h_axis], input_shape[w_axis]\n        kernel_h, kernel_w = self.kernel_size\n        stride_h, stride_w = self.strides\n        if self.output_padding is None:\n            out_pad_h = out_pad_w = None\n        else:\n            out_pad_h, out_pad_w = self.output_padding\n\n        # Infer the dynamic output shape:\n        out_height = conv_utils.deconv_length(height,\n                                              stride_h, kernel_h,\n                                              self.padding,\n                                              out_pad_h,\n                                              self.dilation_rate[0])\n        out_width = conv_utils.deconv_length(width,\n                                             stride_w, kernel_w,\n                                             self.padding,\n                                             out_pad_w,\n                                             self.dilation_rate[1])\n        if self.data_format == 'channels_first':\n            output_shape = (batch_size, self.filters, out_height, out_width)\n        else:\n            output_shape = (batch_size, out_height, out_width, self.filters)\n\n        outputs = K.conv2d_transpose(\n            inputs,\n            self.kernel,\n            output_shape,\n            self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs",
        "begin_line": 805,
        "end_line": 854,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2DTranspose.compute_output_shape#856",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2DTranspose",
        "signature": "keras.layers.convolutional.Conv2DTranspose.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        output_shape = list(input_shape)\n        if self.data_format == 'channels_first':\n            c_axis, h_axis, w_axis = 1, 2, 3\n        else:\n            c_axis, h_axis, w_axis = 3, 1, 2\n\n        kernel_h, kernel_w = self.kernel_size\n        stride_h, stride_w = self.strides\n        if self.output_padding is None:\n            out_pad_h = out_pad_w = None\n        else:\n            out_pad_h, out_pad_w = self.output_padding\n\n        output_shape[c_axis] = self.filters\n        output_shape[h_axis] = conv_utils.deconv_length(output_shape[h_axis],\n                                                        stride_h,\n                                                        kernel_h,\n                                                        self.padding,\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n        output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                        stride_w,\n                                                        kernel_w,\n                                                        self.padding,\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\n        return tuple(output_shape)",
        "begin_line": 856,
        "end_line": 883,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2DTranspose.get_config#885",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2DTranspose",
        "signature": "keras.layers.convolutional.Conv2DTranspose.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(Conv2DTranspose, self).get_config()\n        config['output_padding'] = self.output_padding\n        return config",
        "begin_line": 885,
        "end_line": 888,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3DTranspose.__init__#997",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3DTranspose",
        "signature": "keras.layers.convolutional.Conv3DTranspose.__init__(self, filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1, 1),\n                 padding='valid',\n                 output_padding=None,\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(Conv3DTranspose, self).__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)\n\n        self.output_padding = output_padding\n        if self.output_padding is not None:\n            self.output_padding = conv_utils.normalize_tuple(\n                self.output_padding, 3, 'output_padding')\n            for stride, out_pad in zip(self.strides, self.output_padding):\n                if out_pad >= stride:\n                    raise ValueError('Stride ' + str(self.strides) + ' must be '\n                                     'greater than output padding ' +\n                                     str(self.output_padding))",
        "begin_line": 997,
        "end_line": 1038,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3DTranspose.build#1040",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3DTranspose",
        "signature": "keras.layers.convolutional.Conv3DTranspose.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if len(input_shape) != 5:\n            raise ValueError('Inputs should have rank ' +\n                             str(5) +\n                             '; Received input shape:', str(input_shape))\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (self.filters, input_dim)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=5, axes={channel_axis: input_dim})\n        self.built = True",
        "begin_line": 1040,
        "end_line": 1070,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3DTranspose.call#1072",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3DTranspose",
        "signature": "keras.layers.convolutional.Conv3DTranspose.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        input_shape = K.shape(inputs)\n        batch_size = input_shape[0]\n        if self.data_format == 'channels_first':\n            d_axis, h_axis, w_axis = 2, 3, 4\n        else:\n            d_axis, h_axis, w_axis = 1, 2, 3\n\n        depth = input_shape[d_axis]\n        height = input_shape[h_axis]\n        width = input_shape[w_axis]\n\n        kernel_d, kernel_h, kernel_w = self.kernel_size\n        stride_d, stride_h, stride_w = self.strides\n        if self.output_padding is None:\n            out_pad_d = out_pad_h = out_pad_w = None\n        else:\n            out_pad_d, out_pad_h, out_pad_w = self.output_padding\n\n        # Infer the dynamic output shape:\n        out_depth = conv_utils.deconv_length(depth,\n                                             stride_d, kernel_d,\n                                             self.padding,\n                                             out_pad_d)\n        out_height = conv_utils.deconv_length(height,\n                                              stride_h, kernel_h,\n                                              self.padding,\n                                              out_pad_h)\n        out_width = conv_utils.deconv_length(width,\n                                             stride_w, kernel_w,\n                                             self.padding,\n                                             out_pad_w)\n\n        if self.data_format == 'channels_first':\n            output_shape = (batch_size, self.filters, out_depth, out_height, out_width)\n        else:\n            output_shape = (batch_size, out_depth, out_height, out_width, self.filters)\n\n        outputs = K.conv3d_transpose(inputs,\n                                     self.kernel,\n                                     output_shape,\n                                     self.strides,\n                                     padding=self.padding,\n                                     data_format=self.data_format)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs",
        "begin_line": 1072,
        "end_line": 1125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3DTranspose.compute_output_shape#1127",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3DTranspose",
        "signature": "keras.layers.convolutional.Conv3DTranspose.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        output_shape = list(input_shape)\n        if self.data_format == 'channels_first':\n            c_axis, d_axis, h_axis, w_axis = 1, 2, 3, 4\n        else:\n            c_axis, d_axis, h_axis, w_axis = 4, 1, 2, 3\n\n        kernel_d, kernel_h, kernel_w = self.kernel_size\n        stride_d, stride_h, stride_w = self.strides\n        if self.output_padding is None:\n            out_pad_d = out_pad_h = out_pad_w = None\n        else:\n            out_pad_d, out_pad_h, out_pad_w = self.output_padding\n\n        output_shape[c_axis] = self.filters\n        output_shape[d_axis] = conv_utils.deconv_length(output_shape[d_axis],\n                                                        stride_d,\n                                                        kernel_d,\n                                                        self.padding,\n                                                        out_pad_d)\n        output_shape[h_axis] = conv_utils.deconv_length(output_shape[h_axis],\n                                                        stride_h,\n                                                        kernel_h,\n                                                        self.padding,\n                                                        out_pad_h)\n        output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                        stride_w,\n                                                        kernel_w,\n                                                        self.padding,\n                                                        out_pad_w)\n\n        return tuple(output_shape)",
        "begin_line": 1127,
        "end_line": 1158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Conv3DTranspose.get_config#1160",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv3DTranspose",
        "signature": "keras.layers.convolutional.Conv3DTranspose.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(Conv3DTranspose, self).get_config()\n        config.pop('dilation_rate')\n        config['output_padding'] = self.output_padding\n        return config",
        "begin_line": 1160,
        "end_line": 1164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000100999899000101,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._SeparableConv.__init__#1267",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._SeparableConv",
        "signature": "keras.layers.convolutional._SeparableConv.__init__(self, rank, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, rank,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=1,\n                 depth_multiplier=1,\n                 activation=None,\n                 use_bias=True,\n                 depthwise_initializer='glorot_uniform',\n                 pointwise_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 depthwise_regularizer=None,\n                 pointwise_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 depthwise_constraint=None,\n                 pointwise_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(_SeparableConv, self).__init__(\n            rank=rank,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            bias_initializer=bias_initializer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            bias_constraint=bias_constraint,\n            **kwargs)\n        self.depth_multiplier = depth_multiplier\n        self.depthwise_initializer = initializers.get(depthwise_initializer)\n        self.pointwise_initializer = initializers.get(pointwise_initializer)\n        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n        self.depthwise_constraint = constraints.get(depthwise_constraint)\n        self.pointwise_constraint = constraints.get(pointwise_constraint)",
        "begin_line": 1267,
        "end_line": 1309,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003629764065335753,
            "pseudo_dstar_susp": 0.0003252032520325203,
            "pseudo_tarantula_susp": 0.0005966587112171838,
            "pseudo_op2_susp": 0.0003252032520325203,
            "pseudo_barinel_susp": 0.0005966587112171838
        }
    },
    {
        "name": "keras.layers.convolutional._SeparableConv.build#1311",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._SeparableConv",
        "signature": "keras.layers.convolutional._SeparableConv.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if len(input_shape) < self.rank + 2:\n            raise ValueError('Inputs to `SeparableConv' + str(self.rank) + 'D` '\n                             'should have rank ' + str(self.rank + 2) + '. '\n                             'Received input shape:', str(input_shape))\n        channel_axis = 1 if self.data_format == 'channels_first' else -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = int(input_shape[channel_axis])\n        depthwise_kernel_shape = self.kernel_size + (input_dim, self.depth_multiplier)\n        pointwise_kernel_shape = (1,) * self.rank + (self.depth_multiplier * input_dim, self.filters)\n\n        self.depthwise_kernel = self.add_weight(\n            shape=depthwise_kernel_shape,\n            initializer=self.depthwise_initializer,\n            name='depthwise_kernel',\n            regularizer=self.depthwise_regularizer,\n            constraint=self.depthwise_constraint)\n        self.pointwise_kernel = self.add_weight(\n            shape=pointwise_kernel_shape,\n            initializer=self.pointwise_initializer,\n            name='pointwise_kernel',\n            regularizer=self.pointwise_regularizer,\n            constraint=self.pointwise_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True",
        "begin_line": 1311,
        "end_line": 1348,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.layers.convolutional._SeparableConv.call#1350",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._SeparableConv",
        "signature": "keras.layers.convolutional._SeparableConv.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.rank == 1:\n            outputs = K.separable_conv1d(\n                inputs,\n                self.depthwise_kernel,\n                self.pointwise_kernel,\n                data_format=self.data_format,\n                strides=self.strides,\n                padding=self.padding,\n                dilation_rate=self.dilation_rate)\n        if self.rank == 2:\n            outputs = K.separable_conv2d(\n                inputs,\n                self.depthwise_kernel,\n                self.pointwise_kernel,\n                data_format=self.data_format,\n                strides=self.strides,\n                padding=self.padding,\n                dilation_rate=self.dilation_rate)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs",
        "begin_line": 1350,
        "end_line": 1378,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.layers.convolutional._SeparableConv.get_config#1380",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._SeparableConv",
        "signature": "keras.layers.convolutional._SeparableConv.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(_SeparableConv, self).get_config()\n        config.pop('rank')\n        config.pop('kernel_initializer')\n        config.pop('kernel_regularizer')\n        config.pop('kernel_constraint')\n        config['depth_multiplier'] = self.depth_multiplier\n        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n        config['pointwise_initializer'] = initializers.serialize(self.pointwise_initializer)\n        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n        config['pointwise_regularizer'] = regularizers.serialize(self.pointwise_regularizer)\n        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n        config['pointwise_constraint'] = constraints.serialize(self.pointwise_constraint)\n        return config",
        "begin_line": 1380,
        "end_line": 1393,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.46163307786924e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.SeparableConv1D.__init__#1484",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.SeparableConv1D",
        "signature": "keras.layers.convolutional.SeparableConv1D.__init__(self, filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format='channels_last',\n                 dilation_rate=1,\n                 depth_multiplier=1,\n                 activation=None,\n                 use_bias=True,\n                 depthwise_initializer='glorot_uniform',\n                 pointwise_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 depthwise_regularizer=None,\n                 pointwise_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 depthwise_constraint=None,\n                 pointwise_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(SeparableConv1D, self).__init__(\n            rank=1,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            depth_multiplier=depth_multiplier,\n            activation=activation,\n            use_bias=use_bias,\n            depthwise_initializer=depthwise_initializer,\n            pointwise_initializer=pointwise_initializer,\n            bias_initializer=bias_initializer,\n            depthwise_regularizer=depthwise_regularizer,\n            pointwise_regularizer=pointwise_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            depthwise_constraint=depthwise_constraint,\n            pointwise_constraint=pointwise_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)",
        "begin_line": 1484,
        "end_line": 1525,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010481081647626035,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.SeparableConv2D.__init__#1625",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.SeparableConv2D",
        "signature": "keras.layers.convolutional.SeparableConv2D.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 depth_multiplier=1,\n                 activation=None,\n                 use_bias=True,\n                 depthwise_initializer='glorot_uniform',\n                 pointwise_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 depthwise_regularizer=None,\n                 pointwise_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 depthwise_constraint=None,\n                 pointwise_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(SeparableConv2D, self).__init__(\n            rank=2,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            depth_multiplier=depth_multiplier,\n            activation=activation,\n            use_bias=use_bias,\n            depthwise_initializer=depthwise_initializer,\n            pointwise_initializer=pointwise_initializer,\n            bias_initializer=bias_initializer,\n            depthwise_regularizer=depthwise_regularizer,\n            pointwise_regularizer=pointwise_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            depthwise_constraint=depthwise_constraint,\n            pointwise_constraint=pointwise_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)",
        "begin_line": 1625,
        "end_line": 1666,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00041841004184100416,
            "pseudo_dstar_susp": 0.0003563791874554526,
            "pseudo_tarantula_susp": 0.0008525149190110827,
            "pseudo_op2_susp": 0.0003563791874554526,
            "pseudo_barinel_susp": 0.0008532423208191126
        }
    },
    {
        "name": "keras.layers.convolutional.DepthwiseConv2D.__init__#1746",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.DepthwiseConv2D",
        "signature": "keras.layers.convolutional.DepthwiseConv2D.__init__(self, kernel_size, strides=(1, 1), padding='valid', depth_multiplier=1, data_format=None, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 depth_multiplier=1,\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 depthwise_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 depthwise_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 depthwise_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(DepthwiseConv2D, self).__init__(\n            filters=None,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            bias_constraint=bias_constraint,\n            **kwargs)\n        self.depth_multiplier = depth_multiplier\n        self.depthwise_initializer = initializers.get(depthwise_initializer)\n        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n        self.depthwise_constraint = constraints.get(depthwise_constraint)\n        self.bias_initializer = initializers.get(bias_initializer)",
        "begin_line": 1746,
        "end_line": 1778,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004928536224741252,
            "pseudo_dstar_susp": 0.000388651379712398,
            "pseudo_tarantula_susp": 0.0012121212121212121,
            "pseudo_op2_susp": 0.000388651379712398,
            "pseudo_barinel_susp": 0.0012121212121212121
        }
    },
    {
        "name": "keras.layers.convolutional.DepthwiseConv2D.build#1780",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.DepthwiseConv2D",
        "signature": "keras.layers.convolutional.DepthwiseConv2D.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if len(input_shape) < 4:\n            raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '\n                             'Received input shape:', str(input_shape))\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = 3\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs to '\n                             '`DepthwiseConv2D` '\n                             'should be defined. Found `None`.')\n        input_dim = int(input_shape[channel_axis])\n        depthwise_kernel_shape = (self.kernel_size[0],\n                                  self.kernel_size[1],\n                                  input_dim,\n                                  self.depth_multiplier)\n\n        self.depthwise_kernel = self.add_weight(\n            shape=depthwise_kernel_shape,\n            initializer=self.depthwise_initializer,\n            name='depthwise_kernel',\n            regularizer=self.depthwise_regularizer,\n            constraint=self.depthwise_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n        self.built = True",
        "begin_line": 1780,
        "end_line": 1815,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.layers.convolutional.DepthwiseConv2D.call#1817",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.DepthwiseConv2D",
        "signature": "keras.layers.convolutional.DepthwiseConv2D.call(self, inputs, training=None)",
        "snippet": "    def call(self, inputs, training=None):\n        outputs = K.depthwise_conv2d(\n            inputs,\n            self.depthwise_kernel,\n            strides=self.strides,\n            padding=self.padding,\n            dilation_rate=self.dilation_rate,\n            data_format=self.data_format)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n\n        return outputs",
        "begin_line": 1817,
        "end_line": 1835,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.00040064102564102563,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00040064102564102563,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.layers.convolutional.DepthwiseConv2D.compute_output_shape#1837",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.DepthwiseConv2D",
        "signature": "keras.layers.convolutional.DepthwiseConv2D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            rows = input_shape[2]\n            cols = input_shape[3]\n            out_filters = input_shape[1] * self.depth_multiplier\n        elif self.data_format == 'channels_last':\n            rows = input_shape[1]\n            cols = input_shape[2]\n            out_filters = input_shape[3] * self.depth_multiplier\n\n        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n                                             self.padding,\n                                             self.strides[0])\n        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n                                             self.padding,\n                                             self.strides[1])\n        if self.data_format == 'channels_first':\n            return (input_shape[0], out_filters, rows, cols)\n        elif self.data_format == 'channels_last':\n            return (input_shape[0], rows, cols, out_filters)",
        "begin_line": 1837,
        "end_line": 1856,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0004139072847682119,
            "pseudo_tarantula_susp": 0.0015600624024961,
            "pseudo_op2_susp": 0.0004139072847682119,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.layers.convolutional.DepthwiseConv2D.get_config#1858",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.DepthwiseConv2D",
        "signature": "keras.layers.convolutional.DepthwiseConv2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(DepthwiseConv2D, self).get_config()\n        config.pop('filters')\n        config.pop('kernel_initializer')\n        config.pop('kernel_regularizer')\n        config.pop('kernel_constraint')\n        config['depth_multiplier'] = self.depth_multiplier\n        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n        return config",
        "begin_line": 1858,
        "end_line": 1868,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._UpSampling.__init__#1886",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._UpSampling",
        "signature": "keras.layers.convolutional._UpSampling.__init__(self, size, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, size, data_format=None, **kwargs):\n        # self.rank is 1 for UpSampling1D, 2 for UpSampling2D.\n        self.rank = len(size)\n        self.size = size\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=self.rank + 2)\n        super(_UpSampling, self).__init__(**kwargs)",
        "begin_line": 1886,
        "end_line": 1892,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._UpSampling.compute_output_shape#1897",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._UpSampling",
        "signature": "keras.layers.convolutional._UpSampling.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        size_all_dims = (1,) + self.size + (1,)\n        spatial_axes = list(range(1, 1 + self.rank))\n        size_all_dims = transpose_shape(size_all_dims,\n                                        self.data_format,\n                                        spatial_axes)\n        output_shape = list(input_shape)\n        for dim in range(len(output_shape)):\n            if output_shape[dim] is not None:\n                output_shape[dim] *= size_all_dims[dim]\n        return tuple(output_shape)",
        "begin_line": 1897,
        "end_line": 1907,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._UpSampling.get_config#1909",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._UpSampling",
        "signature": "keras.layers.convolutional._UpSampling.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'size': self.size,\n                  'data_format': self.data_format}\n        base_config = super(_UpSampling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 1909,
        "end_line": 1913,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling1D.__init__#1932",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling1D",
        "signature": "keras.layers.convolutional.UpSampling1D.__init__(self, size=2, **kwargs)",
        "snippet": "    def __init__(self, size=2, **kwargs):\n        super(UpSampling1D, self).__init__((int(size),), 'channels_last', **kwargs)",
        "begin_line": 1932,
        "end_line": 1933,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling1D.call#1935",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling1D",
        "signature": "keras.layers.convolutional.UpSampling1D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = K.repeat_elements(inputs, self.size[0], axis=1)\n        return output",
        "begin_line": 1935,
        "end_line": 1937,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling1D.get_config#1939",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling1D",
        "signature": "keras.layers.convolutional.UpSampling1D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(UpSampling1D, self).get_config()\n        config['size'] = self.size[0]\n        config.pop('data_format')\n        return config",
        "begin_line": 1939,
        "end_line": 1943,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling2D.__init__#1985",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling2D",
        "signature": "keras.layers.convolutional.UpSampling2D.__init__(self, size=(2, 2), data_format=None, interpolation='nearest', **kwargs)",
        "snippet": "    def __init__(self, size=(2, 2), data_format=None, interpolation='nearest', **kwargs):\n        normalized_size = conv_utils.normalize_tuple(size, 2, 'size')\n        super(UpSampling2D, self).__init__(normalized_size, data_format, **kwargs)\n        if interpolation not in ['nearest', 'bilinear']:\n            raise ValueError('interpolation should be one '\n                             'of \"nearest\" or \"bilinear\".')\n        self.interpolation = interpolation",
        "begin_line": 1985,
        "end_line": 1991,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling2D.call#1993",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling2D",
        "signature": "keras.layers.convolutional.UpSampling2D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.resize_images(inputs, self.size[0], self.size[1],\n                               self.data_format, self.interpolation)",
        "begin_line": 1993,
        "end_line": 1995,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling2D.get_config#1997",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling2D",
        "signature": "keras.layers.convolutional.UpSampling2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(UpSampling2D, self).get_config()\n        config['interpolation'] = self.interpolation\n        return config",
        "begin_line": 1997,
        "end_line": 2000,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling3D.__init__#2039",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling3D",
        "signature": "keras.layers.convolutional.UpSampling3D.__init__(self, size=(2, 2, 2), data_format=None, **kwargs)",
        "snippet": "    def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n        normalized_size = conv_utils.normalize_tuple(size, 3, 'size')\n        super(UpSampling3D, self).__init__(normalized_size, data_format, **kwargs)",
        "begin_line": 2039,
        "end_line": 2041,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.UpSampling3D.call#2043",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.UpSampling3D",
        "signature": "keras.layers.convolutional.UpSampling3D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.resize_volumes(inputs,\n                                self.size[0], self.size[1], self.size[2],\n                                self.data_format)",
        "begin_line": 2043,
        "end_line": 2046,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._ZeroPadding.__init__#2065",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._ZeroPadding",
        "signature": "keras.layers.convolutional._ZeroPadding.__init__(self, padding, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, padding, data_format=None, **kwargs):\n        # self.rank is 1 for ZeroPadding1D, 2 for ZeroPadding2D.\n        self.rank = len(padding)\n        self.padding = padding\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=self.rank + 2)\n        super(_ZeroPadding, self).__init__(**kwargs)",
        "begin_line": 2065,
        "end_line": 2071,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.layers.convolutional._ZeroPadding.compute_output_shape#2076",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._ZeroPadding",
        "signature": "keras.layers.convolutional._ZeroPadding.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        padding_all_dims = ((0, 0),) + self.padding + ((0, 0),)\n        spatial_axes = list(range(1, 1 + self.rank))\n        padding_all_dims = transpose_shape(padding_all_dims,\n                                           self.data_format,\n                                           spatial_axes)\n        output_shape = list(input_shape)\n        for dim in range(len(output_shape)):\n            if output_shape[dim] is not None:\n                output_shape[dim] += sum(padding_all_dims[dim])\n        return tuple(output_shape)",
        "begin_line": 2076,
        "end_line": 2086,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045998160073597056,
            "pseudo_dstar_susp": 0.00038022813688212925,
            "pseudo_tarantula_susp": 0.0010822510822510823,
            "pseudo_op2_susp": 0.00038022813688212925,
            "pseudo_barinel_susp": 0.0010822510822510823
        }
    },
    {
        "name": "keras.layers.convolutional._ZeroPadding.get_config#2088",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._ZeroPadding",
        "signature": "keras.layers.convolutional._ZeroPadding.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'padding': self.padding,\n                  'data_format': self.data_format}\n        base_config = super(_ZeroPadding, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 2088,
        "end_line": 2092,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010583130489998942,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding1D.__init__#2114",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding1D",
        "signature": "keras.layers.convolutional.ZeroPadding1D.__init__(self, padding=1, **kwargs)",
        "snippet": "    def __init__(self, padding=1, **kwargs):\n        normalized_padding = (conv_utils.normalize_tuple(padding, 2, 'padding'),)\n        super(ZeroPadding1D, self).__init__(normalized_padding,\n                                            'channels_last',\n                                            **kwargs)",
        "begin_line": 2114,
        "end_line": 2118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding1D.call#2120",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding1D",
        "signature": "keras.layers.convolutional.ZeroPadding1D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.temporal_padding(inputs, padding=self.padding[0])",
        "begin_line": 2120,
        "end_line": 2121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding1D.get_config#2123",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding1D",
        "signature": "keras.layers.convolutional.ZeroPadding1D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = super(ZeroPadding1D, self).get_config()\n        config['padding'] = config['padding'][0]\n        config.pop('data_format')\n        return config",
        "begin_line": 2123,
        "end_line": 2127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding2D.__init__#2174",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding2D",
        "signature": "keras.layers.convolutional.ZeroPadding2D.__init__(self, padding=(1, 1), data_format=None, **kwargs)",
        "snippet": "    def __init__(self,\n                 padding=(1, 1),\n                 data_format=None,\n                 **kwargs):\n        if isinstance(padding, int):\n            normalized_padding = ((padding, padding), (padding, padding))\n        elif hasattr(padding, '__len__'):\n            if len(padding) != 2:\n                raise ValueError('`padding` should have two elements. '\n                                 'Found: ' + str(padding))\n            height_padding = conv_utils.normalize_tuple(padding[0], 2,\n                                                        '1st entry of padding')\n            width_padding = conv_utils.normalize_tuple(padding[1], 2,\n                                                       '2nd entry of padding')\n            normalized_padding = (height_padding, width_padding)\n        else:\n            raise ValueError('`padding` should be either an int, '\n                             'a tuple of 2 ints '\n                             '(symmetric_height_pad, symmetric_width_pad), '\n                             'or a tuple of 2 tuples of 2 ints '\n                             '((top_pad, bottom_pad), (left_pad, right_pad)). '\n                             'Found: ' + str(padding))\n        super(ZeroPadding2D, self).__init__(normalized_padding,\n                                            data_format,\n                                            **kwargs)",
        "begin_line": 2174,
        "end_line": 2198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0004139072847682119,
            "pseudo_tarantula_susp": 0.0015600624024961,
            "pseudo_op2_susp": 0.0004139072847682119,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding2D.call#2200",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding2D",
        "signature": "keras.layers.convolutional.ZeroPadding2D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.spatial_2d_padding(inputs,\n                                    padding=self.padding,\n                                    data_format=self.data_format)",
        "begin_line": 2200,
        "end_line": 2203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.00041946308724832214,
            "pseudo_tarantula_susp": 0.0016556291390728477,
            "pseudo_op2_susp": 0.00041946308724832214,
            "pseudo_barinel_susp": 0.0016556291390728477
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding3D.__init__#2247",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding3D",
        "signature": "keras.layers.convolutional.ZeroPadding3D.__init__(self, padding=(1, 1, 1), data_format=None, **kwargs)",
        "snippet": "    def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):\n        if isinstance(padding, int):\n            normalized_padding = ((padding, padding), (padding, padding), (padding, padding))\n        elif hasattr(padding, '__len__'):\n            if len(padding) != 3:\n                raise ValueError('`padding` should have 3 elements. '\n                                 'Found: ' + str(padding))\n            dim1_padding = conv_utils.normalize_tuple(padding[0], 2,\n                                                      '1st entry of padding')\n            dim2_padding = conv_utils.normalize_tuple(padding[1], 2,\n                                                      '2nd entry of padding')\n            dim3_padding = conv_utils.normalize_tuple(padding[2], 2,\n                                                      '3rd entry of padding')\n            normalized_padding = (dim1_padding, dim2_padding, dim3_padding)\n        else:\n            raise ValueError('`padding` should be either an int, '\n                             'a tuple of 3 ints '\n                             '(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), '\n                             'or a tuple of 3 tuples of 2 ints '\n                             '((left_dim1_pad, right_dim1_pad),'\n                             ' (left_dim2_pad, right_dim2_pad),'\n                             ' (left_dim3_pad, right_dim2_pad)). '\n                             'Found: ' + str(padding))\n        super(ZeroPadding3D, self).__init__(normalized_padding,\n                                            data_format,\n                                            **kwargs)",
        "begin_line": 2247,
        "end_line": 2272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.ZeroPadding3D.call#2274",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.ZeroPadding3D",
        "signature": "keras.layers.convolutional.ZeroPadding3D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.spatial_3d_padding(inputs,\n                                    padding=self.padding,\n                                    data_format=self.data_format)",
        "begin_line": 2274,
        "end_line": 2277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._Cropping.__init__#2297",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Cropping",
        "signature": "keras.layers.convolutional._Cropping.__init__(self, cropping, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, cropping,\n                 data_format=None,\n                 **kwargs):\n        super(_Cropping, self).__init__(**kwargs)\n        # self.rank is 1 for Cropping1D, 2 for Cropping2D...\n        self.rank = len(cropping)\n        self.cropping = cropping\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=2 + self.rank)",
        "begin_line": 2297,
        "end_line": 2305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._Cropping.call#2307",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Cropping",
        "signature": "keras.layers.convolutional._Cropping.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        slices_dims = []\n        for start, end in self.cropping:\n            if end == 0:\n                end = None\n            else:\n                end = -end\n            slices_dims.append(slice(start, end))\n\n        slices = [slice(None)] + slices_dims + [slice(None)]\n        slices = tuple(slices)\n        spatial_axes = list(range(1, 1 + self.rank))\n        slices = transpose_shape(slices, self.data_format, spatial_axes)\n        return inputs[slices]",
        "begin_line": 2307,
        "end_line": 2320,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._Cropping.compute_output_shape#2322",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Cropping",
        "signature": "keras.layers.convolutional._Cropping.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        cropping_all_dims = ((0, 0),) + self.cropping + ((0, 0),)\n        spatial_axes = list(range(1, 1 + self.rank))\n        cropping_all_dims = transpose_shape(cropping_all_dims,\n                                            self.data_format,\n                                            spatial_axes)\n        output_shape = list(input_shape)\n        for dim in range(len(output_shape)):\n            if output_shape[dim] is not None:\n                output_shape[dim] -= sum(cropping_all_dims[dim])\n        return tuple(output_shape)",
        "begin_line": 2322,
        "end_line": 2332,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional._Cropping.get_config#2334",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Cropping",
        "signature": "keras.layers.convolutional._Cropping.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'cropping': self.cropping,\n                  'data_format': self.data_format}\n        base_config = super(_Cropping, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 2334,
        "end_line": 2338,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Cropping1D.__init__#2360",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Cropping1D",
        "signature": "keras.layers.convolutional.Cropping1D.__init__(self, cropping=(1, 1), **kwargs)",
        "snippet": "    def __init__(self, cropping=(1, 1), **kwargs):\n        normalized_cropping = (conv_utils.normalize_tuple(cropping, 2, 'cropping'),)\n        super(Cropping1D, self).__init__(normalized_cropping,\n                                         'channels_last',\n                                         **kwargs)",
        "begin_line": 2360,
        "end_line": 2364,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Cropping1D.get_config#2366",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Cropping1D",
        "signature": "keras.layers.convolutional.Cropping1D.get_config(self)",
        "snippet": "    def get_config(self):\n        base_config = super(Cropping1D, self).get_config()\n        base_config.pop('data_format')\n        base_config['cropping'] = base_config['cropping'][0]\n        return base_config",
        "begin_line": 2366,
        "end_line": 2370,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Cropping2D.__init__#2429",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Cropping2D",
        "signature": "keras.layers.convolutional.Cropping2D.__init__(self, cropping=((0, 0), (0, 0)), data_format=None, **kwargs)",
        "snippet": "    def __init__(self, cropping=((0, 0), (0, 0)),\n                 data_format=None, **kwargs):\n        if isinstance(cropping, int):\n            normalized_cropping = ((cropping, cropping), (cropping, cropping))\n        elif hasattr(cropping, '__len__'):\n            if len(cropping) != 2:\n                raise ValueError('`cropping` should have two elements. '\n                                 'Found: ' + str(cropping))\n            height_cropping = conv_utils.normalize_tuple(\n                cropping[0], 2,\n                '1st entry of cropping')\n            width_cropping = conv_utils.normalize_tuple(\n                cropping[1], 2,\n                '2nd entry of cropping')\n            normalized_cropping = (height_cropping, width_cropping)\n        else:\n            raise ValueError('`cropping` should be either an int, '\n                             'a tuple of 2 ints '\n                             '(symmetric_height_crop, symmetric_width_crop), '\n                             'or a tuple of 2 tuples of 2 ints '\n                             '((top_crop, bottom_crop), (left_crop, right_crop)). '\n                             'Found: ' + str(cropping))\n        super(Cropping2D, self).__init__(normalized_cropping,\n                                         data_format,\n                                         **kwargs)",
        "begin_line": 2429,
        "end_line": 2453,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.convolutional.Cropping3D.__init__#2497",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Cropping3D",
        "signature": "keras.layers.convolutional.Cropping3D.__init__(self, cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs)",
        "snippet": "    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)),\n                 data_format=None, **kwargs):\n        self.data_format = K.normalize_data_format(data_format)\n        if isinstance(cropping, int):\n            normalized_cropping = ((cropping, cropping),\n                                   (cropping, cropping),\n                                   (cropping, cropping))\n        elif hasattr(cropping, '__len__'):\n            if len(cropping) != 3:\n                raise ValueError('`cropping` should have 3 elements. '\n                                 'Found: ' + str(cropping))\n            dim1_cropping = conv_utils.normalize_tuple(cropping[0], 2,\n                                                       '1st entry of cropping')\n            dim2_cropping = conv_utils.normalize_tuple(cropping[1], 2,\n                                                       '2nd entry of cropping')\n            dim3_cropping = conv_utils.normalize_tuple(cropping[2], 2,\n                                                       '3rd entry of cropping')\n            normalized_cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n        else:\n            raise ValueError('`cropping` should be either an int, '\n                             'a tuple of 3 ints '\n                             '(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), '\n                             'or a tuple of 3 tuples of 2 ints '\n                             '((left_dim1_crop, right_dim1_crop),'\n                             ' (left_dim2_crop, right_dim2_crop),'\n                             ' (left_dim3_crop, right_dim2_crop)). '\n                             'Found: ' + str(cropping))\n        super(Cropping3D, self).__init__(normalized_cropping,\n                                         data_format,\n                                         **kwargs)",
        "begin_line": 2497,
        "end_line": 2526,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.Initializer.get_config#21",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Initializer",
        "signature": "keras.initializers.Initializer.get_config(self)",
        "snippet": "    def get_config(self):\n        return {}",
        "begin_line": 21,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000473260766682442,
            "pseudo_dstar_susp": 0.0009041591320072332,
            "pseudo_tarantula_susp": 0.000468384074941452,
            "pseudo_op2_susp": 0.0009041591320072332,
            "pseudo_barinel_susp": 0.000468384074941452
        }
    },
    {
        "name": "keras.initializers.Initializer.from_config#25",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Initializer",
        "signature": "keras.initializers.Initializer.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        if 'dtype' in config:\n            # Initializers saved from `tf.keras`\n            # may contain an unused `dtype` argument.\n            config.pop('dtype')\n        return cls(**config)",
        "begin_line": 25,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010256410256410256,
            "pseudo_dstar_susp": 0.004329004329004329,
            "pseudo_tarantula_susp": 0.00045662100456621003,
            "pseudo_op2_susp": 0.004329004329004329,
            "pseudo_barinel_susp": 0.00045662100456621003
        }
    },
    {
        "name": "keras.initializers.Zeros.__call__#37",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Zeros",
        "signature": "keras.initializers.Zeros.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.constant(0, shape=shape, dtype=dtype)",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011737089201877935,
            "pseudo_dstar_susp": 0.005847953216374269,
            "pseudo_tarantula_susp": 0.00047961630695443646,
            "pseudo_op2_susp": 0.005847953216374269,
            "pseudo_barinel_susp": 0.00047961630695443646
        }
    },
    {
        "name": "keras.initializers.Ones.__call__#45",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Ones",
        "signature": "keras.initializers.Ones.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.constant(1, shape=shape, dtype=dtype)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00029877502240812666,
            "pseudo_dstar_susp": 0.0002935995302407516,
            "pseudo_tarantula_susp": 0.00032499187520311994,
            "pseudo_op2_susp": 0.0002935995302407516,
            "pseudo_barinel_susp": 0.00032499187520311994
        }
    },
    {
        "name": "keras.initializers.Constant.__init__#56",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Constant",
        "signature": "keras.initializers.Constant.__init__(self, value=0)",
        "snippet": "    def __init__(self, value=0):\n        self.value = value",
        "begin_line": 56,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.initializers.Constant.__call__#59",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Constant",
        "signature": "keras.initializers.Constant.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.constant(self.value, shape=shape, dtype=dtype)",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.initializers.Constant.get_config#62",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Constant",
        "signature": "keras.initializers.Constant.get_config(self)",
        "snippet": "    def get_config(self):\n        return {'value': self.value}",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004830917874396135,
            "pseudo_dstar_susp": 0.0004935834155972359,
            "pseudo_tarantula_susp": 0.007407407407407408,
            "pseudo_op2_susp": 0.0004935834155972359,
            "pseudo_barinel_susp": 0.007407407407407408
        }
    },
    {
        "name": "keras.initializers.RandomNormal.__init__#77",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.RandomNormal",
        "signature": "keras.initializers.RandomNormal.__init__(self, mean=0.0, stddev=0.05, seed=None)",
        "snippet": "    def __init__(self, mean=0., stddev=0.05, seed=None):\n        self.mean = mean\n        self.stddev = stddev\n        self.seed = seed",
        "begin_line": 77,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000100999899000101,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.RandomNormal.__call__#82",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.RandomNormal",
        "signature": "keras.initializers.RandomNormal.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)",
        "begin_line": 82,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.RandomNormal.get_config#86",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.RandomNormal",
        "signature": "keras.initializers.RandomNormal.get_config(self)",
        "snippet": "    def get_config(self):\n        return {\n            'mean': self.mean,\n            'stddev': self.stddev,\n            'seed': self.seed\n        }",
        "begin_line": 86,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010357327809425168,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.RandomUniform.__init__#105",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.RandomUniform",
        "signature": "keras.initializers.RandomUniform.__init__(self, minval=-0.05, maxval=0.05, seed=None)",
        "snippet": "    def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n        self.minval = minval\n        self.maxval = maxval\n        self.seed = seed",
        "begin_line": 105,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.51746454744456e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.RandomUniform.__call__#110",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.RandomUniform",
        "signature": "keras.initializers.RandomUniform.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.random_uniform(shape, self.minval, self.maxval,\n                                dtype=dtype, seed=self.seed)",
        "begin_line": 110,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.654373431164318e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.RandomUniform.get_config#114",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.RandomUniform",
        "signature": "keras.initializers.RandomUniform.get_config(self)",
        "snippet": "    def get_config(self):\n        return {\n            'minval': self.minval,\n            'maxval': self.maxval,\n            'seed': self.seed,\n        }",
        "begin_line": 114,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.TruncatedNormal.__init__#138",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.TruncatedNormal",
        "signature": "keras.initializers.TruncatedNormal.__init__(self, mean=0.0, stddev=0.05, seed=None)",
        "snippet": "    def __init__(self, mean=0., stddev=0.05, seed=None):\n        self.mean = mean\n        self.stddev = stddev\n        self.seed = seed",
        "begin_line": 138,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.TruncatedNormal.__call__#143",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.TruncatedNormal",
        "signature": "keras.initializers.TruncatedNormal.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)",
        "begin_line": 143,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.VarianceScaling.__init__#180",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.VarianceScaling",
        "signature": "keras.initializers.VarianceScaling.__init__(self, scale=1.0, mode='fan_in', distribution='normal', seed=None)",
        "snippet": "    def __init__(self, scale=1.0,\n                 mode='fan_in',\n                 distribution='normal',\n                 seed=None):\n        if scale <= 0.:\n            raise ValueError('`scale` must be a positive float. Got:', scale)\n        mode = mode.lower()\n        if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n            raise ValueError('Invalid `mode` argument: '\n                             'expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} '\n                             'but got', mode)\n        distribution = distribution.lower()\n        if distribution not in {'normal', 'uniform'}:\n            raise ValueError('Invalid `distribution` argument: '\n                             'expected one of {\"normal\", \"uniform\"} '\n                             'but got', distribution)\n        self.scale = scale\n        self.mode = mode\n        self.distribution = distribution\n        self.seed = seed",
        "begin_line": 180,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010660980810234541,
            "pseudo_dstar_susp": 0.004651162790697674,
            "pseudo_tarantula_susp": 0.00046685340802987864,
            "pseudo_op2_susp": 0.004651162790697674,
            "pseudo_barinel_susp": 0.00046685340802987864
        }
    },
    {
        "name": "keras.initializers.VarianceScaling.__call__#201",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.VarianceScaling",
        "signature": "keras.initializers.VarianceScaling.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        fan_in, fan_out = _compute_fans(shape)\n        scale = self.scale\n        if self.mode == 'fan_in':\n            scale /= max(1., fan_in)\n        elif self.mode == 'fan_out':\n            scale /= max(1., fan_out)\n        else:\n            scale /= max(1., float(fan_in + fan_out) / 2)\n        if self.distribution == 'normal':\n            # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n            stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n        else:\n            limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)",
        "begin_line": 201,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001375515818431912,
            "pseudo_dstar_susp": 0.006369426751592357,
            "pseudo_tarantula_susp": 0.0005254860746190226,
            "pseudo_op2_susp": 0.006369426751592357,
            "pseudo_barinel_susp": 0.0005254860746190226
        }
    },
    {
        "name": "keras.initializers.VarianceScaling.get_config#220",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.VarianceScaling",
        "signature": "keras.initializers.VarianceScaling.get_config(self)",
        "snippet": "    def get_config(self):\n        return {\n            'scale': self.scale,\n            'mode': self.mode,\n            'distribution': self.distribution,\n            'seed': self.seed\n        }",
        "begin_line": 220,
        "end_line": 226,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004784688995215311,
            "pseudo_dstar_susp": 0.0009090909090909091,
            "pseudo_tarantula_susp": 0.000481000481000481,
            "pseudo_op2_susp": 0.0009090909090909091,
            "pseudo_barinel_susp": 0.000481000481000481
        }
    },
    {
        "name": "keras.initializers.Orthogonal.__init__#240",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Orthogonal",
        "signature": "keras.initializers.Orthogonal.__init__(self, gain=1.0, seed=None)",
        "snippet": "    def __init__(self, gain=1., seed=None):\n        self.gain = gain\n        self.seed = seed",
        "begin_line": 240,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.833141948591114e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.Orthogonal.__call__#244",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Orthogonal",
        "signature": "keras.initializers.Orthogonal.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        num_rows = 1\n        for dim in shape[:-1]:\n            num_rows *= dim\n        num_cols = shape[-1]\n        flat_shape = (num_rows, num_cols)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        # Pick the one with the correct shape.\n        q = u if u.shape == flat_shape else v\n        q = q.reshape(shape)\n        return self.gain * q[:shape[0], :shape[1]]",
        "begin_line": 244,
        "end_line": 257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.892050506846879e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.Orthogonal.get_config#259",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Orthogonal",
        "signature": "keras.initializers.Orthogonal.get_config(self)",
        "snippet": "    def get_config(self):\n        return {\n            'gain': self.gain,\n            'seed': self.seed\n        }",
        "begin_line": 259,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.045680687471733e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.Identity.__init__#277",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Identity",
        "signature": "keras.initializers.Identity.__init__(self, gain=1.0)",
        "snippet": "    def __init__(self, gain=1.):\n        self.gain = gain",
        "begin_line": 277,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.Identity.__call__#280",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Identity",
        "signature": "keras.initializers.Identity.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        if len(shape) != 2:\n            raise ValueError(\n                'Identity matrix initializer can only be used for 2D matrices.')\n\n        if max(shape) % min(shape) != 0:\n            raise ValueError('Long side should be multiple of short side.')\n\n        if shape[0] == shape[1]:\n            return self.gain * np.identity(shape[0])\n        elif shape[0] > shape[1]:\n            return self.gain * np.concatenate(\n                [np.identity(shape[1])] * (shape[0] // shape[1]), axis=0)\n        else:\n            return self.gain * np.concatenate(\n                [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)",
        "begin_line": 280,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.lecun_uniform#303",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.lecun_uniform(seed=None)",
        "snippet": "def lecun_uniform(seed=None):\n    \"\"\"LeCun uniform initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(3 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        LeCun 98, Efficient Backprop,\n        http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_in',\n                           distribution='uniform',\n                           seed=seed)",
        "begin_line": 303,
        "end_line": 323,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.glorot_normal#326",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.glorot_normal(seed=None)",
        "snippet": "def glorot_normal(seed=None):\n    \"\"\"Glorot normal initializer, also called Xavier normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(2 / (fan_in + fan_out))`\n    where `fan_in` is the number of input units in the weight tensor\n    and `fan_out` is the number of output units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        Glorot & Bengio, AISTATS 2010\n        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_avg',\n                           distribution='normal',\n                           seed=seed)",
        "begin_line": 326,
        "end_line": 347,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.glorot_uniform#350",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.glorot_uniform(seed=None)",
        "snippet": "def glorot_uniform(seed=None):\n    \"\"\"Glorot uniform initializer, also called Xavier uniform initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / (fan_in + fan_out))`\n    where `fan_in` is the number of input units in the weight tensor\n    and `fan_out` is the number of output units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        Glorot & Bengio, AISTATS 2010\n        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_avg',\n                           distribution='uniform',\n                           seed=seed)",
        "begin_line": 350,
        "end_line": 371,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011210762331838565,
            "pseudo_dstar_susp": 0.005494505494505495,
            "pseudo_tarantula_susp": 0.0004764173415912339,
            "pseudo_op2_susp": 0.005494505494505495,
            "pseudo_barinel_susp": 0.0004764173415912339
        }
    },
    {
        "name": "keras.initializers.he_normal#374",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.he_normal(seed=None)",
        "snippet": "def he_normal(seed=None):\n    \"\"\"He normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(2 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        He et al., http://arxiv.org/abs/1502.01852\n    \"\"\"\n    return VarianceScaling(scale=2.,\n                           mode='fan_in',\n                           distribution='normal',\n                           seed=seed)",
        "begin_line": 374,
        "end_line": 393,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.lecun_normal#396",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.lecun_normal(seed=None)",
        "snippet": "def lecun_normal(seed=None):\n    \"\"\"LeCun normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with `stddev = sqrt(1 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n        - [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_in',\n                           distribution='normal',\n                           seed=seed)",
        "begin_line": 396,
        "end_line": 416,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers.he_uniform#419",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.he_uniform(seed=None)",
        "snippet": "def he_uniform(seed=None):\n    \"\"\"He uniform variance scaling initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / fan_in)`\n    where `fan_in` is the number of input units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        He et al., http://arxiv.org/abs/1502.01852\n    \"\"\"\n    return VarianceScaling(scale=2.,\n                           mode='fan_in',\n                           distribution='uniform',\n                           seed=seed)",
        "begin_line": 419,
        "end_line": 438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.initializers._compute_fans#455",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers._compute_fans(shape, data_format='channels_last')",
        "snippet": "def _compute_fans(shape, data_format='channels_last'):\n    \"\"\"Computes the number of input and output units for a weight shape.\n\n    # Arguments\n        shape: Integer shape tuple.\n        data_format: Image data format to use for convolution kernels.\n            Note that all kernels in Keras are standardized on the\n            `channels_last` ordering (even when inputs are set\n            to `channels_first`).\n\n    # Returns\n        A tuple of scalars, `(fan_in, fan_out)`.\n\n    # Raises\n        ValueError: in case of invalid `data_format` argument.\n    \"\"\"\n    if len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    elif len(shape) in {3, 4, 5}:\n        # Assuming convolution kernels (1D, 2D or 3D).\n        # TH kernel shape: (depth, input_depth, ...)\n        # TF kernel shape: (..., input_depth, depth)\n        if data_format == 'channels_first':\n            receptive_field_size = np.prod(shape[2:])\n            fan_in = shape[1] * receptive_field_size\n            fan_out = shape[0] * receptive_field_size\n        elif data_format == 'channels_last':\n            receptive_field_size = np.prod(shape[:-2])\n            fan_in = shape[-2] * receptive_field_size\n            fan_out = shape[-1] * receptive_field_size\n        else:\n            raise ValueError('Invalid data_format: ' + data_format)\n    else:\n        # No specific assumptions.\n        fan_in = np.sqrt(np.prod(shape))\n        fan_out = np.sqrt(np.prod(shape))\n    return fan_in, fan_out",
        "begin_line": 455,
        "end_line": 492,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0018214936247723133,
            "pseudo_dstar_susp": 0.006993006993006993,
            "pseudo_tarantula_susp": 0.0005871990604815032,
            "pseudo_op2_susp": 0.006993006993006993,
            "pseudo_barinel_susp": 0.0005871990604815032
        }
    },
    {
        "name": "keras.initializers.serialize#495",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.serialize(initializer)",
        "snippet": "def serialize(initializer):\n    return serialize_keras_object(initializer)",
        "begin_line": 495,
        "end_line": 496,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004716981132075472,
            "pseudo_dstar_susp": 0.0009000900090009,
            "pseudo_tarantula_susp": 0.000462962962962963,
            "pseudo_op2_susp": 0.0009000900090009,
            "pseudo_barinel_susp": 0.000462962962962963
        }
    },
    {
        "name": "keras.initializers.deserialize#499",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    return deserialize_keras_object(config,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='initializer')",
        "begin_line": 499,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010256410256410256,
            "pseudo_dstar_susp": 0.004329004329004329,
            "pseudo_tarantula_susp": 0.00045662100456621003,
            "pseudo_op2_susp": 0.004329004329004329,
            "pseudo_barinel_susp": 0.00045662100456621003
        }
    },
    {
        "name": "keras.initializers.get#506",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.get(identifier)",
        "snippet": "def get(identifier):\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret initializer identifier: ' +\n                         str(identifier))",
        "begin_line": 506,
        "end_line": 516,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011160714285714285,
            "pseudo_dstar_susp": 0.005376344086021506,
            "pseudo_tarantula_susp": 0.00047460844803037496,
            "pseudo_op2_susp": 0.005376344086021506,
            "pseudo_barinel_susp": 0.00047460844803037496
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_single_array#16",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_single_array(x)",
        "snippet": "def standardize_single_array(x):\n    if x is None:\n        return None\n    elif K.is_tensor(x):\n        shape = K.int_shape(x)\n        if shape is None or shape[0] is None:\n            raise ValueError(\n                'When feeding symbolic tensors to a model, we expect the'\n                'tensors to have a static batch size. '\n                'Got tensor with shape: %s' % str(shape))\n        return x\n    elif x.ndim == 1:\n        x = np.expand_dims(x, 1)\n    return x",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002959455460195324,
            "pseudo_dstar_susp": 0.0005382131324004305,
            "pseudo_tarantula_susp": 0.0002777777777777778,
            "pseudo_op2_susp": 0.0005382131324004305,
            "pseudo_barinel_susp": 0.0002777777777777778
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_input_data#32",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_input_data(data, names, shapes=None, check_batch_axis=True, exception_prefix='')",
        "snippet": "def standardize_input_data(data,\n                           names,\n                           shapes=None,\n                           check_batch_axis=True,\n                           exception_prefix=''):\n    \"\"\"Normalizes inputs and targets provided by users.\n\n    Users may pass data as a list of arrays, dictionary of arrays,\n    or as a single array. We normalize this to an ordered list of\n    arrays (same order as `names`), while checking that the provided\n    arrays have shapes that match the network's expectations.\n\n    # Arguments\n        data: User-provided input data (polymorphic).\n        names: List of expected array names.\n        shapes: Optional list of expected array shapes.\n        check_batch_axis: Boolean; whether to check that\n            the batch axis of the arrays matches the expected\n            value found in `shapes`.\n        exception_prefix: String prefix used for exception formatting.\n\n    # Returns\n        List of standardized input arrays (one array per model input).\n\n    # Raises\n        ValueError: in case of improperly formatted user-provided data.\n    \"\"\"\n    if not names:\n        if data is not None and hasattr(data, '__len__') and len(data):\n            raise ValueError('Error when checking model ' +\n                             exception_prefix + ': '\n                             'expected no data, but got:', data)\n        return []\n    if data is None:\n        return [None for _ in range(len(names))]\n\n    if isinstance(data, dict):\n        try:\n            data = [\n                data[x].values\n                if data[x].__class__.__name__ == 'DataFrame' else data[x]\n                for x in names\n            ]\n        except KeyError as e:\n            raise ValueError('No data provided for \"' + e.args[0] +\n                             '\". Need data '\n                             'for each key in: ' + str(names))\n    elif isinstance(data, list):\n        if isinstance(data[0], list):\n            data = [np.asarray(d) for d in data]\n        elif len(names) == 1 and isinstance(data[0], (float, int)):\n            data = [np.asarray(data)]\n        else:\n            data = [\n                x.values if x.__class__.__name__ == 'DataFrame'\n                else x for x in data\n            ]\n    else:\n        data = data.values if data.__class__.__name__ == 'DataFrame' else data\n        data = [data]\n    data = [standardize_single_array(x) for x in data]\n\n    if len(data) != len(names):\n        if data and hasattr(data[0], 'shape'):\n            raise ValueError(\n                'Error when checking model ' + exception_prefix +\n                ': the list of Numpy arrays that you are passing to '\n                'your model is not the size the model expected. '\n                'Expected to see ' + str(len(names)) + ' array(s), '\n                'but instead got the following list of ' +\n                str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n        elif len(names) > 1:\n            raise ValueError(\n                'Error when checking model ' + exception_prefix +\n                ': you are passing a list as input to your model, '\n                'but the model expects a list of ' + str(len(names)) +\n                ' Numpy arrays instead. '\n                'The list you passed was: ' + str(data)[:200])\n        elif len(data) == 1 and not hasattr(data[0], 'shape'):\n            raise TypeError('Error when checking model ' + exception_prefix +\n                            ': data should be a Numpy array, or list/dict of '\n                            'Numpy arrays. Found: ' + str(data)[:200] + '...')\n        elif len(names) == 1:\n            data = [np.asarray(data)]\n\n    # Check shapes compatibility.\n    if shapes:\n        for i in range(len(names)):\n            if shapes[i] is not None and not K.is_tensor(data[i]):\n                data_shape = data[i].shape\n                shape = shapes[i]\n                if data[i].ndim != len(shape):\n                    raise ValueError(\n                        'Error when checking ' + exception_prefix +\n                        ': expected ' + names[i] + ' to have ' +\n                        str(len(shape)) + ' dimensions, but got array '\n                        'with shape ' + str(data_shape))\n                if not check_batch_axis:\n                    data_shape = data_shape[1:]\n                    shape = shape[1:]\n                for dim, ref_dim in zip(data_shape, shape):\n                    if ref_dim != dim and ref_dim:\n                        raise ValueError(\n                            'Error when checking ' + exception_prefix +\n                            ': expected ' + names[i] + ' to have shape ' +\n                            str(shape) + ' but got array with shape ' +\n                            str(data_shape))\n    return data",
        "begin_line": 32,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00037383177570093456,
            "pseudo_dstar_susp": 0.0008673026886383347,
            "pseudo_tarantula_susp": 0.0004420866489832007,
            "pseudo_op2_susp": 0.0008673026886383347,
            "pseudo_barinel_susp": 0.0004420866489832007
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_sample_or_class_weights#142",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_sample_or_class_weights(x_weight, output_names, weight_type)",
        "snippet": "def standardize_sample_or_class_weights(x_weight,\n                                        output_names,\n                                        weight_type):\n    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n\n    # Arguments\n        x_weight: User-provided `sample_weight` or `class_weight` argument.\n        output_names: List of output names (strings) in the model.\n        weight_type: A string used purely for exception printing.\n\n    # Returns\n        A list of `sample_weight` or `class_weight` where there are exactly\n            one element per model output.\n\n    # Raises\n        ValueError: In case of invalid user-provided argument.\n    \"\"\"\n    if x_weight is None or len(x_weight) == 0:\n        return [None for _ in output_names]\n    if len(output_names) == 1:\n        if isinstance(x_weight, list) and len(x_weight) == 1:\n            return x_weight\n        if isinstance(x_weight, dict) and output_names[0] in x_weight:\n            return [x_weight[output_names[0]]]\n        else:\n            return [x_weight]\n    if isinstance(x_weight, list):\n        if len(x_weight) != len(output_names):\n            raise ValueError('Provided `' + weight_type + '` was a list of ' +\n                             str(len(x_weight)) +\n                             ' elements, but the model has ' +\n                             str(len(output_names)) + ' outputs. '\n                             'You should provide one `' + weight_type + '`'\n                             'array per model output.')\n        return x_weight\n    if isinstance(x_weight, dict):\n        x_weights = []\n        for name in output_names:\n            x_weights.append(x_weight.get(name))\n        return x_weights\n    else:\n        raise TypeError('The model has multiple outputs, so `' +\n                        weight_type + '` '\n                        'should be either a list or a dict. '\n                        'Provided `' + weight_type +\n                        '` type not understood: ' +\n                        str(x_weight))",
        "begin_line": 142,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00037650602409638556,
            "pseudo_dstar_susp": 0.0008787346221441124,
            "pseudo_tarantula_susp": 0.0003112356053532524,
            "pseudo_op2_susp": 0.0008787346221441124,
            "pseudo_barinel_susp": 0.0003112356053532524
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_class_weights#191",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_class_weights(class_weight, output_names)",
        "snippet": "def standardize_class_weights(class_weight, output_names):\n    return standardize_sample_or_class_weights(class_weight,\n                                               output_names,\n                                               'class_weight')",
        "begin_line": 191,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003270111183780249,
            "pseudo_dstar_susp": 0.0005595970900951316,
            "pseudo_tarantula_susp": 0.0003112356053532524,
            "pseudo_op2_susp": 0.0005595970900951316,
            "pseudo_barinel_susp": 0.0003112356053532524
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_sample_weights#197",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_sample_weights(sample_weight, output_names)",
        "snippet": "def standardize_sample_weights(sample_weight, output_names):\n    return standardize_sample_or_class_weights(sample_weight,\n                                               output_names,\n                                               'sample_weight')",
        "begin_line": 197,
        "end_line": 200,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003270111183780249,
            "pseudo_dstar_susp": 0.0005595970900951316,
            "pseudo_tarantula_susp": 0.0003112356053532524,
            "pseudo_op2_susp": 0.0005595970900951316,
            "pseudo_barinel_susp": 0.0003112356053532524
        }
    },
    {
        "name": "keras.engine.training_utils.check_array_length_consistency#203",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.check_array_length_consistency(inputs, targets, weights=None)",
        "snippet": "def check_array_length_consistency(inputs, targets, weights=None):\n    \"\"\"Checks if batch axes are the same for numpy arrays.\n\n    # Arguments\n        inputs: list of Numpy arrays of inputs.\n        targets: list of Numpy arrays of targets.\n        weights: list of Numpy arrays of sample weights.\n\n    # Raises\n        ValueError: in case of incorrectly formatted data.\n    \"\"\"\n    def set_of_lengths(x):\n        # return a set with the variation between\n        # different shapes, with None => 0\n        if x is None:\n            return {0}\n        else:\n            return set([0 if y is None else int(y.shape[0]) for y in x])\n\n    set_x = set_of_lengths(inputs)\n    set_y = set_of_lengths(targets)\n    set_w = set_of_lengths(weights)\n    if len(set_x) > 1:\n        raise ValueError('All input arrays (x) should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([x.shape for x in inputs]))\n    if len(set_y) > 1:\n        raise ValueError('All target arrays (y) should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([y.shape for y in targets]))\n    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n        raise ValueError('Input arrays should have '\n                         'the same number of samples as target arrays. '\n                         'Found ' + str(list(set_x)[0]) + ' input samples '\n                         'and ' + str(list(set_y)[0]) + ' target samples.')\n    if len(set_w) > 1:\n        raise ValueError('All sample_weight arrays should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([w.shape for w in weights]))\n    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n        raise ValueError('Sample_weight arrays should have '\n                         'the same number of samples as target arrays. Got ' +\n                         str(list(set_y)[0]) + ' input samples and ' +\n                         str(list(set_w)[0]) + ' target samples.')",
        "begin_line": 203,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003270111183780249,
            "pseudo_dstar_susp": 0.0005595970900951316,
            "pseudo_tarantula_susp": 0.0003112356053532524,
            "pseudo_op2_susp": 0.0005595970900951316,
            "pseudo_barinel_susp": 0.0003112356053532524
        }
    },
    {
        "name": "keras.engine.training_utils.set_of_lengths#214",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.set_of_lengths(x)",
        "snippet": "    def set_of_lengths(x):\n        # return a set with the variation between\n        # different shapes, with None => 0\n        if x is None:\n            return {0}\n        else:\n            return set([0 if y is None else int(y.shape[0]) for y in x])",
        "begin_line": 214,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00037650602409638556,
            "pseudo_dstar_susp": 0.0008787346221441124,
            "pseudo_tarantula_susp": 0.0003112356053532524,
            "pseudo_op2_susp": 0.0008787346221441124,
            "pseudo_barinel_susp": 0.0003112356053532524
        }
    },
    {
        "name": "keras.engine.training_utils.check_loss_and_target_compatibility#249",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.check_loss_and_target_compatibility(targets, loss_fns, output_shapes)",
        "snippet": "def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n    \"\"\"Does validation on the compatibility of targets and loss functions.\n\n    This helps prevent users from using loss functions incorrectly.\n\n    # Arguments\n        targets: list of Numpy arrays of targets.\n        loss_fns: list of loss functions.\n        output_shapes: list of shapes of model outputs.\n\n    # Raises\n        ValueError: if a loss function or target array\n            is incompatible with an output.\n    \"\"\"\n    key_losses = {losses.mean_squared_error,\n                  losses.binary_crossentropy,\n                  losses.categorical_crossentropy}\n    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n        if y is None or loss is None:\n            continue\n        if loss is losses.categorical_crossentropy:\n            if y.shape[-1] == 1:\n                raise ValueError(\n                    'You are passing a target array of shape ' + str(y.shape) +\n                    ' while using as loss `categorical_crossentropy`. '\n                    '`categorical_crossentropy` expects '\n                    'targets to be binary matrices (1s and 0s) '\n                    'of shape (samples, classes). '\n                    'If your targets are integer classes, '\n                    'you can convert them to the expected format via:\\n'\n                    '```\\n'\n                    'from keras.utils import to_categorical\\n'\n                    'y_binary = to_categorical(y_int)\\n'\n                    '```\\n'\n                    '\\n'\n                    'Alternatively, you can use the loss function '\n                    '`sparse_categorical_crossentropy` instead, '\n                    'which does expect integer targets.')\n        if loss in key_losses:\n            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n                if out_dim is not None and target_dim != out_dim:\n                    raise ValueError(\n                        'A target array with shape ' + str(y.shape) +\n                        ' was passed for an output of shape ' + str(shape) +\n                        ' while using as loss `' + loss.__name__ + '`. '\n                        'This loss expects '\n                        'targets to have the same shape '\n                        'as the output.')",
        "begin_line": 249,
        "end_line": 296,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003444712366517396,
            "pseudo_dstar_susp": 0.0005868544600938967,
            "pseudo_tarantula_susp": 0.0003718854592785422,
            "pseudo_op2_susp": 0.0005868544600938967,
            "pseudo_barinel_susp": 0.0003718854592785422
        }
    },
    {
        "name": "keras.engine.training_utils.collect_metrics#299",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.collect_metrics(metrics, output_names)",
        "snippet": "def collect_metrics(metrics, output_names):\n    \"\"\"Maps metric functions to model outputs.\n\n    # Arguments\n        metrics: a list or dict of metric functions.\n        output_names: a list of the names (strings) of model outputs.\n\n    # Returns\n        A list (one entry per model output) of lists of metric functions.\n        For instance, if the model has 2 outputs, and for the first output\n        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n        and just \"binary_accuracy\" for the second output,\n        the list would look like:\n            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n\n    # Raises\n        TypeError: if an incorrect type is passed for the `metrics` argument.\n    \"\"\"\n    if not metrics:\n        return [[] for _ in output_names]\n    if isinstance(metrics, list):\n        # we then apply all metrics to all outputs.\n        return [copy.copy(metrics) for _ in output_names]\n    elif isinstance(metrics, dict):\n        nested_metrics = []\n        for name in output_names:\n            output_metrics = metrics.get(name, [])\n            output_metrics = to_list(output_metrics)\n            nested_metrics.append(output_metrics)\n        return nested_metrics\n    else:\n        raise TypeError('Type of `metrics` argument not understood. '\n                        'Expected a list or dictionary, found: ' +\n                        str(metrics))",
        "begin_line": 299,
        "end_line": 332,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001718213058419244,
            "pseudo_dstar_susp": 0.027777777777777776,
            "pseudo_tarantula_susp": 0.0006443298969072165,
            "pseudo_op2_susp": 0.027777777777777776,
            "pseudo_barinel_susp": 0.0006443298969072165
        }
    },
    {
        "name": "keras.engine.training_utils.batch_shuffle#335",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.batch_shuffle(index_array, batch_size)",
        "snippet": "def batch_shuffle(index_array, batch_size):\n    \"\"\"Shuffles an array in a batch-wise fashion.\n\n    Useful for shuffling HDF5 arrays\n    (where one cannot access arbitrary indices).\n\n    # Arguments\n        index_array: array of indices to be shuffled.\n        batch_size: integer.\n\n    # Returns\n        The `index_array` array, shuffled in a batch-wise fashion.\n    \"\"\"\n    batch_count = int(len(index_array) / batch_size)\n    # to reshape we need to be cleanly divisible by batch size\n    # we stash extra items and reappend them after shuffling\n    last_batch = index_array[batch_count * batch_size:]\n    index_array = index_array[:batch_count * batch_size]\n    index_array = index_array.reshape((batch_count, batch_size))\n    np.random.shuffle(index_array)\n    index_array = index_array.flatten()\n    return np.append(index_array, last_batch)",
        "begin_line": 335,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.training_utils.make_batches#359",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.make_batches(size, batch_size)",
        "snippet": "def make_batches(size, batch_size):\n    \"\"\"Returns a list of batch indices (tuples of indices).\n\n    # Arguments\n        size: Integer, total size of the data to slice into batches.\n        batch_size: Integer, batch size.\n\n    # Returns\n        A list of tuples of array indices.\n    \"\"\"\n    num_batches = (size + batch_size - 1) // batch_size  # round up\n    return [(i * batch_size, min(size, (i + 1) * batch_size))\n            for i in range(num_batches)]",
        "begin_line": 359,
        "end_line": 371,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00027412280701754384,
            "pseudo_dstar_susp": 0.0005254860746190226,
            "pseudo_tarantula_susp": 0.00026014568158168577,
            "pseudo_op2_susp": 0.0005254860746190226,
            "pseudo_barinel_susp": 0.00026014568158168577
        }
    },
    {
        "name": "keras.engine.training_utils.weighted_masked_objective#374",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.weighted_masked_objective(fn)",
        "snippet": "def weighted_masked_objective(fn):\n    \"\"\"Adds support for masking and sample-weighting to an objective function.\n\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    \"\"\"\n    if fn is None:\n        return None\n\n    def weighted(y_true, y_pred, weights, mask=None):\n        \"\"\"Wrapper function.\n\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n\n        # Returns\n            Scalar tensor.\n        \"\"\"\n        # score_array has ndim >= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask)\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)\n    return weighted",
        "begin_line": 374,
        "end_line": 424,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005543237250554324,
            "pseudo_dstar_susp": 0.0009950248756218905,
            "pseudo_tarantula_susp": 0.0004962779156327543,
            "pseudo_op2_susp": 0.0009950248756218905,
            "pseudo_barinel_susp": 0.0004962779156327543
        }
    },
    {
        "name": "keras.engine.training_utils.weighted#391",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.weighted(y_true, y_pred, weights, mask=None)",
        "snippet": "    def weighted(y_true, y_pred, weights, mask=None):\n        \"\"\"Wrapper function.\n\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n\n        # Returns\n            Scalar tensor.\n        \"\"\"\n        # score_array has ndim >= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask)\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)",
        "begin_line": 391,
        "end_line": 423,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017006802721088435,
            "pseudo_dstar_susp": 0.025,
            "pseudo_tarantula_susp": 0.0005149330587023687,
            "pseudo_op2_susp": 0.025,
            "pseudo_barinel_susp": 0.0005149330587023687
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_weights#427",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_weights(y, sample_weight=None, class_weight=None, sample_weight_mode=None)",
        "snippet": "def standardize_weights(y,\n                        sample_weight=None,\n                        class_weight=None,\n                        sample_weight_mode=None):\n    \"\"\"Performs sample weight validation and standardization.\n\n    Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array.\n\n    # Arguments\n        y: Numpy array of model targets to be weighted.\n        sample_weight: User-provided `sample_weight` argument.\n        class_weight: User-provided `class_weight` argument.\n        sample_weight_mode: One of `None` or `\"temporal\"`.\n            `\"temporal\"` indicated that we expect 2D weight data\n            that will be applied to the last 2 dimensions of\n            the targets (i.e. we are weighting timesteps, not samples).\n\n    # Returns\n        A numpy array of target weights, one entry per sample to weight.\n\n    # Raises\n        ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    if sample_weight_mode is not None:\n        if sample_weight_mode != 'temporal':\n            raise ValueError('\"sample_weight_mode '\n                             'should be None or \"temporal\". '\n                             'Found: ' + str(sample_weight_mode))\n        if len(y.shape) < 3:\n            raise ValueError('Found a sample_weight array for '\n                             'an input with shape ' +\n                             str(y.shape) + '. '\n                             'Timestep-wise sample weighting (use of '\n                             'sample_weight_mode=\"temporal\") is restricted to '\n                             'outputs that are at least 3D, i.e. that have '\n                             'a time dimension.')\n        if sample_weight is not None and len(sample_weight.shape) != 2:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) + '. '\n                             'In order to use timestep-wise sample weighting, '\n                             'you should pass a 2D sample_weight array.')\n    else:\n        if sample_weight is not None and len(sample_weight.shape) != 1:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) + '. '\n                             'In order to use timestep-wise sample weights, '\n                             'you should specify '\n                             'sample_weight_mode=\"temporal\" '\n                             'in compile(). If you just mean to use '\n                             'sample-wise weights, make sure your '\n                             'sample_weight array is 1D.')\n\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n\n    if sample_weight is not None:\n        if len(sample_weight.shape) > len(y.shape):\n            raise ValueError('Found a sample_weight with shape' +\n                             str(sample_weight.shape) + '.'\n                             'Expected sample_weight with rank '\n                             'less than or equal to ' + str(len(y.shape)))\n\n        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) +\n                             ' for an input with shape ' +\n                             str(y.shape) + '. '\n                             'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n        if len(y.shape) > 2:\n            raise ValueError('`class_weight` not supported for '\n                             '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n        else:\n            y_classes = y\n\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n\n        if len(weights) != len(y_classes):\n            # subtract the sets to pick all missing classes\n            existing_classes = set(y_classes)\n            existing_class_weight = set(class_weight.keys())\n            raise ValueError('`class_weight` must contain '\n                             'all classes in the data.'\n                             ' The classes %s exist in the data but not in '\n                             '`class_weight`.'\n                             % (existing_classes - existing_class_weight))\n        return weights\n    else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())",
        "begin_line": 427,
        "end_line": 526,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003360215053763441,
            "pseudo_dstar_susp": 0.0005861664712778429,
            "pseudo_tarantula_susp": 0.0003194888178913738,
            "pseudo_op2_susp": 0.0005861664712778429,
            "pseudo_barinel_susp": 0.0003194888178913738
        }
    },
    {
        "name": "keras.engine.training_utils.check_num_samples#529",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.check_num_samples(ins, batch_size=None, steps=None, steps_name='steps')",
        "snippet": "def check_num_samples(ins,\n                      batch_size=None,\n                      steps=None,\n                      steps_name='steps'):\n    \"\"\"Checks the number of samples provided for training and evaluation.\n\n    The number of samples is not defined when running with `steps`,\n    in which case the number of samples is set to `None`.\n\n    # Arguments\n        ins: List of tensors to be fed to the Keras function.\n        batch_size: Integer batch size or `None` if not defined.\n        steps: Total number of steps (batches of samples)\n            before declaring `predict_loop` finished.\n            Ignored with the default value of `None`.\n        steps_name: The public API's parameter name for `steps`.\n\n    # Raises\n        ValueError: when `steps` is `None` and the attribute `ins.shape`\n        does not exist. Also raises ValueError when `steps` is not `None`\n        and `batch_size` is not `None` because they are mutually\n        exclusive.\n\n    # Returns\n        When steps is `None`, returns the number of samples to be\n        processed based on the size of the first dimension of the\n        first input numpy array. When steps is not `None` and\n        `batch_size` is `None`, returns `None`.\n\n    # Raises\n        ValueError: In case of invalid arguments.\n    \"\"\"\n    if steps is not None and batch_size is not None:\n        raise ValueError(\n            'If ' + steps_name + ' is set, the `batch_size` must be None.')\n\n    if not ins or any(K.is_tensor(x) for x in ins):\n        if steps is None:\n            raise ValueError(\n                'If your data is in the form of symbolic tensors, '\n                'you should specify the `' + steps_name + '` argument '\n                '(instead of the `batch_size` argument, '\n                'because symbolic tensors are expected to produce '\n                'batches of input data).')\n        return None\n\n    if hasattr(ins[0], 'shape'):\n        return int(ins[0].shape[0])\n    return None  # Edge case where ins == [static_learning_phase]",
        "begin_line": 529,
        "end_line": 577,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000273972602739726,
            "pseudo_dstar_susp": 0.0005249343832020997,
            "pseudo_tarantula_susp": 0.00026014568158168577,
            "pseudo_op2_susp": 0.0005249343832020997,
            "pseudo_barinel_susp": 0.00026014568158168577
        }
    },
    {
        "name": "keras.engine.training.Model.compile#37",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)",
        "snippet": "    def compile(self, optimizer,\n                loss=None,\n                metrics=None,\n                loss_weights=None,\n                sample_weight_mode=None,\n                weighted_metrics=None,\n                target_tensors=None,\n                **kwargs):\n        \"\"\"Configures the model for training.\n\n        # Arguments\n            optimizer: String (name of optimizer) or optimizer instance.\n                See [optimizers](/optimizers).\n            loss: String (name of objective function) or objective function.\n                See [losses](/losses).\n                If the model has multiple outputs, you can use a different loss\n                on each output by passing a dictionary or a list of losses.\n                The loss value that will be minimized by the model\n                will then be the sum of all individual losses.\n            metrics: List of metrics to be evaluated by the model\n                during training and testing.\n                Typically you will use `metrics=['accuracy']`.\n                To specify different metrics for different outputs of a\n                multi-output model, you could also pass a dictionary,\n                such as `metrics={'output_a': 'accuracy'}`.\n            loss_weights: Optional list or dictionary specifying scalar\n                coefficients (Python floats) to weight the loss contributions\n                of different model outputs.\n                The loss value that will be minimized by the model\n                will then be the *weighted sum* of all individual losses,\n                weighted by the `loss_weights` coefficients.\n                If a list, it is expected to have a 1:1 mapping\n                to the model's outputs. If a tensor, it is expected to map\n                output names (strings) to scalar coefficients.\n            sample_weight_mode: If you need to do timestep-wise\n                sample weighting (2D weights), set this to `\"temporal\"`.\n                `None` defaults to sample-wise weights (1D).\n                If the model has multiple outputs, you can use a different\n                `sample_weight_mode` on each output by passing a\n                dictionary or a list of modes.\n            weighted_metrics: List of metrics to be evaluated and weighted\n                by sample_weight or class_weight during training and testing.\n            target_tensors: By default, Keras will create placeholders for the\n                model's target, which will be fed with the target data during\n                training. If instead you would like to use your own\n                target tensors (in turn, Keras will not expect external\n                Numpy data for these targets at training time), you\n                can specify them via the `target_tensors` argument. It can be\n                a single tensor (for a single-output model), a list of tensors,\n                or a dict mapping output names to target tensors.\n            **kwargs: When using the Theano/CNTK backends, these arguments\n                are passed into `K.function`.\n                When using the TensorFlow backend,\n                these arguments are passed into `tf.Session.run`.\n\n        # Raises\n            ValueError: In case of invalid arguments for\n                `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n        \"\"\"\n        self.optimizer = optimizers.get(optimizer)\n        self.loss = loss or []\n        self.metrics = metrics or []\n        self.loss_weights = loss_weights\n        self.sample_weight_mode = sample_weight_mode\n        self.weighted_metrics = weighted_metrics\n\n        if not self.built:\n            # Model is not compilable because\n            # it does not know its number of inputs\n            # and outputs, nor their shapes and names.\n            # We will compile after the first\n            # time the model gets called on training data.\n            return\n        self._is_compiled = True\n\n        # Prepare loss functions.\n        if isinstance(loss, dict):\n            for name in loss:\n                if name not in self.output_names:\n                    raise ValueError('Unknown entry in loss '\n                                     'dictionary: \"' + name + '\". '\n                                     'Only expected the following keys: ' +\n                                     str(self.output_names))\n            loss_functions = []\n            for name in self.output_names:\n                if name not in loss:\n                    warnings.warn('Output \"' + name +\n                                  '\" missing from loss dictionary. '\n                                  'We assume this was done on purpose, '\n                                  'and we will not be expecting '\n                                  'any data to be passed to \"' + name +\n                                  '\" during training.', stacklevel=2)\n                loss_functions.append(losses.get(loss.get(name)))\n        elif isinstance(loss, list):\n            if len(loss) != len(self.outputs):\n                raise ValueError('When passing a list as loss, '\n                                 'it should have one entry per model outputs. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed loss=' +\n                                 str(loss))\n            loss_functions = [losses.get(l) for l in loss]\n        else:\n            loss_function = losses.get(loss)\n            loss_functions = [loss_function for _ in range(len(self.outputs))]\n        self.loss_functions = loss_functions\n        weighted_losses = [\n            weighted_masked_objective(fn) for fn in loss_functions]\n        skip_target_indices = []\n        skip_target_weighing_indices = []\n        self._feed_outputs = []\n        self._feed_output_names = []\n        self._feed_output_shapes = []\n        self._feed_loss_fns = []\n        for i in range(len(weighted_losses)):\n            if weighted_losses[i] is None:\n                skip_target_indices.append(i)\n                skip_target_weighing_indices.append(i)\n\n        # Prepare output masks.\n        masks = self.compute_mask(self.inputs, mask=None)\n        if masks is None:\n            masks = [None for _ in self.outputs]\n        masks = to_list(masks)\n\n        # Prepare loss weights.\n        if loss_weights is None:\n            loss_weights_list = [1. for _ in range(len(self.outputs))]\n        elif isinstance(loss_weights, dict):\n            for name in loss_weights:\n                if name not in self.output_names:\n                    raise ValueError('Unknown entry in loss_weights '\n                                     'dictionary: \"' + name + '\". '\n                                     'Only expected the following keys: ' +\n                                     str(self.output_names))\n            loss_weights_list = []\n            for name in self.output_names:\n                loss_weights_list.append(loss_weights.get(name, 1.))\n        elif isinstance(loss_weights, list):\n            if len(loss_weights) != len(self.outputs):\n                raise ValueError('When passing a list as loss_weights, '\n                                 'it should have one entry per model output. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed loss_weights=' +\n                                 str(loss_weights))\n            loss_weights_list = loss_weights\n        else:\n            raise TypeError('Could not interpret loss_weights argument: ' +\n                            str(loss_weights) +\n                            ' - expected a list of dicts.')\n\n        # Prepare targets of model.\n        self.targets = []\n        self._feed_targets = []\n        if target_tensors is not None:\n            if isinstance(target_tensors, list):\n                if len(target_tensors) != len(self.outputs):\n                    raise ValueError(\n                        'When passing a list as `target_tensors`, '\n                        'it should have one entry per model output. '\n                        'The model has ' + str(len(self.outputs)) +\n                        ' outputs, but you passed target_tensors=' +\n                        str(target_tensors))\n            elif isinstance(target_tensors, dict):\n                for name in target_tensors:\n                    if name not in self.output_names:\n                        raise ValueError('Unknown entry in `target_tensors` '\n                                         'dictionary: \"' + name + '\". '\n                                         'Only expected the following keys: ' +\n                                         str(self.output_names))\n                tmp_target_tensors = []\n                for name in self.output_names:\n                    tmp_target_tensors.append(target_tensors.get(name, None))\n                target_tensors = tmp_target_tensors\n            elif K.is_tensor(target_tensors):\n                if len(self.outputs) != 1:\n                    raise ValueError('The model has ' + str(len(self.outputs)) +\n                                     ' outputs, but you passed a single tensor as '\n                                     '`target_tensors`. Expected a list or a dict '\n                                     'of tensors.')\n                target_tensors = [target_tensors]\n            else:\n                raise TypeError('Expected `target_tensors` to be a tensor, '\n                                'a list of tensors, or dict of tensors, but got:', target_tensors)\n\n        for i in range(len(self.outputs)):\n            if i in skip_target_indices:\n                self.targets.append(None)\n            else:\n                shape = K.int_shape(self.outputs[i])\n                name = self.output_names[i]\n                if target_tensors is not None:\n                    target = target_tensors[i]\n                else:\n                    target = None\n                if target is None or K.is_placeholder(target):\n                    if target is None:\n                        target = K.placeholder(\n                            ndim=len(shape),\n                            name=name + '_target',\n                            sparse=K.is_sparse(self.outputs[i]),\n                            dtype=K.dtype(self.outputs[i]))\n                    self._feed_targets.append(target)\n                    self._feed_outputs.append(self.outputs[i])\n                    self._feed_output_names.append(name)\n                    self._feed_output_shapes.append(shape)\n                    self._feed_loss_fns.append(self.loss_functions[i])\n                else:\n                    skip_target_weighing_indices.append(i)\n                self.targets.append(target)\n\n        # Prepare sample weights.\n        sample_weights = []\n        sample_weight_modes = []\n        if isinstance(sample_weight_mode, dict):\n            for name in sample_weight_mode:\n                if name not in self.output_names:\n                    raise ValueError('Unknown entry in '\n                                     'sample_weight_mode dictionary: \"' +\n                                     name + '\". '\n                                     'Only expected the following keys: ' +\n                                     str(self.output_names))\n            for i, name in enumerate(self.output_names):\n                if i in skip_target_weighing_indices:\n                    weight = None\n                    sample_weight_modes.append(None)\n                else:\n                    if name not in sample_weight_mode:\n                        raise ValueError('Output \"' + name +\n                                         '\" missing from sample_weight_modes '\n                                         'dictionary')\n                    if sample_weight_mode.get(name) == 'temporal':\n                        weight = K.placeholder(ndim=2,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append('temporal')\n                    else:\n                        weight = K.placeholder(ndim=1,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append(None)\n                sample_weights.append(weight)\n        elif isinstance(sample_weight_mode, list):\n            if len(sample_weight_mode) != len(self.outputs):\n                raise ValueError('When passing a list as sample_weight_mode, '\n                                 'it should have one entry per model output. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed '\n                                 'sample_weight_mode=' +\n                                 str(sample_weight_mode))\n            for i in range(len(self.output_names)):\n                if i in skip_target_weighing_indices:\n                    weight = None\n                    sample_weight_modes.append(None)\n                else:\n                    mode = sample_weight_mode[i]\n                    name = self.output_names[i]\n                    if mode == 'temporal':\n                        weight = K.placeholder(ndim=2,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append('temporal')\n                    else:\n                        weight = K.placeholder(ndim=1,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append(None)\n                sample_weights.append(weight)\n        else:\n            for i, name in enumerate(self.output_names):\n                if i in skip_target_weighing_indices:\n                    sample_weight_modes.append(None)\n                    sample_weights.append(None)\n                else:\n                    if sample_weight_mode == 'temporal':\n                        sample_weights.append(\n                            K.placeholder(ndim=2,\n                                          name=name + '_sample_weights'))\n                        sample_weight_modes.append('temporal')\n                    else:\n                        sample_weights.append(\n                            K.placeholder(ndim=1,\n                                          name=name + '_sample_weights'))\n                        sample_weight_modes.append(None)\n        self.sample_weight_modes = sample_weight_modes\n        self._feed_sample_weight_modes = []\n        for i in range(len(self.outputs)):\n            if i not in skip_target_weighing_indices:\n                self._feed_sample_weight_modes.append(\n                    self.sample_weight_modes[i])\n\n        # Prepare metrics.\n        self.metrics_names = ['loss']\n        self.metrics_tensors = []\n\n        # Compute total loss.\n        total_loss = None\n        with K.name_scope('loss'):\n            for i in range(len(self.outputs)):\n                if i in skip_target_indices:\n                    continue\n                y_true = self.targets[i]\n                y_pred = self.outputs[i]\n                weighted_loss = weighted_losses[i]\n                sample_weight = sample_weights[i]\n                mask = masks[i]\n                loss_weight = loss_weights_list[i]\n                with K.name_scope(self.output_names[i] + '_loss'):\n                    output_loss = weighted_loss(y_true, y_pred,\n                                                sample_weight, mask)\n                if len(self.outputs) > 1:\n                    self.metrics_tensors.append(output_loss)\n                    self.metrics_names.append(self.output_names[i] + '_loss')\n                if total_loss is None:\n                    total_loss = loss_weight * output_loss\n                else:\n                    total_loss += loss_weight * output_loss\n            if total_loss is None:\n                if not self.losses:\n                    raise ValueError('The model cannot be compiled '\n                                     'because it has no loss to optimize.')\n                else:\n                    total_loss = 0.\n\n            # Add regularization penalties\n            # and other layer-specific losses.\n            for loss_tensor in self.losses:\n                total_loss += loss_tensor\n\n        # List of same size as output_names.\n        # contains tuples (metrics for output, names of metrics).\n        nested_metrics = collect_metrics(metrics, self.output_names)\n        nested_weighted_metrics = collect_metrics(weighted_metrics,\n                                                  self.output_names)\n        self.metrics_updates = []\n        self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n\n        def handle_metrics(metrics, weights=None):\n            metric_name_prefix = 'weighted_' if weights is not None else ''\n\n            for metric in metrics:\n                if metric in ('accuracy', 'acc', 'crossentropy', 'ce'):\n                    # custom handling of accuracy/crossentropy\n                    # (because of class mode duality)\n                    output_shape = K.int_shape(self.outputs[i])\n                    if (output_shape[-1] == 1 or\n                       self.loss_functions[i] == losses.binary_crossentropy):\n                        # case: binary accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.binary_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.binary_crossentropy\n                    elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n                        # case: categorical accuracy/crossentropy\n                        # with sparse targets\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.sparse_categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.sparse_categorical_crossentropy\n                    else:\n                        # case: categorical accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.categorical_crossentropy\n                    if metric in ('accuracy', 'acc'):\n                            suffix = 'acc'\n                    elif metric in ('crossentropy', 'ce'):\n                            suffix = 'ce'\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    metric_name = metric_name_prefix + suffix\n                else:\n                    metric_fn = metrics_module.get(metric)\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    # Get metric name as string\n                    if hasattr(metric_fn, 'name'):\n                        metric_name = metric_fn.name\n                    else:\n                        metric_name = metric_fn.__name__\n                    metric_name = metric_name_prefix + metric_name\n\n                with K.name_scope(metric_name):\n                    metric_result = weighted_metric_fn(y_true, y_pred,\n                                                       weights=weights,\n                                                       mask=masks[i])\n\n                # Append to self.metrics_names, self.metric_tensors,\n                # self.stateful_metric_names\n                if len(self.output_names) > 1:\n                    metric_name = self.output_names[i] + '_' + metric_name\n                # Dedupe name\n                j = 1\n                base_metric_name = metric_name\n                while metric_name in self.metrics_names:\n                    metric_name = base_metric_name + '_' + str(j)\n                    j += 1\n                self.metrics_names.append(metric_name)\n                self.metrics_tensors.append(metric_result)\n\n                # Keep track of state updates created by\n                # stateful metrics (i.e. metrics layers).\n                if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                    self.stateful_metric_names.append(metric_name)\n                    self.stateful_metric_functions.append(metric_fn)\n                    self.metrics_updates += metric_fn.updates\n        with K.name_scope('metrics'):\n            for i in range(len(self.outputs)):\n                if i in skip_target_indices:\n                    continue\n\n                y_true = self.targets[i]\n                y_pred = self.outputs[i]\n                weights = sample_weights[i]\n                output_metrics = nested_metrics[i]\n                output_weighted_metrics = nested_weighted_metrics[i]\n                handle_metrics(output_metrics)\n                handle_metrics(output_weighted_metrics, weights=weights)\n\n        # Prepare gradient updates and state updates.\n        self.total_loss = total_loss\n        self.sample_weights = sample_weights\n        self._feed_sample_weights = []\n        for i in range(len(self.sample_weights)):\n            if i not in skip_target_weighing_indices:\n                self._feed_sample_weights.append(sample_weights[i])\n\n        # Functions for train, test and predict will\n        # be compiled lazily when required.\n        # This saves time when the user is not using all functions.\n        self._function_kwargs = kwargs\n\n        self.train_function = None\n        self.test_function = None\n        self.predict_function = None\n\n        # Collected trainable weights, sorted in topological order.\n        trainable_weights = self.trainable_weights\n        self._collected_trainable_weights = trainable_weights",
        "begin_line": 37,
        "end_line": 470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.03125,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.03125,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.training.Model.handle_metrics#370",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.handle_metrics(metrics, weights=None)",
        "snippet": "        def handle_metrics(metrics, weights=None):\n            metric_name_prefix = 'weighted_' if weights is not None else ''\n\n            for metric in metrics:\n                if metric in ('accuracy', 'acc', 'crossentropy', 'ce'):\n                    # custom handling of accuracy/crossentropy\n                    # (because of class mode duality)\n                    output_shape = K.int_shape(self.outputs[i])\n                    if (output_shape[-1] == 1 or\n                       self.loss_functions[i] == losses.binary_crossentropy):\n                        # case: binary accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.binary_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.binary_crossentropy\n                    elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n                        # case: categorical accuracy/crossentropy\n                        # with sparse targets\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.sparse_categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.sparse_categorical_crossentropy\n                    else:\n                        # case: categorical accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.categorical_crossentropy\n                    if metric in ('accuracy', 'acc'):\n                            suffix = 'acc'\n                    elif metric in ('crossentropy', 'ce'):\n                            suffix = 'ce'\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    metric_name = metric_name_prefix + suffix\n                else:\n                    metric_fn = metrics_module.get(metric)\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    # Get metric name as string\n                    if hasattr(metric_fn, 'name'):\n                        metric_name = metric_fn.name\n                    else:\n                        metric_name = metric_fn.__name__\n                    metric_name = metric_name_prefix + metric_name\n\n                with K.name_scope(metric_name):\n                    metric_result = weighted_metric_fn(y_true, y_pred,\n                                                       weights=weights,\n                                                       mask=masks[i])\n\n                # Append to self.metrics_names, self.metric_tensors,\n                # self.stateful_metric_names\n                if len(self.output_names) > 1:\n                    metric_name = self.output_names[i] + '_' + metric_name\n                # Dedupe name\n                j = 1\n                base_metric_name = metric_name\n                while metric_name in self.metrics_names:\n                    metric_name = base_metric_name + '_' + str(j)\n                    j += 1\n                self.metrics_names.append(metric_name)\n                self.metrics_tensors.append(metric_result)\n\n                # Keep track of state updates created by\n                # stateful metrics (i.e. metrics layers).\n                if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                    self.stateful_metric_names.append(metric_name)\n                    self.stateful_metric_functions.append(metric_fn)\n                    self.metrics_updates += metric_fn.updates",
        "begin_line": 370,
        "end_line": 437,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017064846416382253,
            "pseudo_dstar_susp": 0.02631578947368421,
            "pseudo_tarantula_susp": 0.0009074410163339383,
            "pseudo_op2_susp": 0.02631578947368421,
            "pseudo_barinel_susp": 0.0009074410163339383
        }
    },
    {
        "name": "keras.engine.training.Model._check_trainable_weights_consistency#472",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._check_trainable_weights_consistency(self)",
        "snippet": "    def _check_trainable_weights_consistency(self):\n        \"\"\"Check trainable weights count consistency.\n\n        This will raise a warning if `trainable_weights` and\n        `_collected_trainable_weights` are inconsistent (i.e. have different\n        number of parameters).\n        Inconsistency will typically arise when one modifies `model.trainable`\n        without calling `model.compile` again.\n        \"\"\"\n        if not hasattr(self, '_collected_trainable_weights'):\n            return\n\n        if (len(self.trainable_weights) !=\n                len(self._collected_trainable_weights)):\n            warnings.warn(UserWarning(\n                'Discrepancy between trainable weights and collected trainable'\n                ' weights, did you set `model.trainable` without calling'\n                ' `model.compile` after ?'))",
        "begin_line": 472,
        "end_line": 489,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009345794392523364,
            "pseudo_dstar_susp": 0.0008960573476702509,
            "pseudo_tarantula_susp": 0.0055248618784530384,
            "pseudo_op2_susp": 0.0008960573476702509,
            "pseudo_barinel_susp": 0.0055248618784530384
        }
    },
    {
        "name": "keras.engine.training.Model._make_train_function#491",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._make_train_function(self)",
        "snippet": "    def _make_train_function(self):\n        if not hasattr(self, 'train_function'):\n            raise RuntimeError('You must compile your model before using it.')\n        self._check_trainable_weights_consistency()\n        if self.train_function is None:\n            inputs = (self._feed_inputs +\n                      self._feed_targets +\n                      self._feed_sample_weights)\n            if self._uses_dynamic_learning_phase():\n                inputs += [K.learning_phase()]\n\n            with K.name_scope('training'):\n                with K.name_scope(self.optimizer.__class__.__name__):\n                    training_updates = self.optimizer.get_updates(\n                        params=self._collected_trainable_weights,\n                        loss=self.total_loss)\n                updates = (self.updates +\n                           training_updates +\n                           self.metrics_updates)\n                # Gets loss and metrics. Updates weights at each call.\n                self.train_function = K.function(\n                    inputs,\n                    [self.total_loss] + self.metrics_tensors,\n                    updates=updates,\n                    name='train_function',\n                    **self._function_kwargs)",
        "begin_line": 491,
        "end_line": 516,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033311125916055963,
            "pseudo_dstar_susp": 0.0005777007510109763,
            "pseudo_tarantula_susp": 0.0003169572107765452,
            "pseudo_op2_susp": 0.0005777007510109763,
            "pseudo_barinel_susp": 0.0003169572107765452
        }
    },
    {
        "name": "keras.engine.training.Model._make_test_function#518",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._make_test_function(self)",
        "snippet": "    def _make_test_function(self):\n        if not hasattr(self, 'test_function'):\n            raise RuntimeError('You must compile your model before using it.')\n        if self.test_function is None:\n            inputs = (self._feed_inputs +\n                      self._feed_targets +\n                      self._feed_sample_weights)\n            if self._uses_dynamic_learning_phase():\n                inputs += [K.learning_phase()]\n            # Return loss and metrics, no gradient updates.\n            # Does update the network states.\n            self.test_function = K.function(\n                inputs,\n                [self.total_loss] + self.metrics_tensors,\n                updates=self.state_updates + self.metrics_updates,\n                name='test_function',\n                **self._function_kwargs)",
        "begin_line": 518,
        "end_line": 534,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031240237425804435,
            "pseudo_dstar_susp": 0.00030553009471432935,
            "pseudo_tarantula_susp": 0.0003718854592785422,
            "pseudo_op2_susp": 0.00030553009471432935,
            "pseudo_barinel_susp": 0.0003718854592785422
        }
    },
    {
        "name": "keras.engine.training.Model._make_predict_function#536",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._make_predict_function(self)",
        "snippet": "    def _make_predict_function(self):\n        if not hasattr(self, 'predict_function'):\n            self.predict_function = None\n        if self.predict_function is None:\n            if self._uses_dynamic_learning_phase():\n                inputs = self._feed_inputs + [K.learning_phase()]\n            else:\n                inputs = self._feed_inputs\n            # Gets network outputs. Does not update weights.\n            # Does update the network states.\n            kwargs = getattr(self, '_function_kwargs', {})\n            self.predict_function = K.function(inputs,\n                                               self.outputs,\n                                               updates=self.state_updates,\n                                               name='predict_function',\n                                               **kwargs)",
        "begin_line": 536,
        "end_line": 551,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002646202699126753,
            "pseudo_dstar_susp": 0.0002646202699126753,
            "pseudo_tarantula_susp": 0.00026483050847457627,
            "pseudo_op2_susp": 0.0002646202699126753,
            "pseudo_barinel_susp": 0.00026483050847457627
        }
    },
    {
        "name": "keras.engine.training.Model._uses_dynamic_learning_phase#553",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._uses_dynamic_learning_phase(self)",
        "snippet": "    def _uses_dynamic_learning_phase(self):\n        return (self.uses_learning_phase and\n                not isinstance(K.learning_phase(), int))",
        "begin_line": 553,
        "end_line": 555,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002959455460195324,
            "pseudo_dstar_susp": 0.0005382131324004305,
            "pseudo_tarantula_susp": 0.0002777777777777778,
            "pseudo_op2_susp": 0.0005382131324004305,
            "pseudo_barinel_susp": 0.0002777777777777778
        }
    },
    {
        "name": "keras.engine.training.Model._set_inputs#557",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._set_inputs(self, inputs, outputs=None, training=None)",
        "snippet": "    def _set_inputs(self, inputs, outputs=None, training=None):\n        \"\"\"Set model's input and output specs based on the input data received.\n\n        This is to be used for Model subclasses, which do not know at instantiation\n        time what their inputs look like.\n\n        # Arguments\n          inputs: Single array, or list of arrays. The arrays could be placeholders,\n            Numpy arrays, or data tensors.\n            - if placeholders: the model is built on top of these placeholders,\n              and we expect Numpy data to be fed for them when calling `fit`/etc.\n            - if Numpy data: we create placeholders matching the shape of the Numpy\n              arrays. We expect Numpy data to be fed for these placeholders\n              when calling `fit`/etc.\n            - if data tensors: the model is built on top of these tensors.\n              We do not expect any Numpy data to be provided when calling `fit`/etc.\n          outputs: Optional output tensors (if already computed by running the model).\n          training: Boolean or None. Only relevant in symbolic mode. Specifies\n            whether to build the model's graph in inference mode (False), training\n            mode (True), or using the Keras learning phase (None).\n        \"\"\"\n        if self.__class__.__name__ == 'Sequential':\n            # Note: we can't test whether the model\n            # is `Sequential` via `isinstance`\n            # since `Sequential` depends on `Model`.\n            if isinstance(inputs, list):\n                assert len(inputs) == 1\n                inputs = inputs[0]\n            self.build(input_shape=(None,) + inputs.shape[1:])\n            return\n\n        if self.inputs:\n            raise ValueError('Model inputs are already set.')\n\n        # On-the-fly setting of symbolic model inputs\n        # (either by using the tensor provided,\n        # or by creating a placeholder if Numpy data was provided).\n        self.inputs = []\n        self.input_names = []\n        self._feed_inputs = []\n        self._feed_input_names = []\n        self._feed_input_shapes = []\n        if isinstance(inputs, (list, tuple)):\n            inputs = list(inputs)\n        else:\n            inputs = [inputs]\n\n        for i, v in enumerate(inputs):\n            name = 'input_%d' % (i + 1)\n            self.input_names.append(name)\n            if isinstance(v, list):\n                v = np.asarray(v)\n                if v.ndim == 1:\n                    v = np.expand_dims(v, 1)\n            if isinstance(v, (np.ndarray)):\n                # We fix the placeholder shape except the batch size.\n                # This is suboptimal, but it is the best we can do with the info\n                # we have. The user should call `model._set_inputs(placeholders)`\n                # to specify custom placeholders if the need arises.\n                shape = (None,) + v.shape[1:]\n                placeholder = K.placeholder(shape=shape, name=name)\n                self.inputs.append(placeholder)\n                self._feed_inputs.append(placeholder)\n                self._feed_input_names.append(name)\n                self._feed_input_shapes.append(shape)\n            else:\n                # Assumed tensor - TODO(fchollet) additional type check?\n                self.inputs.append(v)\n                if K.is_placeholder(v):\n                    self._feed_inputs.append(v)\n                    self._feed_input_names.append(name)\n                    self._feed_input_shapes.append(K.int_shape(v))\n\n        if outputs is None:\n            # Obtain symbolic outputs by calling the model.\n            if self._expects_training_arg:\n                outputs = self.call(unpack_singleton(self.inputs), training=training)\n            else:\n                outputs = self.call(unpack_singleton(self.inputs))\n        if isinstance(outputs, (list, tuple)):\n            outputs = list(outputs)\n        else:\n            outputs = [outputs]\n        self.outputs = outputs\n        self.output_names = [\n            'output_%d' % (i + 1) for i in range(len(self.outputs))]\n        self.built = True",
        "begin_line": 557,
        "end_line": 643,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0004510599909788002,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.training.Model._standardize_user_data#645",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._standardize_user_data(self, x, y=None, sample_weight=None, class_weight=None, check_array_lengths=True, batch_size=None)",
        "snippet": "    def _standardize_user_data(self, x,\n                               y=None,\n                               sample_weight=None,\n                               class_weight=None,\n                               check_array_lengths=True,\n                               batch_size=None):\n        all_inputs = []\n        if not self.built:\n            # We need to use `x` to set the model inputs.\n            # We type-check that `x` and `y` are either single arrays\n            # or lists of arrays.\n            if isinstance(x, (list, tuple)):\n                if not all(isinstance(v, np.ndarray) or\n                           K.is_tensor(v) for v in x):\n                    raise ValueError('Please provide as model inputs '\n                                     'either a single '\n                                     'array or a list of arrays. '\n                                     'You passed: x=' + str(x))\n                all_inputs += list(x)\n            elif isinstance(x, dict):\n                raise ValueError('Please do not pass a dictionary '\n                                 'as model inputs.')\n            else:\n                if not isinstance(x, np.ndarray) and not K.is_tensor(x):\n                    raise ValueError('Please provide as model inputs '\n                                     'either a single '\n                                     'array or a list of arrays. '\n                                     'You passed: x=' + str(x))\n                all_inputs.append(x)\n\n            # Build the model using the retrieved inputs (value or symbolic).\n            # If values, then in symbolic-mode placeholders will be created\n            # to match the value shapes.\n            if not self.inputs:\n                self._set_inputs(x)\n\n        if y is not None:\n            if not self.optimizer:\n                raise RuntimeError('You must compile a model before '\n                                   'training/testing. '\n                                   'Use `model.compile(optimizer, loss)`.')\n            if not self._is_compiled:\n                # On-the-fly compilation of the model.\n                # We need to use `y` to set the model targets.\n                if isinstance(y, (list, tuple)):\n                    if not all(isinstance(v, np.ndarray) or\n                               K.is_tensor(v) for v in y):\n                        raise ValueError('Please provide as model targets '\n                                         'either a single '\n                                         'array or a list of arrays. '\n                                         'You passed: y=' + str(y))\n                elif isinstance(y, dict):\n                    raise ValueError('Please do not pass a dictionary '\n                                     'as model targets.')\n                else:\n                    if not isinstance(y, np.ndarray) and not K.is_tensor(y):\n                        raise ValueError('Please provide as model targets '\n                                         'either a single '\n                                         'array or a list of arrays. '\n                                         'You passed: y=' + str(y))\n                # Typecheck that all inputs are *either* value *or* symbolic.\n                if y is not None:\n                    if isinstance(y, (list, tuple)):\n                        all_inputs += list(y)\n                    else:\n                        all_inputs.append(y)\n                if any(K.is_tensor(v) for v in all_inputs):\n                    if not all(K.is_tensor(v) for v in all_inputs):\n                        raise ValueError('Do not pass inputs that mix Numpy '\n                                         'arrays and symbolic tensors. '\n                                         'You passed: x=' + str(x) +\n                                         '; y=' + str(y))\n\n                # Handle target tensors if any passed.\n                if not isinstance(y, (list, tuple)):\n                    y = [y]\n                target_tensors = [v for v in y if K.is_tensor(v)]\n                if not target_tensors:\n                    target_tensors = None\n                self.compile(optimizer=self.optimizer,\n                             loss=self.loss,\n                             metrics=self.metrics,\n                             loss_weights=self.loss_weights,\n                             target_tensors=target_tensors)\n\n        # If `x` and `y` were all symbolic,\n        # then the model should not be fed any inputs and targets.\n        # Note: in this case, `any` and `all` are equivalent since we disallow\n        # mixed symbolic/value inputs.\n        if any(K.is_tensor(v) for v in all_inputs):\n            return [], [], []\n\n        # What follows is input validation and standardization to list format,\n        # in the case where all inputs are value arrays.\n\n        if not self._is_graph_network:\n            # Case: symbolic-mode subclassed network.\n            # Do not do shape validation.\n            feed_input_names = self._feed_input_names\n            feed_input_shapes = None\n        else:\n            # Case: symbolic-mode graph network.\n            # In this case, we run extensive shape validation checks.\n            feed_input_names = self._feed_input_names\n            feed_input_shapes = self._feed_input_shapes\n\n        # Standardize the inputs.\n        x = standardize_input_data(\n            x,\n            feed_input_names,\n            feed_input_shapes,\n            check_batch_axis=False,  # Don't enforce the batch size.\n            exception_prefix='input')\n\n        if y is not None:\n            if not self._is_graph_network:\n                feed_output_names = self._feed_output_names\n                feed_output_shapes = None\n                # Sample weighting not supported in this case.\n                # TODO: consider supporting it.\n                feed_sample_weight_modes = [None for _ in self.outputs]\n            else:\n                feed_output_names = self._feed_output_names\n                feed_sample_weight_modes = self._feed_sample_weight_modes\n                feed_output_shapes = []\n                for output_shape, loss_fn in zip(self._feed_output_shapes,\n                                                 self._feed_loss_fns):\n                    if loss_fn is losses.sparse_categorical_crossentropy:\n                        if K.image_data_format() == 'channels_first' and len(\n                                output_shape) in [4, 5]:\n                            feed_output_shapes.append(\n                                (output_shape[0], 1) + output_shape[2:])\n                        else:\n                            feed_output_shapes.append(output_shape[:-1] + (1,))\n                    elif (not hasattr(loss_fn, '__name__') or\n                            getattr(losses, loss_fn.__name__, None) is None):\n                        # If `loss_fn` is not a function (e.g. callable class)\n                        # or if it not in the `losses` module, then\n                        # it is a user-defined loss and we make no assumptions\n                        # about it.\n                        feed_output_shapes.append(None)\n                    else:\n                        feed_output_shapes.append(output_shape)\n\n            # Standardize the outputs.\n            y = standardize_input_data(\n                y,\n                feed_output_names,\n                feed_output_shapes,\n                check_batch_axis=False,  # Don't enforce the batch size.\n                exception_prefix='target')\n\n            # Generate sample-wise weight values given the `sample_weight` and\n            # `class_weight` arguments.\n            sample_weights = standardize_sample_weights(\n                sample_weight, feed_output_names)\n            class_weights = standardize_class_weights(\n                class_weight, feed_output_names)\n            sample_weights = [\n                standardize_weights(ref, sw, cw, mode)\n                for (ref, sw, cw, mode) in\n                zip(y, sample_weights, class_weights,\n                    feed_sample_weight_modes)\n            ]\n            # Check that all arrays have the same length.\n            check_array_length_consistency(x, y, sample_weights)\n            if self._is_graph_network:\n                # Additional checks to avoid users mistakenly\n                # using improper loss fns.\n                check_loss_and_target_compatibility(\n                    y, self._feed_loss_fns, feed_output_shapes)\n        else:\n            y = []\n            sample_weights = []\n\n        if self.stateful and batch_size:\n            # Check that for stateful networks, number of samples is a multiple\n            # of the static batch size.\n            if x[0].shape[0] % batch_size != 0:\n                raise ValueError('In a stateful network, '\n                                 'you should only pass inputs with '\n                                 'a number of samples that can be '\n                                 'divided by the batch size. Found: ' +\n                                 str(x[0].shape[0]) + ' samples')\n        return x, y, sample_weights",
        "begin_line": 645,
        "end_line": 829,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0008787346221441124,
            "pseudo_tarantula_susp": 0.003816793893129771,
            "pseudo_op2_susp": 0.0008787346221441124,
            "pseudo_barinel_susp": 0.003816793893129771
        }
    },
    {
        "name": "keras.engine.training.Model.fit#831",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)",
        "snippet": "    def fit(self,\n            x=None,\n            y=None,\n            batch_size=None,\n            epochs=1,\n            verbose=1,\n            callbacks=None,\n            validation_split=0.,\n            validation_data=None,\n            shuffle=True,\n            class_weight=None,\n            sample_weight=None,\n            initial_epoch=0,\n            steps_per_epoch=None,\n            validation_steps=None,\n            **kwargs):\n        \"\"\"Trains the model for a given number of epochs (iterations on a dataset).\n\n        # Arguments\n            x: Numpy array of training data (if the model has a single input),\n                or list of Numpy arrays (if the model has multiple inputs).\n                If input layers in the model are named, you can also pass a\n                dictionary mapping input names to Numpy arrays.\n                `x` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            y: Numpy array of target (label) data\n                (if the model has a single output),\n                or list of Numpy arrays (if the model has multiple outputs).\n                If output layers in the model are named, you can also pass a\n                dictionary mapping output names to Numpy arrays.\n                `y` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            batch_size: Integer or `None`.\n                Number of samples per gradient update.\n                If unspecified, `batch_size` will default to 32.\n            epochs: Integer. Number of epochs to train the model.\n                An epoch is an iteration over the entire `x` and `y`\n                data provided.\n                Note that in conjunction with `initial_epoch`,\n                `epochs` is to be understood as \"final epoch\".\n                The model is not trained for a number of iterations\n                given by `epochs`, but merely until the epoch\n                of index `epochs` is reached.\n            verbose: Integer. 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = one line per epoch.\n            callbacks: List of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n                See [callbacks](/callbacks).\n            validation_split: Float between 0 and 1.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling.\n            validation_data: tuple `(x_val, y_val)` or tuple\n                `(x_val, y_val, val_sample_weights)` on which to evaluate\n                the loss and any model metrics at the end of each epoch.\n                The model will not be trained on this data.\n                `validation_data` will override `validation_split`.\n            shuffle: Boolean (whether to shuffle the training data\n                before each epoch) or str (for 'batch').\n                'batch' is a special option for dealing with the\n                limitations of HDF5 data; it shuffles in batch-sized chunks.\n                Has no effect when `steps_per_epoch` is not `None`.\n            class_weight: Optional dictionary mapping class indices (integers)\n                to a weight (float) value, used for weighting the loss function\n                (during training only).\n                This can be useful to tell the model to\n                \"pay more attention\" to samples from\n                an under-represented class.\n            sample_weight: Optional Numpy array of weights for\n                the training samples, used for weighting the loss function\n                (during training only). You can either pass a flat (1D)\n                Numpy array with the same length as the input samples\n                (1:1 mapping between weights and samples),\n                or in the case of temporal data,\n                you can pass a 2D array with shape\n                `(samples, sequence_length)`,\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                `sample_weight_mode=\"temporal\"` in `compile()`.\n            initial_epoch: Integer.\n                Epoch at which to start training\n                (useful for resuming a previous training run).\n            steps_per_epoch: Integer or `None`.\n                Total number of steps (batches of samples)\n                before declaring one epoch finished and starting the\n                next epoch. When training with input tensors such as\n                TensorFlow data tensors, the default `None` is equal to\n                the number of samples in your dataset divided by\n                the batch size, or 1 if that cannot be determined.\n            validation_steps: Only relevant if `steps_per_epoch`\n                is specified. Total number of steps (batches of samples)\n                to validate before stopping.\n\n        # Returns\n            A `History` object. Its `History.history` attribute is\n            a record of training loss values and metrics values\n            at successive epochs, as well as validation loss values\n            and validation metrics values (if applicable).\n\n        # Raises\n            RuntimeError: If the model was never compiled.\n            ValueError: In case of mismatch between the provided input data\n                and what the model expects.\n        \"\"\"\n        # Backwards compatibility\n        if batch_size is None and steps_per_epoch is None:\n            batch_size = 32\n        # Legacy support\n        if 'nb_epoch' in kwargs:\n            warnings.warn('The `nb_epoch` argument in `fit` '\n                          'has been renamed `epochs`.', stacklevel=2)\n            epochs = kwargs.pop('nb_epoch')\n        if kwargs:\n            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n        if x is None and y is None and steps_per_epoch is None:\n            raise ValueError('If fitting from data tensors, '\n                             'you should specify the `steps_per_epoch` '\n                             'argument.')\n        # Validate user data.\n        x, y, sample_weights = self._standardize_user_data(\n            x, y,\n            sample_weight=sample_weight,\n            class_weight=class_weight,\n            batch_size=batch_size)\n        # Prepare validation data.\n        do_validation = False\n        if validation_data:\n            do_validation = True\n            if len(validation_data) == 2:\n                val_x, val_y = validation_data\n                val_sample_weight = None\n            elif len(validation_data) == 3:\n                val_x, val_y, val_sample_weight = validation_data\n            else:\n                raise ValueError('When passing validation_data, '\n                                 'it must contain 2 (x_val, y_val) '\n                                 'or 3 (x_val, y_val, val_sample_weights) '\n                                 'items, however it contains %d items' %\n                                 len(validation_data))\n\n            val_x, val_y, val_sample_weights = self._standardize_user_data(\n                val_x, val_y,\n                sample_weight=val_sample_weight,\n                batch_size=batch_size)\n            if self._uses_dynamic_learning_phase():\n                val_ins = val_x + val_y + val_sample_weights + [0.]\n            else:\n                val_ins = val_x + val_y + val_sample_weights\n\n        elif validation_split and 0. < validation_split < 1.:\n            if any(K.is_tensor(t) for t in x):\n                raise ValueError(\n                    'If your data is in the form of symbolic tensors, '\n                    'you cannot use `validation_split`.')\n            do_validation = True\n            if hasattr(x[0], 'shape'):\n                split_at = int(int(x[0].shape[0]) * (1. - validation_split))\n            else:\n                split_at = int(len(x[0]) * (1. - validation_split))\n            x, val_x = (slice_arrays(x, 0, split_at),\n                        slice_arrays(x, split_at))\n            y, val_y = (slice_arrays(y, 0, split_at),\n                        slice_arrays(y, split_at))\n            sample_weights, val_sample_weights = (\n                slice_arrays(sample_weights, 0, split_at),\n                slice_arrays(sample_weights, split_at))\n            if self._uses_dynamic_learning_phase():\n                val_ins = val_x + val_y + val_sample_weights + [0.]\n            else:\n                val_ins = val_x + val_y + val_sample_weights\n\n        elif validation_steps:\n            do_validation = True\n            if self._uses_dynamic_learning_phase():\n                val_ins = [0.]\n\n        # Prepare input arrays and training function.\n        if self._uses_dynamic_learning_phase():\n            ins = x + y + sample_weights + [1.]\n        else:\n            ins = x + y + sample_weights\n        self._make_train_function()\n        f = self.train_function\n\n        # Prepare display labels.\n        out_labels = self.metrics_names\n\n        if do_validation:\n            self._make_test_function()\n            val_f = self.test_function\n            callback_metrics = copy.copy(out_labels) + [\n                'val_' + n for n in out_labels]\n        else:\n            callback_metrics = copy.copy(out_labels)\n            val_f = None\n            val_ins = []\n\n        # Delegate logic to `fit_loop`.\n        return training_arrays.fit_loop(self, f, ins,\n                                        out_labels=out_labels,\n                                        batch_size=batch_size,\n                                        epochs=epochs,\n                                        verbose=verbose,\n                                        callbacks=callbacks,\n                                        val_f=val_f,\n                                        val_ins=val_ins,\n                                        shuffle=shuffle,\n                                        callback_metrics=callback_metrics,\n                                        initial_epoch=initial_epoch,\n                                        steps_per_epoch=steps_per_epoch,\n                                        validation_steps=validation_steps)",
        "begin_line": 831,
        "end_line": 1045,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.0006693440428380187,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0006693440428380187,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.engine.training.Model.evaluate#1047",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)",
        "snippet": "    def evaluate(self, x=None, y=None,\n                 batch_size=None,\n                 verbose=1,\n                 sample_weight=None,\n                 steps=None):\n        \"\"\"Returns the loss value & metrics values for the model in test mode.\n\n        Computation is done in batches.\n\n        # Arguments\n            x: Numpy array of test data (if the model has a single input),\n                or list of Numpy arrays (if the model has multiple inputs).\n                If input layers in the model are named, you can also pass a\n                dictionary mapping input names to Numpy arrays.\n                `x` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            y: Numpy array of target (label) data\n                (if the model has a single output),\n                or list of Numpy arrays (if the model has multiple outputs).\n                If output layers in the model are named, you can also pass a\n                dictionary mapping output names to Numpy arrays.\n                `y` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            batch_size: Integer or `None`.\n                Number of samples per evaluation step.\n                If unspecified, `batch_size` will default to 32.\n            verbose: 0 or 1. Verbosity mode.\n                0 = silent, 1 = progress bar.\n            sample_weight: Optional Numpy array of weights for\n                the test samples, used for weighting the loss function.\n                You can either pass a flat (1D)\n                Numpy array with the same length as the input samples\n                (1:1 mapping between weights and samples),\n                or in the case of temporal data,\n                you can pass a 2D array with shape\n                `(samples, sequence_length)`,\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                `sample_weight_mode=\"temporal\"` in `compile()`.\n            steps: Integer or `None`.\n                Total number of steps (batches of samples)\n                before declaring the evaluation round finished.\n                Ignored with the default value of `None`.\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics)\n            or list of scalars (if the model has multiple outputs\n            and/or metrics). The attribute `model.metrics_names` will give you\n            the display labels for the scalar outputs.\n        \"\"\"\n        # Backwards compatibility.\n        if batch_size is None and steps is None:\n            batch_size = 32\n        if x is None and y is None and steps is None:\n            raise ValueError('If evaluating from data tensors, '\n                             'you should specify the `steps` '\n                             'argument.')\n        # Validate user data.\n        x, y, sample_weights = self._standardize_user_data(\n            x, y,\n            sample_weight=sample_weight,\n            batch_size=batch_size)\n        # Prepare inputs, delegate logic to `test_loop`.\n        if self._uses_dynamic_learning_phase():\n            ins = x + y + sample_weights + [0.]\n        else:\n            ins = x + y + sample_weights\n        self._make_test_function()\n        f = self.test_function\n        return training_arrays.test_loop(self, f, ins,\n                                         batch_size=batch_size,\n                                         verbose=verbose,\n                                         steps=steps)",
        "begin_line": 1047,
        "end_line": 1119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045998160073597056,
            "pseudo_dstar_susp": 0.00038022813688212925,
            "pseudo_tarantula_susp": 0.0010822510822510823,
            "pseudo_op2_susp": 0.00038022813688212925,
            "pseudo_barinel_susp": 0.0010822510822510823
        }
    },
    {
        "name": "keras.engine.training.Model.predict#1121",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.predict(self, x, batch_size=None, verbose=0, steps=None)",
        "snippet": "    def predict(self, x,\n                batch_size=None,\n                verbose=0,\n                steps=None):\n        \"\"\"Generates output predictions for the input samples.\n\n        Computation is done in batches.\n\n        # Arguments\n            x: The input data, as a Numpy array\n                (or list of Numpy arrays if the model has multiple inputs).\n            batch_size: Integer. If unspecified, it will default to 32.\n            verbose: Verbosity mode, 0 or 1.\n            steps: Total number of steps (batches of samples)\n                before declaring the prediction round finished.\n                Ignored with the default value of `None`.\n\n        # Returns\n            Numpy array(s) of predictions.\n\n        # Raises\n            ValueError: In case of mismatch between the provided\n                input data and the model's expectations,\n                or in case a stateful model receives a number of samples\n                that is not a multiple of the batch size.\n        \"\"\"\n        # Backwards compatibility.\n        if batch_size is None and steps is None:\n            batch_size = 32\n        if x is None and steps is None:\n            raise ValueError('If predicting from data tensors, '\n                             'you should specify the `steps` '\n                             'argument.')\n        # Validate user data.\n        x, _, _ = self._standardize_user_data(x)\n        if self.stateful:\n            if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0:\n                raise ValueError('In a stateful network, '\n                                 'you should only pass inputs with '\n                                 'a number of samples that can be '\n                                 'divided by the batch size. Found: ' +\n                                 str(x[0].shape[0]) + ' samples. '\n                                 'Batch size: ' + str(batch_size) + '.')\n\n        # Prepare inputs, delegate logic to `predict_loop`.\n        if self._uses_dynamic_learning_phase():\n            ins = x + [0.]\n        else:\n            ins = x\n        self._make_predict_function()\n        f = self.predict_function\n        return training_arrays.predict_loop(self, f, ins,\n                                            batch_size=batch_size,\n                                            verbose=verbose,\n                                            steps=steps)",
        "begin_line": 1121,
        "end_line": 1175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026469031233456857,
            "pseudo_dstar_susp": 0.00026469031233456857,
            "pseudo_tarantula_susp": 0.00026490066225165563,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00026490066225165563
        }
    },
    {
        "name": "keras.engine.training.Model.train_on_batch#1177",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.train_on_batch(self, x, y, sample_weight=None, class_weight=None)",
        "snippet": "    def train_on_batch(self, x, y,\n                       sample_weight=None,\n                       class_weight=None):\n        \"\"\"Runs a single gradient update on a single batch of data.\n\n        # Arguments\n            x: Numpy array of training data,\n                or list of Numpy arrays if the model has multiple inputs.\n                If all inputs in the model are named,\n                you can also pass a dictionary\n                mapping input names to Numpy arrays.\n            y: Numpy array of target data,\n                or list of Numpy arrays if the model has multiple outputs.\n                If all outputs in the model are named,\n                you can also pass a dictionary\n                mapping output names to Numpy arrays.\n            sample_weight: Optional array of the same length as x, containing\n                weights to apply to the model's loss for each sample.\n                In the case of temporal data, you can pass a 2D array\n                with shape (samples, sequence_length),\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                sample_weight_mode=\"temporal\" in compile().\n            class_weight: Optional dictionary mapping\n                class indices (integers) to\n                a weight (float) to apply to the model's loss for the samples\n                from this class during training.\n                This can be useful to tell the model to \"pay more attention\" to\n                samples from an under-represented class.\n\n        # Returns\n            Scalar training loss\n            (if the model has a single output and no metrics)\n            or list of scalars (if the model has multiple outputs\n            and/or metrics). The attribute `model.metrics_names` will give you\n            the display labels for the scalar outputs.\n        \"\"\"\n        x, y, sample_weights = self._standardize_user_data(\n            x, y,\n            sample_weight=sample_weight,\n            class_weight=class_weight)\n        if self._uses_dynamic_learning_phase():\n            ins = x + y + sample_weights + [1.]\n        else:\n            ins = x + y + sample_weights\n        self._make_train_function()\n        outputs = self.train_function(ins)\n        return unpack_singleton(outputs)",
        "begin_line": 1177,
        "end_line": 1224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003757985719654265,
            "pseudo_dstar_susp": 0.0006157635467980296,
            "pseudo_tarantula_susp": 0.0004553734061930783,
            "pseudo_op2_susp": 0.0006157635467980296,
            "pseudo_barinel_susp": 0.0004553734061930783
        }
    },
    {
        "name": "keras.engine.training.Model.test_on_batch#1226",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.test_on_batch(self, x, y, sample_weight=None)",
        "snippet": "    def test_on_batch(self, x, y, sample_weight=None):\n        \"\"\"Test the model on a single batch of samples.\n\n        # Arguments\n            x: Numpy array of test data,\n                or list of Numpy arrays if the model has multiple inputs.\n                If all inputs in the model are named,\n                you can also pass a dictionary\n                mapping input names to Numpy arrays.\n            y: Numpy array of target data,\n                or list of Numpy arrays if the model has multiple outputs.\n                If all outputs in the model are named,\n                you can also pass a dictionary\n                mapping output names to Numpy arrays.\n            sample_weight: Optional array of the same length as x, containing\n                weights to apply to the model's loss for each sample.\n                In the case of temporal data, you can pass a 2D array\n                with shape (samples, sequence_length),\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                sample_weight_mode=\"temporal\" in compile().\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics)\n            or list of scalars (if the model has multiple outputs\n            and/or metrics). The attribute `model.metrics_names` will give you\n            the display labels for the scalar outputs.\n        \"\"\"\n        x, y, sample_weights = self._standardize_user_data(\n            x, y,\n            sample_weight=sample_weight)\n        if self._uses_dynamic_learning_phase():\n            ins = x + y + sample_weights + [0.]\n        else:\n            ins = x + y + sample_weights\n        self._make_test_function()\n        outputs = self.test_function(ins)\n        return unpack_singleton(outputs)",
        "begin_line": 1226,
        "end_line": 1263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043687199650502403,
            "pseudo_dstar_susp": 0.00036845983787767134,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.00036845983787767134,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.engine.training.Model.predict_on_batch#1265",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.predict_on_batch(self, x)",
        "snippet": "    def predict_on_batch(self, x):\n        \"\"\"Returns predictions for a single batch of samples.\n\n        # Arguments\n            x: Input samples, as a Numpy array.\n\n        # Returns\n            Numpy array(s) of predictions.\n        \"\"\"\n        x, _, _ = self._standardize_user_data(x)\n        if self._uses_dynamic_learning_phase():\n            ins = x + [0.]\n        else:\n            ins = x\n        self._make_predict_function()\n        outputs = self.predict_function(ins)\n        return unpack_singleton(outputs)",
        "begin_line": 1265,
        "end_line": 1281,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0004139072847682119,
            "pseudo_tarantula_susp": 0.0015600624024961,
            "pseudo_op2_susp": 0.0004139072847682119,
            "pseudo_barinel_susp": 0.0015503875968992248
        }
    },
    {
        "name": "keras.engine.training.Model.fit_generator#1284",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)",
        "snippet": "    def fit_generator(self, generator,\n                      steps_per_epoch=None,\n                      epochs=1,\n                      verbose=1,\n                      callbacks=None,\n                      validation_data=None,\n                      validation_steps=None,\n                      class_weight=None,\n                      max_queue_size=10,\n                      workers=1,\n                      use_multiprocessing=False,\n                      shuffle=True,\n                      initial_epoch=0):\n        \"\"\"Trains the model on data generated batch-by-batch by a Python generator (or an instance of `Sequence`).\n\n        The generator is run in parallel to the model, for efficiency.\n        For instance, this allows you to do real-time data augmentation\n        on images on CPU in parallel to training your model on GPU.\n\n        The use of `keras.utils.Sequence` guarantees the ordering\n        and guarantees the single use of every input per epoch when\n        using `use_multiprocessing=True`.\n\n        # Arguments\n            generator: A generator or an instance of `Sequence`\n                (`keras.utils.Sequence`) object in order to avoid\n                duplicate data when using multiprocessing.\n                The output of the generator must be either\n                - a tuple `(inputs, targets)`\n                - a tuple `(inputs, targets, sample_weights)`.\n                This tuple (a single output of the generator) makes a single\n                batch. Therefore, all arrays in this tuple must have the same\n                length (equal to the size of this batch). Different batches may\n                have different sizes. For example, the last batch of the epoch\n                is commonly smaller than the others, if the size of the dataset\n                is not divisible by the batch size.\n                The generator is expected to loop over its data\n                indefinitely. An epoch finishes when `steps_per_epoch`\n                batches have been seen by the model.\n            steps_per_epoch: Integer.\n                Total number of steps (batches of samples)\n                to yield from `generator` before declaring one epoch\n                finished and starting the next epoch. It should typically\n                be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n            epochs: Integer. Number of epochs to train the model.\n                An epoch is an iteration over the entire data provided,\n                as defined by `steps_per_epoch`.\n                Note that in conjunction with `initial_epoch`,\n                `epochs` is to be understood as \"final epoch\".\n                The model is not trained for a number of iterations\n                given by `epochs`, but merely until the epoch\n                of index `epochs` is reached.\n            verbose: Integer. 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = one line per epoch.\n            callbacks: List of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n                See [callbacks](/callbacks).\n            validation_data: This can be either\n                - a generator or a `Sequence` object for the validation data\n                - tuple `(x_val, y_val)`\n                - tuple `(x_val, y_val, val_sample_weights)`\n                on which to evaluate\n                the loss and any model metrics at the end of each epoch.\n                The model will not be trained on this data.\n            validation_steps: Only relevant if `validation_data`\n                is a generator. Total number of steps (batches of samples)\n                to yield from `validation_data` generator before stopping\n                at the end of every epoch. It should typically\n                be equal to the number of samples of your\n                validation dataset divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n            class_weight: Optional dictionary mapping class indices (integers)\n                to a weight (float) value, used for weighting the loss function\n                (during training only). This can be useful to tell the model to\n                \"pay more attention\" to samples\n                from an under-represented class.\n            max_queue_size: Integer. Maximum size for the generator queue.\n                If unspecified, `max_queue_size` will default to 10.\n            workers: Integer. Maximum number of processes to spin up\n                when using process-based threading.\n                If unspecified, `workers` will default to 1. If 0, will\n                execute the generator on the main thread.\n            use_multiprocessing: Boolean.\n                If `True`, use process-based threading.\n                If unspecified, `use_multiprocessing` will default to `False`.\n                Note that because this implementation\n                relies on multiprocessing,\n                you should not pass non-picklable arguments to the generator\n                as they can't be passed easily to children processes.\n            shuffle: Boolean. Whether to shuffle the order of the batches at\n                the beginning of each epoch. Only used with instances\n                of `Sequence` (`keras.utils.Sequence`).\n                Has no effect when `steps_per_epoch` is not `None`.\n            initial_epoch: Integer.\n                Epoch at which to start training\n                (useful for resuming a previous training run).\n\n        # Returns\n            A `History` object. Its `History.history` attribute is\n            a record of training loss values and metrics values\n            at successive epochs, as well as validation loss values\n            and validation metrics values (if applicable).\n\n        # Raises\n            ValueError: In case the generator yields data in an invalid format.\n\n        # Example\n\n        ```python\n        def generate_arrays_from_file(path):\n            while True:\n                with open(path) as f:\n                    for line in f:\n                        # create numpy arrays of input data\n                        # and labels, from each line in the file\n                        x1, x2, y = process_line(line)\n                        yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n\n        model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                            steps_per_epoch=10000, epochs=10)\n        ```\n        \"\"\"\n        return training_generator.fit_generator(\n            self, generator,\n            steps_per_epoch=steps_per_epoch,\n            epochs=epochs,\n            verbose=verbose,\n            callbacks=callbacks,\n            validation_data=validation_data,\n            validation_steps=validation_steps,\n            class_weight=class_weight,\n            max_queue_size=max_queue_size,\n            workers=workers,\n            use_multiprocessing=use_multiprocessing,\n            shuffle=shuffle,\n            initial_epoch=initial_epoch)",
        "begin_line": 1284,
        "end_line": 1423,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010357327809425168,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.training.Model.evaluate_generator#1426",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)",
        "snippet": "    def evaluate_generator(self, generator,\n                           steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,\n                           verbose=0):\n        \"\"\"Evaluates the model on a data generator.\n\n        The generator should return the same kind of data\n        as accepted by `test_on_batch`.\n\n        # Arguments\n            generator: Generator yielding tuples (inputs, targets)\n                or (inputs, targets, sample_weights)\n                or an instance of Sequence (keras.utils.Sequence)\n                object in order to avoid duplicate data\n                when using multiprocessing.\n            steps: Total number of steps (batches of samples)\n                to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n            max_queue_size: maximum size for the generator queue\n            workers: Integer. Maximum number of processes to spin up\n                when using process based threading.\n                If unspecified, `workers` will default to 1. If 0, will\n                execute the generator on the main thread.\n            use_multiprocessing: if True, use process based threading.\n                Note that because\n                this implementation relies on multiprocessing,\n                you should not pass\n                non picklable arguments to the generator\n                as they can't be passed\n                easily to children processes.\n            verbose: verbosity mode, 0 or 1.\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics)\n            or list of scalars (if the model has multiple outputs\n            and/or metrics). The attribute `model.metrics_names` will give you\n            the display labels for the scalar outputs.\n\n        # Raises\n            ValueError: In case the generator yields\n                data in an invalid format.\n        \"\"\"\n        return training_generator.evaluate_generator(\n            self, generator,\n            steps=steps,\n            max_queue_size=max_queue_size,\n            workers=workers,\n            use_multiprocessing=use_multiprocessing,\n            verbose=verbose)",
        "begin_line": 1426,
        "end_line": 1477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005202913631633715,
            "pseudo_dstar_susp": 0.00040064102564102563,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00040064102564102563,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "keras.engine.training.Model.predict_generator#1480",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)",
        "snippet": "    def predict_generator(self, generator,\n                          steps=None,\n                          max_queue_size=10,\n                          workers=1,\n                          use_multiprocessing=False,\n                          verbose=0):\n        \"\"\"Generates predictions for the input samples from a data generator.\n\n        The generator should return the same kind of data as accepted by\n        `predict_on_batch`.\n\n        # Arguments\n            generator: Generator yielding batches of input samples\n                or an instance of Sequence (keras.utils.Sequence)\n                object in order to avoid duplicate data\n                when using multiprocessing.\n            steps: Total number of steps (batches of samples)\n                to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n            max_queue_size: Maximum size for the generator queue.\n            workers: Integer. Maximum number of processes to spin up\n                when using process based threading.\n                If unspecified, `workers` will default to 1. If 0, will\n                execute the generator on the main thread.\n            use_multiprocessing: If `True`, use process based threading.\n                Note that because\n                this implementation relies on multiprocessing,\n                you should not pass\n                non picklable arguments to the generator\n                as they can't be passed\n                easily to children processes.\n            verbose: verbosity mode, 0 or 1.\n\n        # Returns\n            Numpy array(s) of predictions.\n\n        # Raises\n            ValueError: In case the generator yields\n                data in an invalid format.\n        \"\"\"\n        return training_generator.predict_generator(\n            self, generator,\n            steps=steps,\n            max_queue_size=max_queue_size,\n            workers=workers,\n            use_multiprocessing=use_multiprocessing,\n            verbose=verbose)",
        "begin_line": 1480,
        "end_line": 1527,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.utils.np_utils.to_categorical#9",
        "src_path": "keras/utils/np_utils.py",
        "class_name": "keras.utils.np_utils",
        "signature": "keras.utils.np_utils.to_categorical(y, num_classes=None, dtype='float32')",
        "snippet": "def to_categorical(y, num_classes=None, dtype='float32'):\n    \"\"\"Converts a class vector (integers) to binary class matrix.\n\n    E.g. for use with categorical_crossentropy.\n\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        num_classes: total number of classes.\n        dtype: The data type expected by the input, as a string\n            (`float32`, `float64`, `int32`...)\n\n    # Returns\n        A binary matrix representation of the input. The classes axis\n        is placed last.\n    \"\"\"\n    y = np.array(y, dtype='int')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes), dtype=dtype)\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical",
        "begin_line": 9,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00145985401459854,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.0010626992561105207,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.0010626992561105207
        }
    },
    {
        "name": "keras.layers.__init__.deserialize#37",
        "src_path": "keras/layers/__init__.py",
        "class_name": "keras.layers.__init__",
        "signature": "keras.layers.__init__.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    \"\"\"Instantiate a layer from a config dictionary.\n\n    # Arguments\n        config: dict of the form {'class_name': str, 'config': dict}\n        custom_objects: dict mapping class names (or function names)\n            of custom (non-Keras) objects to class/functions\n\n    # Returns\n        Layer instance (may be Model, Sequential, Layer...)\n    \"\"\"\n    from .. import models\n    globs = globals()  # All layers.\n    globs['Model'] = models.Model\n    globs['Sequential'] = models.Sequential\n    return deserialize_keras_object(config,\n                                    module_objects=globs,\n                                    custom_objects=custom_objects,\n                                    printable_module_name='layer')",
        "begin_line": 37,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031084861672365556,
            "pseudo_dstar_susp": 0.000544069640914037,
            "pseudo_tarantula_susp": 0.0002880184331797235,
            "pseudo_op2_susp": 0.000544069640914037,
            "pseudo_barinel_susp": 0.0002880184331797235
        }
    },
    {
        "name": "keras.applications.xception.Xception#10",
        "src_path": "keras/applications/xception.py",
        "class_name": "keras.applications.xception",
        "signature": "keras.applications.xception.Xception(*args, **kwargs)",
        "snippet": "def Xception(*args, **kwargs):\n    return xception.Xception(*args, **kwargs)",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.optimizers.clip_norm#20",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers",
        "signature": "keras.optimizers.clip_norm(g, c, n)",
        "snippet": "def clip_norm(g, c, n):\n    \"\"\"Clip the gradient `g` if the L2 norm `n` exceeds `c`.\n\n    # Arguments\n        g: Tensor, the gradient tensor\n        c: float >= 0. Gradients will be clipped\n            when their L2 norm exceeds this value.\n        n: Tensor, actual norm of `g`.\n\n    # Returns\n        Tensor, the gradient clipped if required.\n    \"\"\"\n    if c <= 0:  # if clipnorm == 0 no need to add ops to the graph\n        return g\n\n    # tf require using a special op to multiply IndexedSliced by scalar\n    if K.backend() == 'tensorflow':\n        condition = n >= c\n        then_expression = tf.scalar_mul(c / n, g)\n        else_expression = g\n\n        # saving the shape to avoid converting sparse tensor to dense\n        if isinstance(then_expression, tf.Tensor):\n            g_shape = copy.copy(then_expression.get_shape())\n        elif isinstance(then_expression, tf.IndexedSlices):\n            g_shape = copy.copy(then_expression.dense_shape)\n        if condition.dtype != tf.bool:\n            condition = tf.cast(condition, 'bool')\n        g = tf.cond(condition,\n                    lambda: then_expression,\n                    lambda: else_expression)\n        if isinstance(then_expression, tf.Tensor):\n            g.set_shape(g_shape)\n        elif isinstance(then_expression, tf.IndexedSlices):\n            g._dense_shape = g_shape\n    else:\n        g = K.switch(K.greater_equal(n, c), g * c / n, g)\n    return g",
        "begin_line": 20,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Optimizer.__init__#74",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        allowed_kwargs = {'clipnorm', 'clipvalue'}\n        for k in kwargs:\n            if k not in allowed_kwargs:\n                raise TypeError('Unexpected keyword argument '\n                                'passed to optimizer: ' + str(k))\n        self.__dict__.update(kwargs)\n        self.updates = []\n        self.weights = []",
        "begin_line": 74,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005737234652897303,
            "pseudo_dstar_susp": 0.0010460251046025104,
            "pseudo_tarantula_susp": 0.0005094243504839531,
            "pseudo_op2_susp": 0.0010460251046025104,
            "pseudo_barinel_susp": 0.0005094243504839531
        }
    },
    {
        "name": "keras.optimizers.Optimizer.get_gradients#88",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.get_gradients(self, loss, params)",
        "snippet": "    def get_gradients(self, loss, params):\n        grads = K.gradients(loss, params)\n        if None in grads:\n            raise ValueError('An operation has `None` for gradient. '\n                             'Please make sure that all of your ops have a '\n                             'gradient defined (i.e. are differentiable). '\n                             'Common ops without gradient: '\n                             'K.argmax, K.round, K.eval.')\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n        return grads",
        "begin_line": 88,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003354579000335458,
            "pseudo_dstar_susp": 0.0005844535359438924,
            "pseudo_tarantula_susp": 0.0003189792663476874,
            "pseudo_op2_susp": 0.0005844535359438924,
            "pseudo_barinel_susp": 0.0003189792663476874
        }
    },
    {
        "name": "keras.optimizers.Optimizer.set_weights#103",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.set_weights(self, weights)",
        "snippet": "    def set_weights(self, weights):\n        \"\"\"Sets the weights of the optimizer, from Numpy arrays.\n\n        Should only be called after computing the gradients\n        (otherwise the optimizer has no weights).\n\n        # Arguments\n            weights: a list of Numpy arrays. The number\n                of arrays and their shape must match\n                number of the dimensions of the weights\n                of the optimizer (i.e. it should match the\n                output of `get_weights`).\n\n        # Raises\n            ValueError: in case of incompatible weight shapes.\n        \"\"\"\n        params = self.weights\n        if len(params) != len(weights):\n            raise ValueError('Length of the specified weight list (' +\n                             str(len(weights)) +\n                             ') does not match the number of weights ' +\n                             'of the optimizer (' + str(len(params)) + ')')\n        weight_value_tuples = []\n        param_values = K.batch_get_value(params)\n        for pv, p, w in zip(param_values, params, weights):\n            if pv.shape != w.shape:\n                raise ValueError('Optimizer weight shape ' +\n                                 str(pv.shape) +\n                                 ' not compatible with '\n                                 'provided weight shape ' + str(w.shape))\n            weight_value_tuples.append((p, w))\n        K.batch_set_value(weight_value_tuples)",
        "begin_line": 103,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000100999899000101,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Optimizer.get_config#144",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {}\n        if hasattr(self, 'clipnorm'):\n            config['clipnorm'] = self.clipnorm\n        if hasattr(self, 'clipvalue'):\n            config['clipvalue'] = self.clipvalue\n        return config",
        "begin_line": 144,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Optimizer.from_config#153",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        return cls(**config)",
        "begin_line": 153,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006056935190793458,
            "pseudo_dstar_susp": 0.0011337868480725624,
            "pseudo_tarantula_susp": 0.0005350454788657035,
            "pseudo_op2_susp": 0.0011337868480725624,
            "pseudo_barinel_susp": 0.0005350454788657035
        }
    },
    {
        "name": "keras.optimizers.SGD.__init__#171",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.SGD",
        "signature": "keras.optimizers.SGD.__init__(self, lr=0.01, momentum=0.0, decay=0.0, nesterov=False, **kwargs)",
        "snippet": "    def __init__(self, lr=0.01, momentum=0., decay=0.,\n                 nesterov=False, **kwargs):\n        super(SGD, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.momentum = K.variable(momentum, name='momentum')\n            self.decay = K.variable(decay, name='decay')\n        self.initial_decay = decay\n        self.nesterov = nesterov",
        "begin_line": 171,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003992015968063872,
            "pseudo_dstar_susp": 0.0006199628022318661,
            "pseudo_tarantula_susp": 0.0005521811154058532,
            "pseudo_op2_susp": 0.0006199628022318661,
            "pseudo_barinel_susp": 0.0005518763796909492
        }
    },
    {
        "name": "keras.optimizers.SGD.get_updates#183",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.SGD",
        "signature": "keras.optimizers.SGD.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n        # momentum\n        shapes = [K.int_shape(p) for p in params]\n        moments = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + moments\n        for p, g, m in zip(params, grads, moments):\n            v = self.momentum * m - lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 183,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00036643459142543056,
            "pseudo_dstar_susp": 0.0006020469596628537,
            "pseudo_tarantula_susp": 0.00038654812524159255,
            "pseudo_op2_susp": 0.0006020469596628537,
            "pseudo_barinel_susp": 0.00038654812524159255
        }
    },
    {
        "name": "keras.optimizers.SGD.get_config#211",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.SGD",
        "signature": "keras.optimizers.SGD.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'momentum': float(K.get_value(self.momentum)),\n                  'decay': float(K.get_value(self.decay)),\n                  'nesterov': self.nesterov}\n        base_config = super(SGD, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 211,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.RMSprop.__init__#240",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.RMSprop",
        "signature": "keras.optimizers.RMSprop.__init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.0, **kwargs)",
        "snippet": "    def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n                 **kwargs):\n        super(RMSprop, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.lr = K.variable(lr, name='lr')\n            self.rho = K.variable(rho, name='rho')\n            self.decay = K.variable(decay, name='decay')\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay",
        "begin_line": 240,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00046992481203007516,
            "pseudo_dstar_susp": 0.0008103727714748784,
            "pseudo_tarantula_susp": 0.0005688282138794084,
            "pseudo_op2_susp": 0.0008103727714748784,
            "pseudo_barinel_susp": 0.0005688282138794084
        }
    },
    {
        "name": "keras.optimizers.RMSprop.get_updates#254",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.RMSprop",
        "signature": "keras.optimizers.RMSprop.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        self.weights = accumulators\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        for p, g, a in zip(params, grads, accumulators):\n            # update accumulator\n            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n            self.updates.append(K.update(a, new_a))\n            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 254,
        "end_line": 276,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00032206119162640903,
            "pseudo_dstar_susp": 0.0005458515283842794,
            "pseudo_tarantula_susp": 0.0003202049311559398,
            "pseudo_op2_susp": 0.0005458515283842794,
            "pseudo_barinel_susp": 0.0003202049311559398
        }
    },
    {
        "name": "keras.optimizers.RMSprop.get_config#278",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.RMSprop",
        "signature": "keras.optimizers.RMSprop.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'rho': float(K.get_value(self.rho)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(RMSprop, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 278,
        "end_line": 284,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adagrad.__init__#307",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adagrad",
        "signature": "keras.optimizers.Adagrad.__init__(self, lr=0.01, epsilon=None, decay=0.0, **kwargs)",
        "snippet": "    def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n        super(Adagrad, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.lr = K.variable(lr, name='lr')\n            self.decay = K.variable(decay, name='decay')\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay",
        "begin_line": 307,
        "end_line": 316,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adagrad.get_updates#319",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adagrad",
        "signature": "keras.optimizers.Adagrad.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.int_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        for p, g, a in zip(params, grads, accumulators):\n            new_a = a + K.square(g)  # update accumulator\n            self.updates.append(K.update(a, new_a))\n            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 319,
        "end_line": 341,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adagrad.get_config#343",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adagrad",
        "signature": "keras.optimizers.Adagrad.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adagrad, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 343,
        "end_line": 348,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adadelta.__init__#377",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adadelta",
        "signature": "keras.optimizers.Adadelta.__init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.0, **kwargs)",
        "snippet": "    def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n                 **kwargs):\n        super(Adadelta, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.lr = K.variable(lr, name='lr')\n            self.decay = K.variable(decay, name='decay')\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.rho = rho\n        self.epsilon = epsilon\n        self.initial_decay = decay",
        "begin_line": 377,
        "end_line": 388,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adadelta.get_updates#391",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adadelta",
        "signature": "keras.optimizers.Adadelta.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.int_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        delta_accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators + delta_accumulators\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n            # update accumulator\n            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n            self.updates.append(K.update(a, new_a))\n\n            # use the new accumulator and the *old* delta_accumulator\n            update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n            new_p = p - lr * update\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n\n            # update delta_accumulator\n            new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)\n            self.updates.append(K.update(d_a, new_d_a))\n        return self.updates",
        "begin_line": 391,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adadelta.get_config#424",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adadelta",
        "signature": "keras.optimizers.Adadelta.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'rho': self.rho,\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adadelta, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 424,
        "end_line": 430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adam.__init__#453",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adam",
        "signature": "keras.optimizers.Adam.__init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, **kwargs)",
        "snippet": "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n        super(Adam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.amsgrad = amsgrad",
        "begin_line": 453,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.30925339787749e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adam.get_updates#469",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adam",
        "signature": "keras.optimizers.Adam.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 469,
        "end_line": 509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adam.get_config#511",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adam",
        "signature": "keras.optimizers.Adam.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon,\n                  'amsgrad': self.amsgrad}\n        base_config = super(Adam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 511,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adamax.__init__#538",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adamax",
        "signature": "keras.optimizers.Adamax.__init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, **kwargs)",
        "snippet": "    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., **kwargs):\n        super(Adamax, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay",
        "begin_line": 538,
        "end_line": 550,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adamax.get_updates#553",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adamax",
        "signature": "keras.optimizers.Adamax.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr / (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.int_shape(p) for p in params]\n        # zero init of 1st moment\n        ms = [K.zeros(shape) for shape in shapes]\n        # zero init of exponentially weighted infinity norm\n        us = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + ms + us\n\n        for p, g, m, u in zip(params, grads, ms, us):\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            u_t = K.maximum(self.beta_2 * u, K.abs(g))\n            p_t = p - lr_t * m_t / (u_t + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(u, u_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 553,
        "end_line": 587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Adamax.get_config#589",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Adamax",
        "signature": "keras.optimizers.Adamax.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adamax, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 589,
        "end_line": 596,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Nadam.__init__#619",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Nadam",
        "signature": "keras.optimizers.Nadam.__init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004, **kwargs)",
        "snippet": "    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, schedule_decay=0.004, **kwargs):\n        super(Nadam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.m_schedule = K.variable(1., name='m_schedule')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.schedule_decay = schedule_decay",
        "begin_line": 619,
        "end_line": 631,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Nadam.get_updates#634",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Nadam",
        "signature": "keras.optimizers.Nadam.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_t = self.beta_1 * (\n            1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n        momentum_cache_t_1 = self.beta_1 * (\n            1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n        m_schedule_new = self.m_schedule * momentum_cache_t\n        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n        self.updates.append((self.m_schedule, m_schedule_new))\n\n        shapes = [K.int_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            # the following equations given in [1]\n            g_prime = g / (1. - m_schedule_new)\n            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n            m_t_prime = m_t / (1. - m_schedule_next)\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 634,
        "end_line": 675,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.Nadam.get_config#677",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Nadam",
        "signature": "keras.optimizers.Nadam.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'epsilon': self.epsilon,\n                  'schedule_decay': self.schedule_decay}\n        base_config = super(Nadam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 677,
        "end_line": 684,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.TFOptimizer.__init__#691",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.TFOptimizer",
        "signature": "keras.optimizers.TFOptimizer.__init__(self, optimizer)",
        "snippet": "    def __init__(self, optimizer):\n        self.optimizer = optimizer\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')",
        "begin_line": 691,
        "end_line": 694,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.TFOptimizer.get_updates#697",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.TFOptimizer",
        "signature": "keras.optimizers.TFOptimizer.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(\n            grads, global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates",
        "begin_line": 697,
        "end_line": 703,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.TFOptimizer.weights#706",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.TFOptimizer",
        "signature": "keras.optimizers.TFOptimizer.weights(self)",
        "snippet": "    def weights(self):\n        raise NotImplementedError",
        "begin_line": 706,
        "end_line": 707,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.TFOptimizer.get_config#709",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.TFOptimizer",
        "signature": "keras.optimizers.TFOptimizer.get_config(self)",
        "snippet": "    def get_config(self):\n        raise NotImplementedError",
        "begin_line": 709,
        "end_line": 710,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.TFOptimizer.from_config#712",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.TFOptimizer",
        "signature": "keras.optimizers.TFOptimizer.from_config(self, config)",
        "snippet": "    def from_config(self, config):\n        raise NotImplementedError",
        "begin_line": 712,
        "end_line": 713,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.serialize#727",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers",
        "signature": "keras.optimizers.serialize(optimizer)",
        "snippet": "def serialize(optimizer):\n    return serialize_keras_object(optimizer)",
        "begin_line": 727,
        "end_line": 728,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.optimizers.deserialize#731",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers",
        "signature": "keras.optimizers.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    \"\"\"Inverse of the `serialize` function.\n\n    # Arguments\n        config: Optimizer configuration dictionary.\n        custom_objects: Optional dictionary mapping\n            names (strings) to custom objects\n            (classes and functions)\n            to be considered during deserialization.\n\n    # Returns\n        A Keras Optimizer instance.\n    \"\"\"\n    all_classes = {\n        'sgd': SGD,\n        'rmsprop': RMSprop,\n        'adagrad': Adagrad,\n        'adadelta': Adadelta,\n        'adam': Adam,\n        'adamax': Adamax,\n        'nadam': Nadam,\n        'tfoptimizer': TFOptimizer,\n    }\n    # Make deserialization case-insensitive for built-in optimizers.\n    if config['class_name'].lower() in all_classes:\n        config['class_name'] = config['class_name'].lower()\n    return deserialize_keras_object(config,\n                                    module_objects=all_classes,\n                                    custom_objects=custom_objects,\n                                    printable_module_name='optimizer')",
        "begin_line": 731,
        "end_line": 760,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006056935190793458,
            "pseudo_dstar_susp": 0.0011337868480725624,
            "pseudo_tarantula_susp": 0.0005350454788657035,
            "pseudo_op2_susp": 0.0011337868480725624,
            "pseudo_barinel_susp": 0.0005350454788657035
        }
    },
    {
        "name": "keras.optimizers.get#763",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers",
        "signature": "keras.optimizers.get(identifier)",
        "snippet": "def get(identifier):\n    \"\"\"Retrieves a Keras Optimizer instance.\n\n    # Arguments\n        identifier: Optimizer identifier, one of\n            - String: name of an optimizer\n            - Dictionary: configuration dictionary.\n            - Keras Optimizer instance (it will be returned unchanged).\n            - TensorFlow Optimizer instance\n                (it will be wrapped as a Keras Optimizer).\n\n    # Returns\n        A Keras Optimizer instance.\n\n    # Raises\n        ValueError: If `identifier` cannot be interpreted.\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        # Wrap TF optimizer instances\n        if isinstance(identifier, tf.train.Optimizer):\n            return TFOptimizer(identifier)\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    if isinstance(identifier, Optimizer):\n        return identifier\n    else:\n        raise ValueError('Could not interpret optimizer identifier: ' +\n                         str(identifier))",
        "begin_line": 763,
        "end_line": 793,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006752194463200541,
            "pseudo_dstar_susp": 0.0011467889908256881,
            "pseudo_tarantula_susp": 0.0005636978579481398,
            "pseudo_op2_susp": 0.0011467889908256881,
            "pseudo_barinel_susp": 0.0005636978579481398
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.BaseWrapper.__init__#59",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.BaseWrapper",
        "signature": "keras.wrappers.scikit_learn.BaseWrapper.__init__(self, build_fn=None, **sk_params)",
        "snippet": "    def __init__(self, build_fn=None, **sk_params):\n        self.build_fn = build_fn\n        self.sk_params = sk_params\n        self.check_params(sk_params)",
        "begin_line": 59,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.BaseWrapper.check_params#64",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.BaseWrapper",
        "signature": "keras.wrappers.scikit_learn.BaseWrapper.check_params(self, params)",
        "snippet": "    def check_params(self, params):\n        \"\"\"Checks for user typos in `params`.\n\n        # Arguments\n            params: dictionary; the parameters to be checked\n\n        # Raises\n            ValueError: if any member of `params` is not a valid argument.\n        \"\"\"\n        legal_params_fns = [Sequential.fit, Sequential.predict,\n                            Sequential.predict_classes, Sequential.evaluate]\n        if self.build_fn is None:\n            legal_params_fns.append(self.__call__)\n        elif (not isinstance(self.build_fn, types.FunctionType) and\n              not isinstance(self.build_fn, types.MethodType)):\n            legal_params_fns.append(self.build_fn.__call__)\n        else:\n            legal_params_fns.append(self.build_fn)\n\n        for params_name in params:\n            for fn in legal_params_fns:\n                if has_arg(fn, params_name):\n                    break\n            else:\n                if params_name != 'nb_epoch':\n                    raise ValueError(\n                        '{} is not a legal parameter'.format(params_name))",
        "begin_line": 64,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.BaseWrapper.fit#118",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.BaseWrapper",
        "signature": "keras.wrappers.scikit_learn.BaseWrapper.fit(self, x, y, **kwargs)",
        "snippet": "    def fit(self, x, y, **kwargs):\n        \"\"\"Constructs a new model with `build_fn` & fit the model to `(x, y)`.\n\n        # Arguments\n            x : array-like, shape `(n_samples, n_features)`\n                Training samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                True labels for `x`.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments of `Sequential.fit`\n\n        # Returns\n            history : object\n                details about the training history at each epoch.\n        \"\"\"\n        if self.build_fn is None:\n            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n        elif (not isinstance(self.build_fn, types.FunctionType) and\n              not isinstance(self.build_fn, types.MethodType)):\n            self.model = self.build_fn(\n                **self.filter_sk_params(self.build_fn.__call__))\n        else:\n            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n\n        loss_name = self.model.loss\n        if hasattr(loss_name, '__name__'):\n            loss_name = loss_name.__name__\n        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n            y = to_categorical(y)\n\n        fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n        fit_args.update(kwargs)\n\n        history = self.model.fit(x, y, **fit_args)\n\n        return history",
        "begin_line": 118,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.BaseWrapper.filter_sk_params#156",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.BaseWrapper",
        "signature": "keras.wrappers.scikit_learn.BaseWrapper.filter_sk_params(self, fn, override=None)",
        "snippet": "    def filter_sk_params(self, fn, override=None):\n        \"\"\"Filters `sk_params` and returns those in `fn`'s arguments.\n\n        # Arguments\n            fn : arbitrary function\n            override: dictionary, values to override `sk_params`\n\n        # Returns\n            res : dictionary containing variables\n                in both `sk_params` and `fn`'s arguments.\n        \"\"\"\n        override = override or {}\n        res = {}\n        for name, value in self.sk_params.items():\n            if has_arg(fn, name):\n                res.update({name: value})\n        res.update(override)\n        return res",
        "begin_line": 156,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.KerasClassifier.fit#180",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.KerasClassifier",
        "signature": "keras.wrappers.scikit_learn.KerasClassifier.fit(self, x, y, sample_weight=None, **kwargs)",
        "snippet": "    def fit(self, x, y, sample_weight=None, **kwargs):\n        \"\"\"Constructs a new model with `build_fn` & fit the model to `(x, y)`.\n\n        # Arguments\n            x : array-like, shape `(n_samples, n_features)`\n                Training samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                True labels for `x`.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments of `Sequential.fit`\n\n        # Returns\n            history : object\n                details about the training history at each epoch.\n\n        # Raises\n            ValueError: In case of invalid shape for `y` argument.\n        \"\"\"\n        y = np.array(y)\n        if len(y.shape) == 2 and y.shape[1] > 1:\n            self.classes_ = np.arange(y.shape[1])\n        elif (len(y.shape) == 2 and y.shape[1] == 1) or len(y.shape) == 1:\n            self.classes_ = np.unique(y)\n            y = np.searchsorted(self.classes_, y)\n        else:\n            raise ValueError('Invalid shape for y: ' + str(y.shape))\n        self.n_classes_ = len(self.classes_)\n        if sample_weight is not None:\n            kwargs['sample_weight'] = sample_weight\n        return super(KerasClassifier, self).fit(x, y, **kwargs)",
        "begin_line": 180,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.KerasClassifier.predict#212",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.KerasClassifier",
        "signature": "keras.wrappers.scikit_learn.KerasClassifier.predict(self, x, **kwargs)",
        "snippet": "    def predict(self, x, **kwargs):\n        \"\"\"Returns the class predictions for the given test data.\n\n        # Arguments\n            x: array-like, shape `(n_samples, n_features)`\n                Test samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments\n                of `Sequential.predict_classes`.\n\n        # Returns\n            preds: array-like, shape `(n_samples,)`\n                Class predictions.\n        \"\"\"\n        kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n\n        proba = self.model.predict(x, **kwargs)\n        if proba.shape[-1] > 1:\n            classes = proba.argmax(axis=-1)\n        else:\n            classes = (proba > 0.5).astype('int32')\n        return self.classes_[classes]",
        "begin_line": 212,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.KerasClassifier.predict_proba#236",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.KerasClassifier",
        "signature": "keras.wrappers.scikit_learn.KerasClassifier.predict_proba(self, x, **kwargs)",
        "snippet": "    def predict_proba(self, x, **kwargs):\n        \"\"\"Returns class probability estimates for the given test data.\n\n        # Arguments\n            x: array-like, shape `(n_samples, n_features)`\n                Test samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments\n                of `Sequential.predict_classes`.\n\n        # Returns\n            proba: array-like, shape `(n_samples, n_outputs)`\n                Class probability estimates.\n                In the case of binary classification,\n                to match the scikit-learn API,\n                will return an array of shape `(n_samples, 2)`\n                (instead of `(n_sample, 1)` as in Keras).\n        \"\"\"\n        kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n        probs = self.model.predict(x, **kwargs)\n\n        # check if binary classification\n        if probs.shape[1] == 1:\n            # first column is probability of class 0 and second is of class 1\n            probs = np.hstack([1 - probs, probs])\n        return probs",
        "begin_line": 236,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.KerasClassifier.score#264",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.KerasClassifier",
        "signature": "keras.wrappers.scikit_learn.KerasClassifier.score(self, x, y, **kwargs)",
        "snippet": "    def score(self, x, y, **kwargs):\n        \"\"\"Returns the mean accuracy on the given test data and labels.\n\n        # Arguments\n            x: array-like, shape `(n_samples, n_features)`\n                Test samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                True labels for `x`.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments of `Sequential.evaluate`.\n\n        # Returns\n            score: float\n                Mean accuracy of predictions on `x` wrt. `y`.\n\n        # Raises\n            ValueError: If the underlying model isn't configured to\n                compute accuracy. You should pass `metrics=[\"accuracy\"]` to\n                the `.compile()` method of the model.\n        \"\"\"\n        y = np.searchsorted(self.classes_, y)\n        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n\n        loss_name = self.model.loss\n        if hasattr(loss_name, '__name__'):\n            loss_name = loss_name.__name__\n        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n            y = to_categorical(y)\n\n        outputs = self.model.evaluate(x, y, **kwargs)\n        outputs = to_list(outputs)\n        for name, output in zip(self.model.metrics_names, outputs):\n            if name == 'acc':\n                return output\n        raise ValueError('The model is not configured to compute accuracy. '\n                         'You should pass `metrics=[\"accuracy\"]` to '\n                         'the `model.compile()` method.')",
        "begin_line": 264,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.KerasRegressor.predict#308",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.KerasRegressor",
        "signature": "keras.wrappers.scikit_learn.KerasRegressor.predict(self, x, **kwargs)",
        "snippet": "    def predict(self, x, **kwargs):\n        \"\"\"Returns predictions for the given test data.\n\n        # Arguments\n            x: array-like, shape `(n_samples, n_features)`\n                Test samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments of `Sequential.predict`.\n\n        # Returns\n            preds: array-like, shape `(n_samples,)`\n                Predictions.\n        \"\"\"\n        kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n        return np.squeeze(self.model.predict(x, **kwargs))",
        "begin_line": 308,
        "end_line": 323,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.wrappers.scikit_learn.KerasRegressor.score#325",
        "src_path": "keras/wrappers/scikit_learn.py",
        "class_name": "keras.wrappers.scikit_learn.KerasRegressor",
        "signature": "keras.wrappers.scikit_learn.KerasRegressor.score(self, x, y, **kwargs)",
        "snippet": "    def score(self, x, y, **kwargs):\n        \"\"\"Returns the mean loss on the given test data and labels.\n\n        # Arguments\n            x: array-like, shape `(n_samples, n_features)`\n                Test samples where `n_samples` is the number of samples\n                and `n_features` is the number of features.\n            y: array-like, shape `(n_samples,)`\n                True labels for `x`.\n            **kwargs: dictionary arguments\n                Legal arguments are the arguments of `Sequential.evaluate`.\n\n        # Returns\n            score: float\n                Mean accuracy of predictions on `x` wrt. `y`.\n        \"\"\"\n        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n        loss = self.model.evaluate(x, y, **kwargs)\n        if isinstance(loss, list):\n            return -loss[0]\n        return -loss",
        "begin_line": 325,
        "end_line": 345,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.generic_utils.CustomObjectScope.__init__#41",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.CustomObjectScope",
        "signature": "keras.utils.generic_utils.CustomObjectScope.__init__(self, *args)",
        "snippet": "    def __init__(self, *args):\n        self.custom_objects = args\n        self.backup = None",
        "begin_line": 41,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006618133686300463,
            "pseudo_dstar_susp": 0.0015503875968992248,
            "pseudo_tarantula_susp": 0.00035549235691432633,
            "pseudo_op2_susp": 0.0015503875968992248,
            "pseudo_barinel_susp": 0.00035549235691432633
        }
    },
    {
        "name": "keras.utils.generic_utils.CustomObjectScope.__enter__#45",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.CustomObjectScope",
        "signature": "keras.utils.generic_utils.CustomObjectScope.__enter__(self)",
        "snippet": "    def __enter__(self):\n        self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()\n        for objects in self.custom_objects:\n            _GLOBAL_CUSTOM_OBJECTS.update(objects)\n        return self",
        "begin_line": 45,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006618133686300463,
            "pseudo_dstar_susp": 0.0015503875968992248,
            "pseudo_tarantula_susp": 0.00035549235691432633,
            "pseudo_op2_susp": 0.0015503875968992248,
            "pseudo_barinel_susp": 0.00035549235691432633
        }
    },
    {
        "name": "keras.utils.generic_utils.CustomObjectScope.__exit__#51",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.CustomObjectScope",
        "signature": "keras.utils.generic_utils.CustomObjectScope.__exit__(self, *args, **kwargs)",
        "snippet": "    def __exit__(self, *args, **kwargs):\n        _GLOBAL_CUSTOM_OBJECTS.clear()\n        _GLOBAL_CUSTOM_OBJECTS.update(self.backup)",
        "begin_line": 51,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006618133686300463,
            "pseudo_dstar_susp": 0.0015503875968992248,
            "pseudo_tarantula_susp": 0.00035549235691432633,
            "pseudo_op2_susp": 0.0015503875968992248,
            "pseudo_barinel_susp": 0.00035549235691432633
        }
    },
    {
        "name": "keras.utils.generic_utils.custom_object_scope#56",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.custom_object_scope(*args)",
        "snippet": "def custom_object_scope(*args):\n    \"\"\"Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\n\n    Convenience wrapper for `CustomObjectScope`.\n    Code within a `with` statement will be able to access custom objects\n    by name. Changes to global custom objects persist\n    within the enclosing `with` statement. At end of the `with` statement,\n    global custom objects are reverted to state\n    at beginning of the `with` statement.\n\n    # Example\n\n    Consider a custom object `MyObject`\n\n    ```python\n        with custom_object_scope({'MyObject':MyObject}):\n            layer = Dense(..., kernel_regularizer='MyObject')\n            # save, load, etc. will recognize custom object by name\n    ```\n\n    # Arguments\n        *args: Variable length list of dictionaries of name,\n            class pairs to add to custom objects.\n\n    # Returns\n        Object of type `CustomObjectScope`.\n    \"\"\"\n    return CustomObjectScope(*args)",
        "begin_line": 56,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.generic_utils.serialize_keras_object#106",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.serialize_keras_object(instance)",
        "snippet": "def serialize_keras_object(instance):\n    if instance is None:\n        return None\n    if hasattr(instance, 'get_config'):\n        return {\n            'class_name': instance.__class__.__name__,\n            'config': instance.get_config()\n        }\n    if hasattr(instance, '__name__'):\n        return instance.__name__\n    else:\n        raise ValueError('Cannot serialize', instance)",
        "begin_line": 106,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000473260766682442,
            "pseudo_dstar_susp": 0.0009041591320072332,
            "pseudo_tarantula_susp": 0.000468384074941452,
            "pseudo_op2_susp": 0.0009041591320072332,
            "pseudo_barinel_susp": 0.000468384074941452
        }
    },
    {
        "name": "keras.utils.generic_utils.deserialize_keras_object#120",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.deserialize_keras_object(identifier, module_objects=None, custom_objects=None, printable_module_name='object')",
        "snippet": "def deserialize_keras_object(identifier, module_objects=None,\n                             custom_objects=None,\n                             printable_module_name='object'):\n    if isinstance(identifier, dict):\n        # In this case we are dealing with a Keras config dictionary.\n        config = identifier\n        if 'class_name' not in config or 'config' not in config:\n            raise ValueError('Improper config format: ' + str(config))\n        class_name = config['class_name']\n        if custom_objects and class_name in custom_objects:\n            cls = custom_objects[class_name]\n        elif class_name in _GLOBAL_CUSTOM_OBJECTS:\n            cls = _GLOBAL_CUSTOM_OBJECTS[class_name]\n        else:\n            module_objects = module_objects or {}\n            cls = module_objects.get(class_name)\n            if cls is None:\n                raise ValueError('Unknown ' + printable_module_name +\n                                 ': ' + class_name)\n        if hasattr(cls, 'from_config'):\n            custom_objects = custom_objects or {}\n            if has_arg(cls.from_config, 'custom_objects'):\n                return cls.from_config(\n                    config['config'],\n                    custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n                                        list(custom_objects.items())))\n            with CustomObjectScope(custom_objects):\n                return cls.from_config(config['config'])\n        else:\n            # Then `cls` may be a function returning a class.\n            # in this case by convention `config` holds\n            # the kwargs of the function.\n            custom_objects = custom_objects or {}\n            with CustomObjectScope(custom_objects):\n                return cls(**config['config'])\n    elif isinstance(identifier, six.string_types):\n        function_name = identifier\n        if custom_objects and function_name in custom_objects:\n            fn = custom_objects.get(function_name)\n        elif function_name in _GLOBAL_CUSTOM_OBJECTS:\n            fn = _GLOBAL_CUSTOM_OBJECTS[function_name]\n        else:\n            fn = module_objects.get(function_name)\n            if fn is None:\n                raise ValueError('Unknown ' + printable_module_name +\n                                 ':' + function_name)\n        return fn\n    else:\n        raise ValueError('Could not interpret serialized ' +\n                         printable_module_name + ': ' + identifier)",
        "begin_line": 120,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011123470522803114,
            "pseudo_dstar_susp": 0.005291005291005291,
            "pseudo_tarantula_susp": 0.0006626905235255136,
            "pseudo_op2_susp": 0.005291005291005291,
            "pseudo_barinel_susp": 0.0006626905235255136
        }
    },
    {
        "name": "keras.utils.generic_utils.func_dump#172",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.func_dump(func)",
        "snippet": "def func_dump(func):\n    \"\"\"Serializes a user defined function.\n\n    # Arguments\n        func: the function to serialize.\n\n    # Returns\n        A tuple `(code, defaults, closure)`.\n    \"\"\"\n    raw_code = marshal.dumps(func.__code__)\n    code = codecs.encode(raw_code, 'base64').decode('ascii')\n    defaults = func.__defaults__\n    if func.__closure__:\n        closure = tuple(c.cell_contents for c in func.__closure__)\n    else:\n        closure = None\n    return code, defaults, closure",
        "begin_line": 172,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.generic_utils.func_load#191",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.func_load(code, defaults=None, closure=None, globs=None)",
        "snippet": "def func_load(code, defaults=None, closure=None, globs=None):\n    \"\"\"Deserializes a user defined function.\n\n    # Arguments\n        code: bytecode of the function.\n        defaults: defaults of the function.\n        closure: closure of the function.\n        globs: dictionary of global objects.\n\n    # Returns\n        A function object.\n    \"\"\"\n    if isinstance(code, (tuple, list)):  # unpack previous dump\n        code, defaults, closure = code\n        if isinstance(defaults, list):\n            defaults = tuple(defaults)\n\n    def ensure_value_to_cell(value):\n        \"\"\"Ensures that a value is converted to a python cell object.\n\n        # Arguments\n            value: Any value that needs to be casted to the cell type\n\n        # Returns\n            A value wrapped as a cell object (see function \"func_load\")\n\n        \"\"\"\n        def dummy_fn():\n            value  # just access it so it gets captured in .__closure__\n\n        cell_value = dummy_fn.__closure__[0]\n        if not isinstance(value, type(cell_value)):\n            return cell_value\n        else:\n            return value\n\n    if closure is not None:\n        closure = tuple(ensure_value_to_cell(_) for _ in closure)\n    try:\n        raw_code = codecs.decode(code.encode('ascii'), 'base64')\n        code = marshal.loads(raw_code)\n    except (UnicodeEncodeError, binascii.Error, ValueError):\n        # backwards compatibility for models serialized prior to 2.1.2\n        raw_code = code.encode('raw_unicode_escape')\n        code = marshal.loads(raw_code)\n    if globs is None:\n        globs = globals()\n    return python_types.FunctionType(code, globs,\n                                     name=code.co_name,\n                                     argdefs=defaults,\n                                     closure=closure)",
        "begin_line": 191,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.generic_utils.ensure_value_to_cell#208",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.ensure_value_to_cell(value)",
        "snippet": "    def ensure_value_to_cell(value):\n        \"\"\"Ensures that a value is converted to a python cell object.\n\n        # Arguments\n            value: Any value that needs to be casted to the cell type\n\n        # Returns\n            A value wrapped as a cell object (see function \"func_load\")\n\n        \"\"\"\n        def dummy_fn():\n            value  # just access it so it gets captured in .__closure__\n\n        cell_value = dummy_fn.__closure__[0]\n        if not isinstance(value, type(cell_value)):\n            return cell_value\n        else:\n            return value",
        "begin_line": 208,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.generic_utils.dummy_fn#218",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.dummy_fn()",
        "snippet": "        def dummy_fn():\n            value  # just access it so it gets captured in .__closure__",
        "begin_line": 218,
        "end_line": 219,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.utils.generic_utils.has_arg#244",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.has_arg(fn, name, accept_all=False)",
        "snippet": "def has_arg(fn, name, accept_all=False):\n    \"\"\"Checks if a callable accepts a given keyword argument.\n\n    For Python 2, checks if there is an argument with the given name.\n\n    For Python 3, checks if there is an argument with the given name, and\n    also whether this argument can be called with a keyword (i.e. if it is\n    not a positional-only argument).\n\n    # Arguments\n        fn: Callable to inspect.\n        name: Check if `fn` can be called with `name` as a keyword argument.\n        accept_all: What to return if there is no parameter called `name`\n                    but the function accepts a `**kwargs` argument.\n\n    # Returns\n        bool, whether `fn` accepts a `name` keyword argument.\n    \"\"\"\n    if sys.version_info < (3,):\n        arg_spec = inspect.getargspec(fn)\n        if accept_all and arg_spec.keywords is not None:\n            return True\n        return (name in arg_spec.args)\n    elif sys.version_info < (3, 3):\n        arg_spec = inspect.getfullargspec(fn)\n        if accept_all and arg_spec.varkw is not None:\n            return True\n        return (name in arg_spec.args or\n                name in arg_spec.kwonlyargs)\n    else:\n        signature = inspect.signature(fn)\n        parameter = signature.parameters.get(name)\n        if parameter is None:\n            if accept_all:\n                for param in signature.parameters.values():\n                    if param.kind == inspect.Parameter.VAR_KEYWORD:\n                        return True\n            return False\n        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                                   inspect.Parameter.KEYWORD_ONLY))",
        "begin_line": 244,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006706908115358819,
            "pseudo_dstar_susp": 0.0016,
            "pseudo_tarantula_susp": 0.00035803795202291446,
            "pseudo_op2_susp": 0.0016,
            "pseudo_barinel_susp": 0.00035803795202291446
        }
    },
    {
        "name": "keras.utils.generic_utils.Progbar.__init__#300",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.Progbar",
        "signature": "keras.utils.generic_utils.Progbar.__init__(self, target, width=30, verbose=1, interval=0.05, stateful_metrics=None)",
        "snippet": "    def __init__(self, target, width=30, verbose=1, interval=0.05,\n                 stateful_metrics=None):\n        self.target = target\n        self.width = width\n        self.verbose = verbose\n        self.interval = interval\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()\n\n        self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n                                  sys.stdout.isatty()) or\n                                 'ipykernel' in sys.modules)\n        self._total_width = 0\n        self._seen_so_far = 0\n        self._values = collections.OrderedDict()\n        self._start = time.time()\n        self._last_update = 0",
        "begin_line": 300,
        "end_line": 318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030129557095510696,
            "pseudo_dstar_susp": 0.0002959455460195324,
            "pseudo_tarantula_susp": 0.0003380662609871535,
            "pseudo_op2_susp": 0.0002959455460195324,
            "pseudo_barinel_susp": 0.0003380662609871535
        }
    },
    {
        "name": "keras.utils.generic_utils.Progbar.update#320",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.Progbar",
        "signature": "keras.utils.generic_utils.Progbar.update(self, current, values=None)",
        "snippet": "    def update(self, current, values=None):\n        \"\"\"Updates the progress bar.\n\n        # Arguments\n            current: Index of current step.\n            values: List of tuples:\n                `(name, value_for_last_step)`.\n                If `name` is in `stateful_metrics`,\n                `value_for_last_step` will be displayed as-is.\n                Else, an average of the metric over time will be displayed.\n        \"\"\"\n        values = values or []\n        for k, v in values:\n            if k not in self.stateful_metrics:\n                if k not in self._values:\n                    self._values[k] = [v * (current - self._seen_so_far),\n                                       current - self._seen_so_far]\n                else:\n                    self._values[k][0] += v * (current - self._seen_so_far)\n                    self._values[k][1] += (current - self._seen_so_far)\n            else:\n                # Stateful metrics output a numeric value.  This representation\n                # means \"take an average from a single value\" but keeps the\n                # numeric formatting.\n                self._values[k] = [v, 1]\n        self._seen_so_far = current\n\n        now = time.time()\n        info = ' - %.0fs' % (now - self._start)\n        if self.verbose == 1:\n            if (now - self._last_update < self.interval and\n                    self.target is not None and current < self.target):\n                return\n\n            prev_total_width = self._total_width\n            if self._dynamic_display:\n                sys.stdout.write('\\b' * prev_total_width)\n                sys.stdout.write('\\r')\n            else:\n                sys.stdout.write('\\n')\n\n            if self.target is not None:\n                numdigits = int(np.floor(np.log10(self.target))) + 1\n                barstr = '%%%dd/%d [' % (numdigits, self.target)\n                bar = barstr % current\n                prog = float(current) / self.target\n                prog_width = int(self.width * prog)\n                if prog_width > 0:\n                    bar += ('=' * (prog_width - 1))\n                    if current < self.target:\n                        bar += '>'\n                    else:\n                        bar += '='\n                bar += ('.' * (self.width - prog_width))\n                bar += ']'\n            else:\n                bar = '%7d/Unknown' % current\n\n            self._total_width = len(bar)\n            sys.stdout.write(bar)\n\n            if current:\n                time_per_unit = (now - self._start) / current\n            else:\n                time_per_unit = 0\n            if self.target is not None and current < self.target:\n                eta = time_per_unit * (self.target - current)\n                if eta > 3600:\n                    eta_format = ('%d:%02d:%02d' %\n                                  (eta // 3600, (eta % 3600) // 60, eta % 60))\n                elif eta > 60:\n                    eta_format = '%d:%02d' % (eta // 60, eta % 60)\n                else:\n                    eta_format = '%ds' % eta\n\n                info = ' - ETA: %s' % eta_format\n            else:\n                if time_per_unit >= 1:\n                    info += ' %.0fs/step' % time_per_unit\n                elif time_per_unit >= 1e-3:\n                    info += ' %.0fms/step' % (time_per_unit * 1e3)\n                else:\n                    info += ' %.0fus/step' % (time_per_unit * 1e6)\n\n            for k in self._values:\n                info += ' - %s:' % k\n                if isinstance(self._values[k], list):\n                    avg = np.mean(\n                        self._values[k][0] / max(1, self._values[k][1]))\n                    if abs(avg) > 1e-3:\n                        info += ' %.4f' % avg\n                    else:\n                        info += ' %.4e' % avg\n                else:\n                    info += ' %s' % self._values[k]\n\n            self._total_width += len(info)\n            if prev_total_width > self._total_width:\n                info += (' ' * (prev_total_width - self._total_width))\n\n            if self.target is not None and current >= self.target:\n                info += '\\n'\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n        elif self.verbose == 2:\n            if self.target is None or current >= self.target:\n                for k in self._values:\n                    info += ' - %s:' % k\n                    avg = np.mean(\n                        self._values[k][0] / max(1, self._values[k][1]))\n                    if avg > 1e-3:\n                        info += ' %.4f' % avg\n                    else:\n                        info += ' %.4e' % avg\n                info += '\\n'\n\n                sys.stdout.write(info)\n                sys.stdout.flush()\n\n        self._last_update = now",
        "begin_line": 320,
        "end_line": 441,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.utils.generic_utils.to_list#447",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.to_list(x)",
        "snippet": "def to_list(x):\n    \"\"\"Normalizes a list/tensor into a list.\n\n    If a tensor is passed, we return\n    a list of size 1 containing the tensor.\n\n    # Arguments\n        x: target object to be normalized.\n\n    # Returns\n        A list.\n    \"\"\"\n    if isinstance(x, list):\n        return x\n    return [x]",
        "begin_line": 447,
        "end_line": 461,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007855459544383347,
            "pseudo_dstar_susp": 0.0022371364653243847,
            "pseudo_tarantula_susp": 0.000388651379712398,
            "pseudo_op2_susp": 0.0022371364653243847,
            "pseudo_barinel_susp": 0.000388651379712398
        }
    },
    {
        "name": "keras.utils.generic_utils.unpack_singleton#464",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.unpack_singleton(x)",
        "snippet": "def unpack_singleton(x):\n    \"\"\"Gets the first element if the iterable has only one value.\n\n    Otherwise return the iterable.\n\n    # Argument:\n        x: A list or tuple.\n\n    # Returns:\n        The same iterable or the first element.\n    \"\"\"\n    if len(x) == 1:\n        return x[0]\n    return x",
        "begin_line": 464,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006816632583503749,
            "pseudo_dstar_susp": 0.0016260162601626016,
            "pseudo_tarantula_susp": 0.00036114120621162876,
            "pseudo_op2_susp": 0.0016260162601626016,
            "pseudo_barinel_susp": 0.00036114120621162876
        }
    },
    {
        "name": "keras.utils.generic_utils.object_list_uid#480",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.object_list_uid(object_list)",
        "snippet": "def object_list_uid(object_list):\n    object_list = to_list(object_list)\n    return ', '.join([str(abs(id(x))) for x in object_list])",
        "begin_line": 480,
        "end_line": 482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019569471624266144,
            "pseudo_dstar_susp": 0.1111111111111111,
            "pseudo_tarantula_susp": 0.000388651379712398,
            "pseudo_op2_susp": 0.1111111111111111,
            "pseudo_barinel_susp": 0.000388651379712398
        }
    },
    {
        "name": "keras.utils.generic_utils.is_all_none#485",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.is_all_none(iterable_or_element)",
        "snippet": "def is_all_none(iterable_or_element):\n    if not isinstance(iterable_or_element, (list, tuple)):\n        iterable = [iterable_or_element]\n    else:\n        iterable = iterable_or_element\n    for element in iterable:\n        if element is not None:\n            return False\n    return True",
        "begin_line": 485,
        "end_line": 493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000864304235090752,
            "pseudo_dstar_susp": 0.0030211480362537764,
            "pseudo_tarantula_susp": 0.0006455777921239509,
            "pseudo_op2_susp": 0.0030211480362537764,
            "pseudo_barinel_susp": 0.000649772579597141
        }
    },
    {
        "name": "keras.utils.generic_utils.slice_arrays#496",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.slice_arrays(arrays, start=None, stop=None)",
        "snippet": "def slice_arrays(arrays, start=None, stop=None):\n    \"\"\"Slices an array or list of arrays.\n\n    This takes an array-like, or a list of\n    array-likes, and outputs:\n        - arrays[start:stop] if `arrays` is an array-like\n        - [x[start:stop] for x in arrays] if `arrays` is a list\n\n    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n\n    # Arguments\n        arrays: Single array or list of arrays.\n        start: can be an integer index (start index)\n            or a list/array of indices\n        stop: integer (stop index); should be None if\n            `start` was a list.\n\n    # Returns\n        A slice of the array(s).\n    \"\"\"\n    if arrays is None:\n        return [None]\n    elif isinstance(arrays, list):\n        if hasattr(start, '__len__'):\n            # hdf5 datasets only support list objects as indices\n            if hasattr(start, 'shape'):\n                start = start.tolist()\n            return [None if x is None else x[start] for x in arrays]\n        else:\n            return [None if x is None else x[start:stop] for x in arrays]\n    else:\n        if hasattr(start, '__len__'):\n            if hasattr(start, 'shape'):\n                start = start.tolist()\n            return arrays[start]\n        elif hasattr(start, '__getitem__'):\n            return arrays[start:stop]\n        else:\n            return [None]",
        "begin_line": 496,
        "end_line": 534,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.0006693440428380187,
            "pseudo_tarantula_susp": 0.0010822510822510823,
            "pseudo_op2_susp": 0.0006693440428380187,
            "pseudo_barinel_susp": 0.0010822510822510823
        }
    },
    {
        "name": "keras.utils.generic_utils.transpose_shape#537",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.transpose_shape(shape, target_format, spatial_axes)",
        "snippet": "def transpose_shape(shape, target_format, spatial_axes):\n    \"\"\"Converts a tuple or a list to the correct `data_format`.\n\n    It does so by switching the positions of its elements.\n\n    # Arguments\n        shape: Tuple or list, often representing shape,\n            corresponding to `'channels_last'`.\n        target_format: A string, either `'channels_first'` or `'channels_last'`.\n        spatial_axes: A tuple of integers.\n            Correspond to the indexes of the spatial axes.\n            For example, if you pass a shape\n            representing (batch_size, timesteps, rows, cols, channels),\n            then `spatial_axes=(2, 3)`.\n\n    # Returns\n        A tuple or list, with the elements permuted according\n        to `target_format`.\n\n    # Example\n    ```python\n        >>> from keras.utils.generic_utils import transpose_shape\n        >>> transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))\n        (16, 32, 128, 128)\n        >>> transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))\n        (16, 128, 128, 32)\n        >>> transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))\n        (32, 128, 128)\n    ```\n\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    \"\"\"\n    if target_format == 'channels_first':\n        new_values = shape[:spatial_axes[0]]\n        new_values += (shape[-1],)\n        new_values += tuple(shape[x] for x in spatial_axes)\n\n        if isinstance(shape, list):\n            return list(new_values)\n        return new_values\n    elif target_format == 'channels_last':\n        return shape\n    else:\n        raise ValueError('The `data_format` argument must be one of '\n                         '\"channels_first\", \"channels_last\". Received: ' +\n                         str(target_format))",
        "begin_line": 537,
        "end_line": 583,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006329113924050633,
            "pseudo_dstar_susp": 0.0013297872340425532,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0013297872340425532,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.constraints.Constraint.get_config#18",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.Constraint",
        "signature": "keras.constraints.Constraint.get_config(self)",
        "snippet": "    def get_config(self):\n        return {}",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.MaxNorm.__init__#46",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.MaxNorm",
        "signature": "keras.constraints.MaxNorm.__init__(self, max_value=2, axis=0)",
        "snippet": "    def __init__(self, max_value=2, axis=0):\n        self.max_value = max_value\n        self.axis = axis",
        "begin_line": 46,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.558401835213153e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.MaxNorm.__call__#50",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.MaxNorm",
        "signature": "keras.constraints.MaxNorm.__call__(self, w)",
        "snippet": "    def __call__(self, w):\n        norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n        desired = K.clip(norms, 0, self.max_value)\n        w *= (desired / (K.epsilon() + norms))\n        return w",
        "begin_line": 50,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.MaxNorm.get_config#56",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.MaxNorm",
        "signature": "keras.constraints.MaxNorm.get_config(self)",
        "snippet": "    def get_config(self):\n        return {'max_value': self.max_value,\n                'axis': self.axis}",
        "begin_line": 56,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.699321047526673e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.NonNeg.__call__#65",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.NonNeg",
        "signature": "keras.constraints.NonNeg.__call__(self, w)",
        "snippet": "    def __call__(self, w):\n        w *= K.cast(K.greater_equal(w, 0.), K.floatx())\n        return w",
        "begin_line": 65,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.UnitNorm.__init__#87",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.UnitNorm",
        "signature": "keras.constraints.UnitNorm.__init__(self, axis=0)",
        "snippet": "    def __init__(self, axis=0):\n        self.axis = axis",
        "begin_line": 87,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010481081647626035,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.UnitNorm.__call__#90",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.UnitNorm",
        "signature": "keras.constraints.UnitNorm.__call__(self, w)",
        "snippet": "    def __call__(self, w):\n        return w / (K.epsilon() + K.sqrt(K.sum(K.square(w),\n                                               axis=self.axis,\n                                               keepdims=True)))",
        "begin_line": 90,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.UnitNorm.get_config#95",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.UnitNorm",
        "signature": "keras.constraints.UnitNorm.get_config(self)",
        "snippet": "    def get_config(self):\n        return {'axis': self.axis}",
        "begin_line": 95,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010583130489998942,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.MinMaxNorm.__init__#128",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.MinMaxNorm",
        "signature": "keras.constraints.MinMaxNorm.__init__(self, min_value=0.0, max_value=1.0, rate=1.0, axis=0)",
        "snippet": "    def __init__(self, min_value=0.0, max_value=1.0, rate=1.0, axis=0):\n        self.min_value = min_value\n        self.max_value = max_value\n        self.rate = rate\n        self.axis = axis",
        "begin_line": 128,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.MinMaxNorm.__call__#134",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.MinMaxNorm",
        "signature": "keras.constraints.MinMaxNorm.__call__(self, w)",
        "snippet": "    def __call__(self, w):\n        norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n        desired = (self.rate * K.clip(norms, self.min_value, self.max_value) +\n                   (1 - self.rate) * norms)\n        w *= (desired / (K.epsilon() + norms))\n        return w",
        "begin_line": 134,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.MinMaxNorm.get_config#141",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints.MinMaxNorm",
        "signature": "keras.constraints.MinMaxNorm.get_config(self)",
        "snippet": "    def get_config(self):\n        return {'min_value': self.min_value,\n                'max_value': self.max_value,\n                'rate': self.rate,\n                'axis': self.axis}",
        "begin_line": 141,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.serialize#162",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints",
        "signature": "keras.constraints.serialize(constraint)",
        "snippet": "def serialize(constraint):\n    return serialize_keras_object(constraint)",
        "begin_line": 162,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00046860356138706655,
            "pseudo_dstar_susp": 0.0008976660682226212,
            "pseudo_tarantula_susp": 0.0004623208506703652,
            "pseudo_op2_susp": 0.0008976660682226212,
            "pseudo_barinel_susp": 0.0004623208506703652
        }
    },
    {
        "name": "keras.constraints.deserialize#166",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints",
        "signature": "keras.constraints.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    return deserialize_keras_object(config,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='constraint')",
        "begin_line": 166,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.51746454744456e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.constraints.get#173",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints",
        "signature": "keras.constraints.get(identifier)",
        "snippet": "def get(identifier):\n    if identifier is None:\n        return None\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret constraint identifier: ' +\n                         str(identifier))",
        "begin_line": 173,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010504201680672268,
            "pseudo_dstar_susp": 0.004524886877828055,
            "pseudo_tarantula_susp": 0.00046125461254612545,
            "pseudo_op2_susp": 0.004524886877828055,
            "pseudo_barinel_susp": 0.00046125461254612545
        }
    },
    {
        "name": "keras.engine.training_generator.fit_generator#21",
        "src_path": "keras/engine/training_generator.py",
        "class_name": "keras.engine.training_generator",
        "signature": "keras.engine.training_generator.fit_generator(model, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)",
        "snippet": "def fit_generator(model,\n                  generator,\n                  steps_per_epoch=None,\n                  epochs=1,\n                  verbose=1,\n                  callbacks=None,\n                  validation_data=None,\n                  validation_steps=None,\n                  class_weight=None,\n                  max_queue_size=10,\n                  workers=1,\n                  use_multiprocessing=False,\n                  shuffle=True,\n                  initial_epoch=0):\n    \"\"\"See docstring for `Model.fit_generator`.\"\"\"\n    wait_time = 0.01  # in seconds\n    epoch = initial_epoch\n\n    do_validation = bool(validation_data)\n    model._make_train_function()\n    if do_validation:\n        model._make_test_function()\n\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        warnings.warn(\n            UserWarning('Using a generator with `use_multiprocessing=True`'\n                        ' and multiple workers may duplicate your data.'\n                        ' Please consider using the`keras.utils.Sequence'\n                        ' class.'))\n    if steps_per_epoch is None:\n        if is_sequence:\n            steps_per_epoch = len(generator)\n        else:\n            raise ValueError('`steps_per_epoch=None` is only valid for a'\n                             ' generator based on the '\n                             '`keras.utils.Sequence`'\n                             ' class. Please specify `steps_per_epoch` '\n                             'or use the `keras.utils.Sequence` class.')\n\n    # python 2 has 'next', 3 has '__next__'\n    # avoid any explicit version checks\n    val_gen = (hasattr(validation_data, 'next') or\n               hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n            not validation_steps):\n        raise ValueError('`validation_steps=None` is only valid for a'\n                         ' generator based on the `keras.utils.Sequence`'\n                         ' class. Please specify `validation_steps` or use'\n                         ' the `keras.utils.Sequence` class.')\n\n    # Prepare display labels.\n    out_labels = model.metrics_names\n    callback_metrics = out_labels + ['val_' + n for n in out_labels]\n\n    # prepare callbacks\n    model.history = cbks.History()\n    _callbacks = [cbks.BaseLogger(\n        stateful_metrics=model.stateful_metric_names)]\n    if verbose:\n        _callbacks.append(\n            cbks.ProgbarLogger(\n                count_mode='steps',\n                stateful_metrics=model.stateful_metric_names))\n    _callbacks += (callbacks or []) + [model.history]\n    callbacks = cbks.CallbackList(_callbacks)\n\n    # it's possible to callback a different model than self:\n    if hasattr(model, 'callback_model') and model.callback_model:\n        callback_model = model.callback_model\n    else:\n        callback_model = model\n    callbacks.set_model(callback_model)\n    callbacks.set_params({\n        'epochs': epochs,\n        'steps': steps_per_epoch,\n        'verbose': verbose,\n        'do_validation': do_validation,\n        'metrics': callback_metrics,\n    })\n    callbacks.on_train_begin()\n\n    enqueuer = None\n    val_enqueuer = None\n\n    try:\n        if do_validation:\n            if val_gen and workers > 0:\n                # Create an Enqueuer that can be reused\n                val_data = validation_data\n                if isinstance(val_data, Sequence):\n                    val_enqueuer = OrderedEnqueuer(val_data,\n                                                   use_multiprocessing=use_multiprocessing)\n                    validation_steps = validation_steps or len(val_data)\n                else:\n                    val_enqueuer = GeneratorEnqueuer(val_data,\n                                                     use_multiprocessing=use_multiprocessing)\n                val_enqueuer.start(workers=workers,\n                                   max_queue_size=max_queue_size)\n                val_enqueuer_gen = val_enqueuer.get()\n            elif val_gen:\n                val_data = validation_data\n                if isinstance(val_data, Sequence):\n                    val_enqueuer_gen = iter_sequence_infinite(generator)\n                else:\n                    val_enqueuer_gen = val_data\n            else:\n                # Prepare data for validation\n                if len(validation_data) == 2:\n                    val_x, val_y = validation_data\n                    val_sample_weight = None\n                elif len(validation_data) == 3:\n                    val_x, val_y, val_sample_weight = validation_data\n                else:\n                    raise ValueError('`validation_data` should be a tuple '\n                                     '`(val_x, val_y, val_sample_weight)` '\n                                     'or `(val_x, val_y)`. Found: ' +\n                                     str(validation_data))\n                val_x, val_y, val_sample_weights = model._standardize_user_data(\n                    val_x, val_y, val_sample_weight)\n                val_data = val_x + val_y + val_sample_weights\n                if model.uses_learning_phase and not isinstance(K.learning_phase(),\n                                                                int):\n                    val_data += [0.]\n                for cbk in callbacks:\n                    cbk.validation_data = val_data\n\n        if workers > 0:\n            if is_sequence:\n                enqueuer = OrderedEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    shuffle=shuffle)\n            else:\n                enqueuer = GeneratorEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    wait_time=wait_time)\n            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n            output_generator = enqueuer.get()\n        else:\n            if is_sequence:\n                output_generator = iter_sequence_infinite(generator)\n            else:\n                output_generator = generator\n\n        callback_model.stop_training = False\n        # Construct epoch logs.\n        epoch_logs = {}\n        while epoch < epochs:\n            for m in model.stateful_metric_functions:\n                m.reset_states()\n            callbacks.on_epoch_begin(epoch)\n            steps_done = 0\n            batch_index = 0\n            while steps_done < steps_per_epoch:\n                generator_output = next(output_generator)\n\n                if not hasattr(generator_output, '__len__'):\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n\n                if len(generator_output) == 2:\n                    x, y = generator_output\n                    sample_weight = None\n                elif len(generator_output) == 3:\n                    x, y, sample_weight = generator_output\n                else:\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n                # build batch logs\n                batch_logs = {}\n                if x is None or len(x) == 0:\n                    # Handle data tensors support when no input given\n                    # step-size = 1 for data tensors\n                    batch_size = 1\n                elif isinstance(x, list):\n                    batch_size = x[0].shape[0]\n                elif isinstance(x, dict):\n                    batch_size = list(x.values())[0].shape[0]\n                else:\n                    batch_size = x.shape[0]\n                batch_logs['batch'] = batch_index\n                batch_logs['size'] = batch_size\n                callbacks.on_batch_begin(batch_index, batch_logs)\n\n                outs = model.train_on_batch(x, y,\n                                            sample_weight=sample_weight,\n                                            class_weight=class_weight)\n\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(batch_index, batch_logs)\n\n                batch_index += 1\n                steps_done += 1\n\n                # Epoch finished.\n                if steps_done >= steps_per_epoch and do_validation:\n                    if val_gen:\n                        val_outs = model.evaluate_generator(\n                            val_enqueuer_gen,\n                            validation_steps,\n                            workers=0)\n                    else:\n                        # No need for try/except because\n                        # data has already been validated.\n                        val_outs = model.evaluate(\n                            val_x, val_y,\n                            batch_size=batch_size,\n                            sample_weight=val_sample_weights,\n                            verbose=0)\n                    val_outs = to_list(val_outs)\n                    # Same labels assumed.\n                    for l, o in zip(out_labels, val_outs):\n                        epoch_logs['val_' + l] = o\n\n                if callback_model.stop_training:\n                    break\n\n            callbacks.on_epoch_end(epoch, epoch_logs)\n            epoch += 1\n            if callback_model.stop_training:\n                break\n\n    finally:\n        try:\n            if enqueuer is not None:\n                enqueuer.stop()\n        finally:\n            if val_enqueuer is not None:\n                val_enqueuer.stop()\n\n    callbacks.on_train_end()\n    return model.history",
        "begin_line": 21,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.engine.training_generator.evaluate_generator#265",
        "src_path": "keras/engine/training_generator.py",
        "class_name": "keras.engine.training_generator",
        "signature": "keras.engine.training_generator.evaluate_generator(model, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)",
        "snippet": "def evaluate_generator(model, generator,\n                       steps=None,\n                       max_queue_size=10,\n                       workers=1,\n                       use_multiprocessing=False,\n                       verbose=0):\n    \"\"\"See docstring for `Model.evaluate_generator`.\"\"\"\n    model._make_test_function()\n\n    if hasattr(model, 'metrics'):\n        for m in model.stateful_metric_functions:\n            m.reset_states()\n        stateful_metric_indices = [\n            i for i, name in enumerate(model.metrics_names)\n            if str(name) in model.stateful_metric_names]\n    else:\n        stateful_metric_indices = []\n\n    steps_done = 0\n    wait_time = 0.01\n    outs_per_batch = []\n    batch_sizes = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        warnings.warn(\n            UserWarning('Using a generator with `use_multiprocessing=True`'\n                        ' and multiple workers may duplicate your data.'\n                        ' Please consider using the`keras.utils.Sequence'\n                        ' class.'))\n    if steps is None:\n        if is_sequence:\n            steps = len(generator)\n        else:\n            raise ValueError('`steps=None` is only valid for a generator'\n                             ' based on the `keras.utils.Sequence` class.'\n                             ' Please specify `steps` or use the'\n                             ' `keras.utils.Sequence` class.')\n    enqueuer = None\n\n    try:\n        if workers > 0:\n            if is_sequence:\n                enqueuer = OrderedEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing)\n            else:\n                enqueuer = GeneratorEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    wait_time=wait_time)\n            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n            output_generator = enqueuer.get()\n        else:\n            if is_sequence:\n                output_generator = iter_sequence_infinite(generator)\n            else:\n                output_generator = generator\n\n        if verbose == 1:\n            progbar = Progbar(target=steps)\n\n        while steps_done < steps:\n            generator_output = next(output_generator)\n            if not hasattr(generator_output, '__len__'):\n                raise ValueError('Output of generator should be a tuple '\n                                 '(x, y, sample_weight) '\n                                 'or (x, y). Found: ' +\n                                 str(generator_output))\n            if len(generator_output) == 2:\n                x, y = generator_output\n                sample_weight = None\n            elif len(generator_output) == 3:\n                x, y, sample_weight = generator_output\n            else:\n                raise ValueError('Output of generator should be a tuple '\n                                 '(x, y, sample_weight) '\n                                 'or (x, y). Found: ' +\n                                 str(generator_output))\n            outs = model.test_on_batch(x, y, sample_weight=sample_weight)\n            outs = to_list(outs)\n            outs_per_batch.append(outs)\n\n            if x is None or len(x) == 0:\n                # Handle data tensors support when no input given\n                # step-size = 1 for data tensors\n                batch_size = 1\n            elif isinstance(x, list):\n                batch_size = x[0].shape[0]\n            elif isinstance(x, dict):\n                batch_size = list(x.values())[0].shape[0]\n            else:\n                batch_size = x.shape[0]\n            if batch_size == 0:\n                raise ValueError('Received an empty batch. '\n                                 'Batches should contain '\n                                 'at least one item.')\n            steps_done += 1\n            batch_sizes.append(batch_size)\n            if verbose == 1:\n                progbar.update(steps_done)\n\n    finally:\n        if enqueuer is not None:\n            enqueuer.stop()\n\n    averages = []\n    for i in range(len(outs)):\n        if i not in stateful_metric_indices:\n            averages.append(np.average([out[i] for out in outs_per_batch],\n                                       weights=batch_sizes))\n        else:\n            averages.append(np.float64(outs_per_batch[-1][i]))\n    return unpack_singleton(averages)",
        "begin_line": 265,
        "end_line": 377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015723270440251573,
            "pseudo_dstar_susp": 0.0006844626967830253,
            "pseudo_tarantula_susp": 0.002551020408163265,
            "pseudo_op2_susp": 0.0006844626967830253,
            "pseudo_barinel_susp": 0.002551020408163265
        }
    },
    {
        "name": "keras.engine.training_generator.predict_generator#380",
        "src_path": "keras/engine/training_generator.py",
        "class_name": "keras.engine.training_generator",
        "signature": "keras.engine.training_generator.predict_generator(model, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)",
        "snippet": "def predict_generator(model, generator,\n                      steps=None,\n                      max_queue_size=10,\n                      workers=1,\n                      use_multiprocessing=False,\n                      verbose=0):\n    \"\"\"See docstring for `Model.predict_generator`.\"\"\"\n    model._make_predict_function()\n\n    steps_done = 0\n    wait_time = 0.01\n    all_outs = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        warnings.warn(\n            UserWarning('Using a generator with `use_multiprocessing=True`'\n                        ' and multiple workers may duplicate your data.'\n                        ' Please consider using the`keras.utils.Sequence'\n                        ' class.'))\n    if steps is None:\n        if is_sequence:\n            steps = len(generator)\n        else:\n            raise ValueError('`steps=None` is only valid for a generator'\n                             ' based on the `keras.utils.Sequence` class.'\n                             ' Please specify `steps` or use the'\n                             ' `keras.utils.Sequence` class.')\n    enqueuer = None\n\n    try:\n        if workers > 0:\n            if is_sequence:\n                enqueuer = OrderedEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing)\n            else:\n                enqueuer = GeneratorEnqueuer(\n                    generator,\n                    use_multiprocessing=use_multiprocessing,\n                    wait_time=wait_time)\n            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n            output_generator = enqueuer.get()\n        else:\n            if is_sequence:\n                output_generator = iter_sequence_infinite(generator)\n            else:\n                output_generator = generator\n\n        if verbose == 1:\n            progbar = Progbar(target=steps)\n\n        while steps_done < steps:\n            generator_output = next(output_generator)\n            if isinstance(generator_output, tuple):\n                # Compatibility with the generators\n                # used for training.\n                if len(generator_output) == 2:\n                    x, _ = generator_output\n                elif len(generator_output) == 3:\n                    x, _, _ = generator_output\n                else:\n                    raise ValueError('Output of generator should be '\n                                     'a tuple `(x, y, sample_weight)` '\n                                     'or `(x, y)`. Found: ' +\n                                     str(generator_output))\n            else:\n                # Assumes a generator that only\n                # yields inputs (not targets and sample weights).\n                x = generator_output\n\n            outs = model.predict_on_batch(x)\n            outs = to_list(outs)\n\n            if not all_outs:\n                for out in outs:\n                    all_outs.append([])\n\n            for i, out in enumerate(outs):\n                all_outs[i].append(out)\n            steps_done += 1\n            if verbose == 1:\n                progbar.update(steps_done)\n\n    finally:\n        if enqueuer is not None:\n            enqueuer.stop()\n\n    if len(all_outs) == 1:\n        if steps_done == 1:\n            return all_outs[0][0]\n        else:\n            return np.concatenate(all_outs[0])\n    if steps_done == 1:\n        return [out[0] for out in all_outs]\n    else:\n        return [np.concatenate(out) for out in all_outs]",
        "begin_line": 380,
        "end_line": 475,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.engine.input_layer.InputLayer.__init__#34",
        "src_path": "keras/engine/input_layer.py",
        "class_name": "keras.engine.input_layer.InputLayer",
        "signature": "keras.engine.input_layer.InputLayer.__init__(self, input_shape=None, batch_size=None, batch_input_shape=None, dtype=None, input_tensor=None, sparse=False, name=None)",
        "snippet": "    def __init__(self, input_shape=None, batch_size=None,\n                 batch_input_shape=None,\n                 dtype=None, input_tensor=None, sparse=False, name=None):\n        if not name:\n            prefix = 'input'\n            name = prefix + '_' + str(K.get_uid(prefix))\n        super(InputLayer, self).__init__(dtype=dtype, name=name)\n\n        self.trainable = False\n        self.built = True\n        self.sparse = sparse\n        self.supports_masking = True\n\n        if input_shape and batch_input_shape:\n            raise ValueError('Only provide the input_shape OR '\n                             'batch_input_shape argument to '\n                             'InputLayer, not both at the same time.')\n        if input_tensor is not None and batch_input_shape is None:\n            # If input_tensor is set, and batch_input_shape is not set:\n            # Attempt automatic input shape inference.\n            try:\n                batch_input_shape = K.int_shape(input_tensor)\n            except TypeError:\n                if not input_shape and not batch_input_shape:\n                    raise ValueError('InputLayer was provided '\n                                     'an input_tensor argument, '\n                                     'but its input shape cannot be '\n                                     'automatically inferred. '\n                                     'You should pass an input_shape or '\n                                     'batch_input_shape argument.')\n        if not batch_input_shape:\n            if not input_shape:\n                raise ValueError('An Input layer should be passed either '\n                                 'a `batch_input_shape` or an `input_shape`.')\n            else:\n                batch_input_shape = (batch_size,) + tuple(input_shape)\n        else:\n            batch_input_shape = tuple(batch_input_shape)\n\n        if not dtype:\n            if input_tensor is None:\n                dtype = K.floatx()\n            else:\n                dtype = K.dtype(input_tensor)\n\n        self.batch_input_shape = batch_input_shape\n        self.dtype = dtype\n\n        if input_tensor is None:\n            self.is_placeholder = True\n            input_tensor = K.placeholder(shape=batch_input_shape,\n                                         dtype=dtype,\n                                         sparse=self.sparse,\n                                         name=self.name)\n        else:\n            self.is_placeholder = False\n            input_tensor._keras_shape = batch_input_shape\n        # Create an input node to add to self.outbound_node\n        # and set output_tensors' _keras_history.\n        input_tensor._uses_learning_phase = False\n        input_tensor._keras_history = (self, 0, 0)\n        Node(self,\n             inbound_layers=[],\n             node_indices=[],\n             tensor_indices=[],\n             input_tensors=[input_tensor],\n             output_tensors=[input_tensor],\n             input_masks=[None],\n             output_masks=[None],\n             input_shapes=[batch_input_shape],\n             output_shapes=[batch_input_shape])",
        "begin_line": 34,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007052186177715092,
            "pseudo_dstar_susp": 0.0017452006980802793,
            "pseudo_tarantula_susp": 0.0003667033370003667,
            "pseudo_op2_susp": 0.0017452006980802793,
            "pseudo_barinel_susp": 0.0003667033370003667
        }
    },
    {
        "name": "keras.engine.input_layer.InputLayer.get_config#106",
        "src_path": "keras/engine/input_layer.py",
        "class_name": "keras.engine.input_layer.InputLayer",
        "signature": "keras.engine.input_layer.InputLayer.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'batch_input_shape': self.batch_input_shape,\n                  'dtype': self.dtype,\n                  'sparse': self.sparse,\n                  'name': self.name}\n        return config",
        "begin_line": 106,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002653223666755107,
            "pseudo_dstar_susp": 0.0002653223666755107,
            "pseudo_tarantula_susp": 0.0002655337227827934,
            "pseudo_op2_susp": 0.0002653223666755107,
            "pseudo_barinel_susp": 0.0002655337227827934
        }
    },
    {
        "name": "keras.engine.input_layer.Input#114",
        "src_path": "keras/engine/input_layer.py",
        "class_name": "keras.engine.input_layer",
        "signature": "keras.engine.input_layer.Input(shape=None, batch_shape=None, name=None, dtype=None, sparse=False, tensor=None)",
        "snippet": "def Input(shape=None, batch_shape=None,\n          name=None, dtype=None, sparse=False,\n          tensor=None):\n    \"\"\"`Input()` is used to instantiate a Keras tensor.\n\n    A Keras tensor is a tensor object from the underlying backend\n    (Theano, TensorFlow or CNTK), which we augment with certain\n    attributes that allow us to build a Keras model\n    just by knowing the inputs and outputs of the model.\n\n    For instance, if a, b and c are Keras tensors,\n    it becomes possible to do:\n    `model = Model(input=[a, b], output=c)`\n\n    The added Keras attributes are:\n        `_keras_shape`: Integer shape tuple propagated\n            via Keras-side shape inference.\n        `_keras_history`: Last layer applied to the tensor.\n            the entire layer graph is retrievable from that layer,\n            recursively.\n\n    # Arguments\n        shape: A shape tuple (integer), not including the batch size.\n            For instance, `shape=(32,)` indicates that the expected input\n            will be batches of 32-dimensional vectors.\n        batch_shape: A shape tuple (integer), including the batch size.\n            For instance, `batch_shape=(10, 32)` indicates that\n            the expected input will be batches of 10 32-dimensional vectors.\n            `batch_shape=(None, 32)` indicates batches of an arbitrary number\n            of 32-dimensional vectors.\n        name: An optional name string for the layer.\n            Should be unique in a model (do not reuse the same name twice).\n            It will be autogenerated if it isn't provided.\n        dtype: The data type expected by the input, as a string\n            (`float32`, `float64`, `int32`...)\n        sparse: A boolean specifying whether the placeholder\n            to be created is sparse.\n        tensor: Optional existing tensor to wrap into the `Input` layer.\n            If set, the layer will not create a placeholder tensor.\n\n    # Returns\n        A tensor.\n\n    # Example\n\n    ```python\n    # this is a logistic regression in Keras\n    x = Input(shape=(32,))\n    y = Dense(16, activation='softmax')(x)\n    model = Model(x, y)\n    ```\n    \"\"\"\n    if not batch_shape and tensor is None:\n        assert shape is not None, ('Please provide to Input either a `shape`'\n                                   ' or a `batch_shape` argument. Note that '\n                                   '`shape` does not include the batch '\n                                   'dimension.')\n    if shape is not None and not batch_shape:\n        batch_shape = (None,) + tuple(shape)\n    if not dtype:\n        dtype = K.floatx()\n    input_layer = InputLayer(batch_input_shape=batch_shape,\n                             name=name, dtype=dtype,\n                             sparse=sparse,\n                             input_tensor=tensor)\n    # Return tensor including _keras_shape and _keras_history.\n    # Note that in this case train_output and test_output are the same pointer.\n    outputs = input_layer._inbound_nodes[0].output_tensors\n    return unpack_singleton(outputs)",
        "begin_line": 114,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007052186177715092,
            "pseudo_dstar_susp": 0.0017452006980802793,
            "pseudo_tarantula_susp": 0.0003667033370003667,
            "pseudo_op2_susp": 0.0017452006980802793,
            "pseudo_barinel_susp": 0.0003667033370003667
        }
    },
    {
        "name": "keras.activations.softmax#14",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.softmax(x, axis=-1)",
        "snippet": "def softmax(x, axis=-1):\n    \"\"\"Softmax activation function.\n\n    # Arguments\n        x: Input tensor.\n        axis: Integer, axis along which the softmax normalization is applied.\n\n    # Returns\n        Tensor, output of softmax transformation.\n\n    # Raises\n        ValueError: In case `dim(x) == 1`.\n    \"\"\"\n    ndim = K.ndim(x)\n    if ndim == 2:\n        return K.softmax(x)\n    elif ndim > 2:\n        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n        s = K.sum(e, axis=axis, keepdims=True)\n        return e / s\n    else:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D. '\n                         'Received input: %s' % x)",
        "begin_line": 14,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002277904328018223,
            "pseudo_dstar_susp": 0.0009233610341643582,
            "pseudo_tarantula_susp": 0.0010822510822510823,
            "pseudo_op2_susp": 0.0009233610341643582,
            "pseudo_barinel_susp": 0.0010822510822510823
        }
    },
    {
        "name": "keras.activations.elu#39",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.elu(x, alpha=1.0)",
        "snippet": "def elu(x, alpha=1.0):\n    \"\"\"Exponential linear unit.\n\n    # Arguments\n        x: Input tensor.\n        alpha: A scalar, slope of negative section.\n\n    # Returns\n        The exponential linear activation: `x` if `x > 0` and\n        `alpha * (exp(x)-1)` if `x < 0`.\n\n    # References\n        - [Fast and Accurate Deep Network Learning by Exponential\n        Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)\n    \"\"\"\n    return K.elu(x, alpha)",
        "begin_line": 39,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.selu#57",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.selu(x)",
        "snippet": "def selu(x):\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    SELU is equal to: `scale * elu(x, alpha)`, where alpha and scale\n    are pre-defined constants. The values of `alpha` and `scale` are\n    chosen so that the mean and variance of the inputs are preserved\n    between two consecutive layers as long as the weights are initialized\n    correctly (see `lecun_normal` initialization) and the number of inputs\n    is \"large enough\" (see references for more information).\n\n    # Arguments\n        x: A tensor or variable to compute the activation function for.\n\n    # Returns\n       The scaled exponential unit activation: `scale * elu(x, alpha)`.\n\n    # Note\n        - To be used together with the initialization \"lecun_normal\".\n        - To be used together with the dropout variant \"AlphaDropout\".\n\n    # References\n        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    return scale * K.elu(x, alpha)",
        "begin_line": 57,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.softplus#85",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.softplus(x)",
        "snippet": "def softplus(x):\n    \"\"\"Softplus activation function.\n\n    # Arguments\n        x: Input tensor.\n\n    # Returns\n        The softplus activation: `log(exp(x) + 1)`.\n    \"\"\"\n    return K.softplus(x)",
        "begin_line": 85,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.softsign#97",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.softsign(x)",
        "snippet": "def softsign(x):\n    \"\"\"Softsign activation function.\n\n    # Arguments\n        x: Input tensor.\n\n    # Returns\n        The softplus activation: `x / (abs(x) + 1)`.\n    \"\"\"\n    return K.softsign(x)",
        "begin_line": 97,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.relu#109",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.relu(x, alpha=0.0, max_value=None)",
        "snippet": "def relu(x, alpha=0., max_value=None):\n    \"\"\"Rectified Linear Unit.\n\n    # Arguments\n        x: Input tensor.\n        alpha: Slope of the negative part. Defaults to zero.\n        max_value: Maximum value for the output.\n\n    # Returns\n        The (leaky) rectified linear unit activation: `x` if `x > 0`,\n        `alpha * x` if `x < 0`. If `max_value` is defined, the result\n        is truncated to this value.\n    \"\"\"\n    return K.relu(x, alpha=alpha, max_value=max_value)",
        "begin_line": 109,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019047619047619048,
            "pseudo_dstar_susp": 0.0009165902841429881,
            "pseudo_tarantula_susp": 0.001040582726326743,
            "pseudo_op2_susp": 0.0009165902841429881,
            "pseudo_barinel_susp": 0.001040582726326743
        }
    },
    {
        "name": "keras.activations.tanh#125",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.tanh(x)",
        "snippet": "def tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\n    \"\"\"\n    return K.tanh(x)",
        "begin_line": 125,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.933357155619081e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.sigmoid#131",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.sigmoid(x)",
        "snippet": "def sigmoid(x):\n    \"\"\"Sigmoid activation function.\n    \"\"\"\n    return K.sigmoid(x)",
        "begin_line": 131,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.hard_sigmoid#137",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.hard_sigmoid(x)",
        "snippet": "def hard_sigmoid(x):\n    \"\"\"Hard sigmoid activation function.\n\n    Faster to compute than sigmoid activation.\n\n    # Arguments\n        x: Input tensor.\n\n    # Returns\n        Hard sigmoid activation:\n\n        - `0` if `x < -2.5`\n        - `1` if `x > 2.5`\n        - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`.\n    \"\"\"\n    return K.hard_sigmoid(x)",
        "begin_line": 137,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 8.994423457456377e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.activations.linear#155",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.linear(x)",
        "snippet": "def linear(x):\n    \"\"\"Linear (i.e. identity) activation function.\n    \"\"\"\n    return x",
        "begin_line": 155,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010384215991692627,
            "pseudo_dstar_susp": 0.0012738853503184713,
            "pseudo_tarantula_susp": 0.0005665722379603399,
            "pseudo_op2_susp": 0.0012738853503184713,
            "pseudo_barinel_susp": 0.0005665722379603399
        }
    },
    {
        "name": "keras.activations.serialize#161",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.serialize(activation)",
        "snippet": "def serialize(activation):\n    return activation.__name__",
        "begin_line": 161,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004743833017077799,
            "pseudo_dstar_susp": 0.0009066183136899365,
            "pseudo_tarantula_susp": 0.000473260766682442,
            "pseudo_op2_susp": 0.0009066183136899365,
            "pseudo_barinel_susp": 0.000473260766682442
        }
    },
    {
        "name": "keras.activations.deserialize#165",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.deserialize(name, custom_objects=None)",
        "snippet": "def deserialize(name, custom_objects=None):\n    return deserialize_keras_object(\n        name,\n        module_objects=globals(),\n        custom_objects=custom_objects,\n        printable_module_name='activation function')",
        "begin_line": 165,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006108735491753207,
            "pseudo_dstar_susp": 0.0012391573729863693,
            "pseudo_tarantula_susp": 0.00045454545454545455,
            "pseudo_op2_susp": 0.0012391573729863693,
            "pseudo_barinel_susp": 0.00045454545454545455
        }
    },
    {
        "name": "keras.activations.get#173",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.get(identifier)",
        "snippet": "def get(identifier):\n    \"\"\"Get the `identifier` activation function.\n\n    # Arguments\n        identifier: None or str, name of the function.\n\n    # Returns\n        The activation function, `linear` if `identifier` is None.\n\n    # Raises\n        ValueError if unknown identifier\n    \"\"\"\n    if identifier is None:\n        return linear\n    if isinstance(identifier, six.string_types):\n        identifier = str(identifier)\n        return deserialize(identifier)\n    elif callable(identifier):\n        if isinstance(identifier, Layer):\n            warnings.warn(\n                'Do not pass a layer instance (such as {identifier}) as the '\n                'activation argument of another layer. Instead, advanced '\n                'activation layers should be used just like any other '\n                'layer in a model.'.format(\n                    identifier=identifier.__class__.__name__))\n        return identifier\n    else:\n        raise ValueError('Could not interpret '\n                         'activation function identifier:', identifier)",
        "begin_line": 173,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001072961373390558,
            "pseudo_dstar_susp": 0.004784688995215311,
            "pseudo_tarantula_susp": 0.0005589714924538849,
            "pseudo_op2_susp": 0.004784688995215311,
            "pseudo_barinel_susp": 0.0005589714924538849
        }
    },
    {
        "name": "keras.regularizers.Regularizer.from_config#21",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers.Regularizer",
        "signature": "keras.regularizers.Regularizer.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        return cls(**config)",
        "begin_line": 21,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.L1L2.__init__#33",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers.L1L2",
        "signature": "keras.regularizers.L1L2.__init__(self, l1=0.0, l2=0.0)",
        "snippet": "    def __init__(self, l1=0., l2=0.):\n        self.l1 = K.cast_to_floatx(l1)\n        self.l2 = K.cast_to_floatx(l2)",
        "begin_line": 33,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.010632546404758e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.L1L2.__call__#37",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers.L1L2",
        "signature": "keras.regularizers.L1L2.__call__(self, x)",
        "snippet": "    def __call__(self, x):\n        regularization = 0.\n        if self.l1:\n            regularization += K.sum(self.l1 * K.abs(x))\n        if self.l2:\n            regularization += K.sum(self.l2 * K.square(x))\n        return regularization",
        "begin_line": 37,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.83477576711251e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.L1L2.get_config#45",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers.L1L2",
        "signature": "keras.regularizers.L1L2.get_config(self)",
        "snippet": "    def get_config(self):\n        return {'l1': float(self.l1),\n                'l2': float(self.l2)}",
        "begin_line": 45,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.208951100469657e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.l1#53",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.l1(l=0.01)",
        "snippet": "def l1(l=0.01):\n    return L1L2(l1=l)",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.267840593141798e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.l2#57",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.l2(l=0.01)",
        "snippet": "def l2(l=0.01):\n    return L1L2(l2=l)",
        "begin_line": 57,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.180207472688883e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.l1_l2#61",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.l1_l2(l1=0.01, l2=0.01)",
        "snippet": "def l1_l2(l1=0.01, l2=0.01):\n    return L1L2(l1=l1, l2=l2)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.serialize#65",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.serialize(regularizer)",
        "snippet": "def serialize(regularizer):\n    return serialize_keras_object(regularizer)",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004716981132075472,
            "pseudo_dstar_susp": 0.0009000900090009,
            "pseudo_tarantula_susp": 0.000462962962962963,
            "pseudo_op2_susp": 0.0009000900090009,
            "pseudo_barinel_susp": 0.000462962962962963
        }
    },
    {
        "name": "keras.regularizers.deserialize#69",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    return deserialize_keras_object(config,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='regularizer')",
        "begin_line": 69,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.045680687471733e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.regularizers.get#76",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.get(identifier)",
        "snippet": "def get(identifier):\n    if identifier is None:\n        return None\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret regularizer identifier: ' +\n                         str(identifier))",
        "begin_line": 76,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010504201680672268,
            "pseudo_dstar_susp": 0.004524886877828055,
            "pseudo_tarantula_susp": 0.00046125461254612545,
            "pseudo_op2_susp": 0.004524886877828055,
            "pseudo_barinel_susp": 0.00046125461254612545
        }
    },
    {
        "name": "keras.layers.core.Masking.__init__#57",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Masking",
        "signature": "keras.layers.core.Masking.__init__(self, mask_value=0.0, **kwargs)",
        "snippet": "    def __init__(self, mask_value=0., **kwargs):\n        super(Masking, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.mask_value = mask_value",
        "begin_line": 57,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Masking.compute_mask#62",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Masking",
        "signature": "keras.layers.core.Masking.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        output_mask = K.any(K.not_equal(inputs, self.mask_value), axis=-1)\n        return output_mask",
        "begin_line": 62,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Masking.call#66",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Masking",
        "signature": "keras.layers.core.Masking.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        boolean_mask = K.any(K.not_equal(inputs, self.mask_value),\n                             axis=-1, keepdims=True)\n        return inputs * K.cast(boolean_mask, K.dtype(inputs))",
        "begin_line": 66,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Masking.get_config#71",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Masking",
        "signature": "keras.layers.core.Masking.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'mask_value': self.mask_value}\n        base_config = super(Masking, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 71,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Masking.compute_output_shape#76",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Masking",
        "signature": "keras.layers.core.Masking.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dropout.__init__#101",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dropout",
        "signature": "keras.layers.core.Dropout.__init__(self, rate, noise_shape=None, seed=None, **kwargs)",
        "snippet": "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n        super(Dropout, self).__init__(**kwargs)\n        self.rate = min(1., max(0., rate))\n        self.noise_shape = noise_shape\n        self.seed = seed\n        self.supports_masking = True",
        "begin_line": 101,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.602458229306702e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dropout._get_noise_shape#108",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dropout",
        "signature": "keras.layers.core.Dropout._get_noise_shape(self, inputs)",
        "snippet": "    def _get_noise_shape(self, inputs):\n        if self.noise_shape is None:\n            return self.noise_shape\n\n        symbolic_shape = K.shape(inputs)\n        noise_shape = [symbolic_shape[axis] if shape is None else shape\n                       for axis, shape in enumerate(self.noise_shape)]\n        return tuple(noise_shape)",
        "begin_line": 108,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dropout.call#117",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dropout",
        "signature": "keras.layers.core.Dropout.call(self, inputs, training=None)",
        "snippet": "    def call(self, inputs, training=None):\n        if 0. < self.rate < 1.:\n            noise_shape = self._get_noise_shape(inputs)\n\n            def dropped_inputs():\n                return K.dropout(inputs, self.rate, noise_shape,\n                                 seed=self.seed)\n            return K.in_train_phase(dropped_inputs, inputs,\n                                    training=training)\n        return inputs",
        "begin_line": 117,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.784735812133073e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dropout.dropped_inputs#121",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dropout",
        "signature": "keras.layers.core.Dropout.dropped_inputs()",
        "snippet": "            def dropped_inputs():\n                return K.dropout(inputs, self.rate, noise_shape,\n                                 seed=self.seed)",
        "begin_line": 121,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.784735812133073e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dropout.get_config#128",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dropout",
        "signature": "keras.layers.core.Dropout.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'rate': self.rate,\n                  'noise_shape': self.noise_shape,\n                  'seed': self.seed}\n        base_config = super(Dropout, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 128,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0001124859392575928,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dropout.compute_output_shape#135",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dropout",
        "signature": "keras.layers.core.Dropout.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 135,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.784735812133073e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.SpatialDropout1D.__init__#165",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.SpatialDropout1D",
        "signature": "keras.layers.core.SpatialDropout1D.__init__(self, rate, **kwargs)",
        "snippet": "    def __init__(self, rate, **kwargs):\n        super(SpatialDropout1D, self).__init__(rate, **kwargs)\n        self.input_spec = InputSpec(ndim=3)",
        "begin_line": 165,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.SpatialDropout1D._get_noise_shape#169",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.SpatialDropout1D",
        "signature": "keras.layers.core.SpatialDropout1D._get_noise_shape(self, inputs)",
        "snippet": "    def _get_noise_shape(self, inputs):\n        input_shape = K.shape(inputs)\n        noise_shape = (input_shape[0], 1, input_shape[2])\n        return noise_shape",
        "begin_line": 169,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.SpatialDropout2D.__init__#210",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.SpatialDropout2D",
        "signature": "keras.layers.core.SpatialDropout2D.__init__(self, rate, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, rate, data_format=None, **kwargs):\n        super(SpatialDropout2D, self).__init__(rate, **kwargs)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=4)",
        "begin_line": 210,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.SpatialDropout2D._get_noise_shape#215",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.SpatialDropout2D",
        "signature": "keras.layers.core.SpatialDropout2D._get_noise_shape(self, inputs)",
        "snippet": "    def _get_noise_shape(self, inputs):\n        input_shape = K.shape(inputs)\n        if self.data_format == 'channels_first':\n            noise_shape = (input_shape[0], input_shape[1], 1, 1)\n        else:\n            noise_shape = (input_shape[0], 1, 1, input_shape[3])\n        return noise_shape",
        "begin_line": 215,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.SpatialDropout3D.__init__#258",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.SpatialDropout3D",
        "signature": "keras.layers.core.SpatialDropout3D.__init__(self, rate, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, rate, data_format=None, **kwargs):\n        super(SpatialDropout3D, self).__init__(rate, **kwargs)\n        self.data_format = K.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=5)",
        "begin_line": 258,
        "end_line": 261,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.SpatialDropout3D._get_noise_shape#263",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.SpatialDropout3D",
        "signature": "keras.layers.core.SpatialDropout3D._get_noise_shape(self, inputs)",
        "snippet": "    def _get_noise_shape(self, inputs):\n        input_shape = K.shape(inputs)\n        if self.data_format == 'channels_first':\n            noise_shape = (input_shape[0], input_shape[1], 1, 1, 1)\n        else:\n            noise_shape = (input_shape[0], 1, 1, 1, input_shape[4])\n        return noise_shape",
        "begin_line": 263,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Activation.__init__#289",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Activation",
        "signature": "keras.layers.core.Activation.__init__(self, activation, **kwargs)",
        "snippet": "    def __init__(self, activation, **kwargs):\n        super(Activation, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.activation = activations.get(activation)",
        "begin_line": 289,
        "end_line": 292,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004928536224741252,
            "pseudo_dstar_susp": 0.0006472491909385113,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0006472491909385113,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.layers.core.Activation.call#294",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Activation",
        "signature": "keras.layers.core.Activation.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return self.activation(inputs)",
        "begin_line": 294,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004928536224741252,
            "pseudo_dstar_susp": 0.0006472491909385113,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0006472491909385113,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.layers.core.Activation.get_config#297",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Activation",
        "signature": "keras.layers.core.Activation.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'activation': activations.serialize(self.activation)}\n        base_config = super(Activation, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 297,
        "end_line": 300,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012547051442910915,
            "pseudo_dstar_susp": 0.000432152117545376,
            "pseudo_tarantula_susp": 0.0020242914979757085,
            "pseudo_op2_susp": 0.000432152117545376,
            "pseudo_barinel_susp": 0.0020242914979757085
        }
    },
    {
        "name": "keras.layers.core.Activation.compute_output_shape#302",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Activation",
        "signature": "keras.layers.core.Activation.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 302,
        "end_line": 303,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004928536224741252,
            "pseudo_dstar_susp": 0.0006472491909385113,
            "pseudo_tarantula_susp": 0.0007407407407407407,
            "pseudo_op2_susp": 0.0006472491909385113,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.layers.core.Reshape.__init__#341",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Reshape",
        "signature": "keras.layers.core.Reshape.__init__(self, target_shape, **kwargs)",
        "snippet": "    def __init__(self, target_shape, **kwargs):\n        super(Reshape, self).__init__(**kwargs)\n        self.target_shape = tuple(target_shape)",
        "begin_line": 341,
        "end_line": 343,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Reshape._fix_unknown_dimension#345",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Reshape",
        "signature": "keras.layers.core.Reshape._fix_unknown_dimension(self, input_shape, output_shape)",
        "snippet": "    def _fix_unknown_dimension(self, input_shape, output_shape):\n        \"\"\"Finds and replaces a missing dimension in an output shape.\n\n        This is a near direct port of the internal Numpy function\n        `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`\n\n        # Arguments\n            input_shape: original shape of array being reshaped\n            output_shape: target shape of the array, with at most\n                a single -1 which indicates a dimension that should be\n                derived from the input shape.\n\n        # Returns\n            The new output shape with a `-1` replaced with its computed value.\n\n        # Raises\n            ValueError: if `input_shape` and `output_shape` do not match.\n        \"\"\"\n        output_shape = list(output_shape)\n        msg = 'total size of new array must be unchanged'\n\n        known, unknown = 1, None\n        for index, dim in enumerate(output_shape):\n            if dim < 0:\n                if unknown is None:\n                    unknown = index\n                else:\n                    raise ValueError('Can only specify one unknown dimension.')\n            else:\n                known *= dim\n\n        original = np.prod(input_shape, dtype=int)\n        if unknown is not None:\n            if known == 0 or original % known != 0:\n                raise ValueError(msg)\n            output_shape[unknown] = original // known\n        elif original != known:\n            raise ValueError(msg)\n\n        return tuple(output_shape)",
        "begin_line": 345,
        "end_line": 384,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Reshape.compute_output_shape#386",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Reshape",
        "signature": "keras.layers.core.Reshape.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if None in input_shape[1:]:\n            # input shape (partially) unknown? replace -1's with None's\n            return ((input_shape[0],) +\n                    tuple(s if s != -1 else None for s in self.target_shape))\n        else:\n            # input shape known? then we can compute the output shape\n            return (input_shape[0],) + self._fix_unknown_dimension(\n                input_shape[1:], self.target_shape)",
        "begin_line": 386,
        "end_line": 394,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Reshape.call#396",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Reshape",
        "signature": "keras.layers.core.Reshape.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.reshape(inputs, (K.shape(inputs)[0],) + self.target_shape)",
        "begin_line": 396,
        "end_line": 397,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Reshape.get_config#399",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Reshape",
        "signature": "keras.layers.core.Reshape.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'target_shape': self.target_shape}\n        base_config = super(Reshape, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 399,
        "end_line": 402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Permute.__init__#435",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Permute",
        "signature": "keras.layers.core.Permute.__init__(self, dims, **kwargs)",
        "snippet": "    def __init__(self, dims, **kwargs):\n        super(Permute, self).__init__(**kwargs)\n        self.dims = tuple(dims)\n        self.input_spec = InputSpec(ndim=len(self.dims) + 1)",
        "begin_line": 435,
        "end_line": 438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Permute.compute_output_shape#440",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Permute",
        "signature": "keras.layers.core.Permute.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        input_shape = list(input_shape)\n        output_shape = copy.copy(input_shape)\n        for i, dim in enumerate(self.dims):\n            target_dim = input_shape[dim]\n            output_shape[i + 1] = target_dim\n        return tuple(output_shape)",
        "begin_line": 440,
        "end_line": 446,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Permute.call#448",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Permute",
        "signature": "keras.layers.core.Permute.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.permute_dimensions(inputs, (0,) + self.dims)",
        "begin_line": 448,
        "end_line": 449,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Permute.get_config#451",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Permute",
        "signature": "keras.layers.core.Permute.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'dims': self.dims}\n        base_config = super(Permute, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 451,
        "end_line": 454,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Flatten.__init__#487",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.__init__(self, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, data_format=None, **kwargs):\n        super(Flatten, self).__init__(**kwargs)\n        self.input_spec = InputSpec(min_ndim=3)\n        self.data_format = K.normalize_data_format(data_format)",
        "begin_line": 487,
        "end_line": 490,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009345794392523364,
            "pseudo_dstar_susp": 0.0007385524372230429,
            "pseudo_tarantula_susp": 0.0055248618784530384,
            "pseudo_op2_susp": 0.0007385524372230429,
            "pseudo_barinel_susp": 0.0055248618784530384
        }
    },
    {
        "name": "keras.layers.core.Flatten.compute_output_shape#492",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if not all(input_shape[1:]):\n            raise ValueError('The shape of the input to \"Flatten\" '\n                             'is not fully defined '\n                             '(got ' + str(input_shape[1:]) + '. '\n                             'Make sure to pass a complete \"input_shape\" '\n                             'or \"batch_input_shape\" argument to the first '\n                             'layer in your model.')\n        return (input_shape[0], np.prod(input_shape[1:]))",
        "begin_line": 492,
        "end_line": 500,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009345794392523364,
            "pseudo_dstar_susp": 0.0007385524372230429,
            "pseudo_tarantula_susp": 0.0055248618784530384,
            "pseudo_op2_susp": 0.0007385524372230429,
            "pseudo_barinel_susp": 0.0055248618784530384
        }
    },
    {
        "name": "keras.layers.core.Flatten.call#502",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.data_format == 'channels_first':\n            # Ensure works for any dim\n            permutation = [0]\n            permutation.extend([i for i in\n                                range(2, K.ndim(inputs))])\n            permutation.append(1)\n            inputs = K.permute_dimensions(inputs, permutation)\n\n        return K.batch_flatten(inputs)",
        "begin_line": 502,
        "end_line": 511,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009345794392523364,
            "pseudo_dstar_susp": 0.0007385524372230429,
            "pseudo_tarantula_susp": 0.0055248618784530384,
            "pseudo_op2_susp": 0.0007385524372230429,
            "pseudo_barinel_susp": 0.0055248618784530384
        }
    },
    {
        "name": "keras.layers.core.Flatten.get_config#513",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'data_format': self.data_format}\n        base_config = super(Flatten, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 513,
        "end_line": 516,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.RepeatVector.__init__#544",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.RepeatVector",
        "signature": "keras.layers.core.RepeatVector.__init__(self, n, **kwargs)",
        "snippet": "    def __init__(self, n, **kwargs):\n        super(RepeatVector, self).__init__(**kwargs)\n        self.n = n\n        self.input_spec = InputSpec(ndim=2)",
        "begin_line": 544,
        "end_line": 547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.RepeatVector.compute_output_shape#549",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.RepeatVector",
        "signature": "keras.layers.core.RepeatVector.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.n, input_shape[1])",
        "begin_line": 549,
        "end_line": 550,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.RepeatVector.call#552",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.RepeatVector",
        "signature": "keras.layers.core.RepeatVector.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        return K.repeat(inputs, self.n)",
        "begin_line": 552,
        "end_line": 553,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.RepeatVector.get_config#555",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.RepeatVector",
        "signature": "keras.layers.core.RepeatVector.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'n': self.n}\n        base_config = super(RepeatVector, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 555,
        "end_line": 558,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Lambda.__init__#620",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Lambda",
        "signature": "keras.layers.core.Lambda.__init__(self, function, output_shape=None, mask=None, arguments=None, **kwargs)",
        "snippet": "    def __init__(self, function, output_shape=None,\n                 mask=None, arguments=None, **kwargs):\n        super(Lambda, self).__init__(**kwargs)\n        self.function = function\n        self.arguments = arguments if arguments else {}\n        if mask is not None:\n            self.supports_masking = True\n        self.mask = mask\n\n        if output_shape is None:\n            self._output_shape = None\n        elif isinstance(output_shape, (tuple, list)):\n            self._output_shape = tuple(output_shape)\n        else:\n            if not callable(output_shape):\n                raise TypeError('In Lambda, `output_shape` '\n                                'must be a list, a tuple, or a function.')\n            self._output_shape = output_shape",
        "begin_line": 620,
        "end_line": 637,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Lambda.compute_output_shape#639",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Lambda",
        "signature": "keras.layers.core.Lambda.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self._output_shape is None:\n            # With TensorFlow or CNTK, we can infer the output shape directly:\n            if K.backend() in ('tensorflow', 'cntk'):\n                if isinstance(input_shape, list):\n                    xs = [K.placeholder(shape=shape) for shape in input_shape]\n                    x = self.call(xs)\n                else:\n                    x = K.placeholder(shape=input_shape)\n                    x = self.call(x)\n                if isinstance(x, list):\n                    return [K.int_shape(x_elem) for x_elem in x]\n                else:\n                    return K.int_shape(x)\n            # Otherwise, we default to the input shape.\n            warnings.warn('`output_shape` argument not specified for layer {} '\n                          'and cannot be automatically inferred '\n                          'with the Theano backend. '\n                          'Defaulting to output shape `{}` '\n                          '(same as input shape). '\n                          'If the expected output shape is different, '\n                          'specify it via the `output_shape` argument.'\n                          .format(self.name, input_shape))\n            return input_shape\n        elif isinstance(self._output_shape, (tuple, list)):\n            if isinstance(input_shape, list):\n                num_samples = input_shape[0][0]\n            else:\n                num_samples = input_shape[0] if input_shape else None\n            return (num_samples,) + tuple(self._output_shape)\n        else:\n            shape = self._output_shape(input_shape)\n            if not isinstance(shape, (list, tuple)):\n                raise ValueError('`output_shape` function must return a tuple or a list of tuples.')\n            if isinstance(shape, list):\n                if isinstance(shape[0], int) or shape[0] is None:\n                    shape = tuple(shape)\n            return shape",
        "begin_line": 639,
        "end_line": 676,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Lambda.call#678",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Lambda",
        "signature": "keras.layers.core.Lambda.call(self, inputs, mask=None)",
        "snippet": "    def call(self, inputs, mask=None):\n        arguments = self.arguments\n        if has_arg(self.function, 'mask'):\n            arguments['mask'] = mask\n        return self.function(inputs, **arguments)",
        "begin_line": 678,
        "end_line": 682,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011027790030877812,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Lambda.compute_mask#684",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Lambda",
        "signature": "keras.layers.core.Lambda.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        if callable(self.mask):\n            return self.mask(inputs, mask)\n        return self.mask",
        "begin_line": 684,
        "end_line": 687,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Lambda.get_config#689",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Lambda",
        "signature": "keras.layers.core.Lambda.get_config(self)",
        "snippet": "    def get_config(self):\n        if isinstance(self.function, python_types.LambdaType):\n            function = func_dump(self.function)\n            function_type = 'lambda'\n        else:\n            function = self.function.__name__\n            function_type = 'function'\n\n        if isinstance(self._output_shape, python_types.LambdaType):\n            output_shape = func_dump(self._output_shape)\n            output_shape_type = 'lambda'\n        elif callable(self._output_shape):\n            output_shape = self._output_shape.__name__\n            output_shape_type = 'function'\n        else:\n            output_shape = self._output_shape\n            output_shape_type = 'raw'\n\n        config = {'function': function,\n                  'function_type': function_type,\n                  'output_shape': output_shape,\n                  'output_shape_type': output_shape_type,\n                  'arguments': self.arguments}\n        base_config = super(Lambda, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 689,
        "end_line": 713,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Lambda.from_config#716",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Lambda",
        "signature": "keras.layers.core.Lambda.from_config(cls, config, custom_objects=None)",
        "snippet": "    def from_config(cls, config, custom_objects=None):\n        config = config.copy()\n        globs = globals()\n        if custom_objects:\n            globs = dict(list(globs.items()) + list(custom_objects.items()))\n        function_type = config.pop('function_type')\n        if function_type == 'function':\n            # Simple lookup in custom objects\n            function = deserialize_keras_object(\n                config['function'],\n                custom_objects=custom_objects,\n                printable_module_name='function in Lambda layer')\n        elif function_type == 'lambda':\n            # Unsafe deserialization from bytecode\n            function = func_load(config['function'], globs=globs)\n        else:\n            raise TypeError('Unknown function type:', function_type)\n\n        output_shape_type = config.pop('output_shape_type')\n        if output_shape_type == 'function':\n            # Simple lookup in custom objects\n            output_shape = deserialize_keras_object(\n                config['output_shape'],\n                custom_objects=custom_objects,\n                printable_module_name='output_shape function in Lambda layer')\n        elif output_shape_type == 'lambda':\n            # Unsafe deserialization from bytecode\n            output_shape = func_load(config['output_shape'], globs=globs)\n        else:\n            output_shape = config['output_shape']\n\n        # If arguments were numpy array, they have been saved as\n        # list. We need to recover the ndarray\n        if 'arguments' in config:\n            for key in config['arguments']:\n                if isinstance(config['arguments'][key], dict):\n                    arg_dict = config['arguments'][key]\n                    if 'type' in arg_dict and arg_dict['type'] == 'ndarray':\n                        # Overwrite the argument with its numpy translation\n                        config['arguments'][key] = np.array(arg_dict['value'])\n\n        config['function'] = function\n        config['output_shape'] = output_shape\n        return cls(**config)",
        "begin_line": 716,
        "end_line": 759,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.Dense.__init__#826",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.__init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super(Dense, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(min_ndim=2)\n        self.supports_masking = True",
        "begin_line": 826,
        "end_line": 851,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0035335689045936395,
            "pseudo_dstar_susp": 0.008333333333333333,
            "pseudo_tarantula_susp": 0.001053740779768177,
            "pseudo_op2_susp": 0.008333333333333333,
            "pseudo_barinel_susp": 0.001053740779768177
        }
    },
    {
        "name": "keras.layers.core.Dense.build#853",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True",
        "begin_line": 853,
        "end_line": 871,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004016064257028112,
            "pseudo_dstar_susp": 0.011627906976744186,
            "pseudo_tarantula_susp": 0.0007782101167315176,
            "pseudo_op2_susp": 0.011627906976744186,
            "pseudo_barinel_susp": 0.0007782101167315176
        }
    },
    {
        "name": "keras.layers.core.Dense.call#873",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        if self.activation is not None:\n            output = self.activation(output)\n        return output",
        "begin_line": 873,
        "end_line": 879,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004016064257028112,
            "pseudo_dstar_susp": 0.011627906976744186,
            "pseudo_tarantula_susp": 0.0007782101167315176,
            "pseudo_op2_susp": 0.011627906976744186,
            "pseudo_barinel_susp": 0.0007782101167315176
        }
    },
    {
        "name": "keras.layers.core.Dense.compute_output_shape#881",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        assert input_shape[-1]\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)",
        "begin_line": 881,
        "end_line": 886,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003787878787878788,
            "pseudo_dstar_susp": 0.009900990099009901,
            "pseudo_tarantula_susp": 0.0007674597083653108,
            "pseudo_op2_susp": 0.009900990099009901,
            "pseudo_barinel_susp": 0.0007674597083653108
        }
    },
    {
        "name": "keras.layers.core.Dense.get_config#888",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'units': self.units,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(Dense, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 888,
        "end_line": 902,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0030864197530864196,
            "pseudo_dstar_susp": 0.0009293680297397769,
            "pseudo_tarantula_susp": 0.001145475372279496,
            "pseudo_op2_susp": 0.0009293680297397769,
            "pseudo_barinel_susp": 0.001145475372279496
        }
    },
    {
        "name": "keras.layers.core.ActivityRegularization.__init__#921",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.ActivityRegularization",
        "signature": "keras.layers.core.ActivityRegularization.__init__(self, l1=0.0, l2=0.0, **kwargs)",
        "snippet": "    def __init__(self, l1=0., l2=0., **kwargs):\n        super(ActivityRegularization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.l1 = l1\n        self.l2 = l2\n        self.activity_regularizer = regularizers.L1L2(l1=l1, l2=l2)",
        "begin_line": 921,
        "end_line": 926,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.ActivityRegularization.get_config#928",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.ActivityRegularization",
        "signature": "keras.layers.core.ActivityRegularization.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'l1': self.l1,\n                  'l2': self.l2}\n        base_config = super(ActivityRegularization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 928,
        "end_line": 932,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.core.ActivityRegularization.compute_output_shape#934",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.ActivityRegularization",
        "signature": "keras.layers.core.ActivityRegularization.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        return input_shape",
        "begin_line": 934,
        "end_line": 935,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.applications.imagenet_utils.decode_predictions#12",
        "src_path": "keras/applications/imagenet_utils.py",
        "class_name": "keras.applications.imagenet_utils",
        "signature": "keras.applications.imagenet_utils.decode_predictions(*args, **kwargs)",
        "snippet": "def decode_predictions(*args, **kwargs):\n    return imagenet_utils.decode_predictions(\n        *args, **kwargs)",
        "begin_line": 12,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.applications.imagenet_utils.preprocess_input#18",
        "src_path": "keras/applications/imagenet_utils.py",
        "class_name": "keras.applications.imagenet_utils",
        "signature": "keras.applications.imagenet_utils.preprocess_input(*args, **kwargs)",
        "snippet": "def preprocess_input(*args, **kwargs):\n    return imagenet_utils.preprocess_input(*args, **kwargs)",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected1D.__init__#81",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected1D",
        "signature": "keras.layers.local.LocallyConnected1D.__init__(self, filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(LocallyConnected1D, self).__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        if self.padding != 'valid':\n            raise ValueError('Invalid border mode for LocallyConnected1D '\n                             '(only \"valid\" is supported): ' + padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(ndim=3)",
        "begin_line": 81,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected1D.build#116",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected1D",
        "signature": "keras.layers.local.LocallyConnected1D.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        input_dim = input_shape[2]\n        if input_dim is None:\n            raise ValueError('Axis 2 of input should be fully-defined. '\n                             'Found shape:', input_shape)\n        output_length = conv_utils.conv_output_length(input_shape[1],\n                                                      self.kernel_size[0],\n                                                      self.padding,\n                                                      self.strides[0])\n        self.kernel_shape = (output_length,\n                             self.kernel_size[0] * input_dim,\n                             self.filters)\n        self.kernel = self.add_weight(\n            shape=self.kernel_shape,\n            initializer=self.kernel_initializer,\n            name='kernel',\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(\n                shape=(output_length, self.filters),\n                initializer=self.bias_initializer,\n                name='bias',\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = InputSpec(ndim=3, axes={2: input_dim})\n        self.built = True",
        "begin_line": 116,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected1D.compute_output_shape#146",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected1D",
        "signature": "keras.layers.local.LocallyConnected1D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        length = conv_utils.conv_output_length(input_shape[1],\n                                               self.kernel_size[0],\n                                               self.padding,\n                                               self.strides[0])\n        return (input_shape[0], length, self.filters)",
        "begin_line": 146,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected1D.call#153",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected1D",
        "signature": "keras.layers.local.LocallyConnected1D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = K.local_conv1d(inputs, self.kernel, self.kernel_size, self.strides)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output",
        "begin_line": 153,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected1D.get_config#161",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected1D",
        "signature": "keras.layers.local.LocallyConnected1D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'filters': self.filters,\n            'kernel_size': self.kernel_size,\n            'strides': self.strides,\n            'padding': self.padding,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(LocallyConnected1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 161,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected2D.__init__#263",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected2D",
        "signature": "keras.layers.local.LocallyConnected2D.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(LocallyConnected2D, self).__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        if self.padding != 'valid':\n            raise ValueError('Invalid border mode for LocallyConnected2D '\n                             '(only \"valid\" is supported): ' + padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(ndim=4)",
        "begin_line": 263,
        "end_line": 296,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected2D.build#298",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected2D",
        "signature": "keras.layers.local.LocallyConnected2D.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if self.data_format == 'channels_last':\n            input_row, input_col = input_shape[1:-1]\n            input_filter = input_shape[3]\n        else:\n            input_row, input_col = input_shape[2:]\n            input_filter = input_shape[1]\n        if input_row is None or input_col is None:\n            raise ValueError('The spatial dimensions of the inputs to '\n                             ' a LocallyConnected2D layer '\n                             'should be fully-defined, but layer received '\n                             'the inputs shape ' + str(input_shape))\n        output_row = conv_utils.conv_output_length(input_row, self.kernel_size[0],\n                                                   self.padding, self.strides[0])\n        output_col = conv_utils.conv_output_length(input_col, self.kernel_size[1],\n                                                   self.padding, self.strides[1])\n        self.output_row = output_row\n        self.output_col = output_col\n        self.kernel_shape = (output_row * output_col,\n                             self.kernel_size[0] * self.kernel_size[1] * input_filter,\n                             self.filters)\n        self.kernel = self.add_weight(shape=self.kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(output_row, output_col, self.filters),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        if self.data_format == 'channels_first':\n            self.input_spec = InputSpec(ndim=4, axes={1: input_filter})\n        else:\n            self.input_spec = InputSpec(ndim=4, axes={-1: input_filter})\n        self.built = True",
        "begin_line": 298,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected2D.compute_output_shape#338",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected2D",
        "signature": "keras.layers.local.LocallyConnected2D.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            rows = input_shape[2]\n            cols = input_shape[3]\n        elif self.data_format == 'channels_last':\n            rows = input_shape[1]\n            cols = input_shape[2]\n\n        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n                                             self.padding, self.strides[0])\n        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n                                             self.padding, self.strides[1])\n\n        if self.data_format == 'channels_first':\n            return (input_shape[0], self.filters, rows, cols)\n        elif self.data_format == 'channels_last':\n            return (input_shape[0], rows, cols, self.filters)",
        "begin_line": 338,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected2D.call#356",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected2D",
        "signature": "keras.layers.local.LocallyConnected2D.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = K.local_conv2d(inputs,\n                                self.kernel,\n                                self.kernel_size,\n                                self.strides,\n                                (self.output_row, self.output_col),\n                                self.data_format)\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format=self.data_format)\n\n        output = self.activation(output)\n        return output",
        "begin_line": 356,
        "end_line": 368,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.local.LocallyConnected2D.get_config#370",
        "src_path": "keras/layers/local.py",
        "class_name": "keras.layers.local.LocallyConnected2D",
        "signature": "keras.layers.local.LocallyConnected2D.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {\n            'filters': self.filters,\n            'kernel_size': self.kernel_size,\n            'strides': self.strides,\n            'padding': self.padding,\n            'data_format': self.data_format,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(LocallyConnected2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 370,
        "end_line": 388,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.embeddings.Embedding.__init__#74",
        "src_path": "keras/layers/embeddings.py",
        "class_name": "keras.layers.embeddings.Embedding",
        "signature": "keras.layers.embeddings.Embedding.__init__(self, input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs)",
        "snippet": "    def __init__(self, input_dim, output_dim,\n                 embeddings_initializer='uniform',\n                 embeddings_regularizer=None,\n                 activity_regularizer=None,\n                 embeddings_constraint=None,\n                 mask_zero=False,\n                 input_length=None,\n                 **kwargs):\n        if 'input_shape' not in kwargs:\n            if input_length:\n                kwargs['input_shape'] = (input_length,)\n            else:\n                kwargs['input_shape'] = (None,)\n        super(Embedding, self).__init__(**kwargs)\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.embeddings_initializer = initializers.get(embeddings_initializer)\n        self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.embeddings_constraint = constraints.get(embeddings_constraint)\n        self.mask_zero = mask_zero\n        self.supports_masking = mask_zero\n        self.input_length = input_length",
        "begin_line": 74,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011439029970258523,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.embeddings.Embedding.build#99",
        "src_path": "keras/layers/embeddings.py",
        "class_name": "keras.layers.embeddings.Embedding",
        "signature": "keras.layers.embeddings.Embedding.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        self.embeddings = self.add_weight(\n            shape=(self.input_dim, self.output_dim),\n            initializer=self.embeddings_initializer,\n            name='embeddings',\n            regularizer=self.embeddings_regularizer,\n            constraint=self.embeddings_constraint,\n            dtype=self.dtype)\n        self.built = True",
        "begin_line": 99,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010223903486351089,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.embeddings.Embedding.compute_mask#109",
        "src_path": "keras/layers/embeddings.py",
        "class_name": "keras.layers.embeddings.Embedding",
        "signature": "keras.layers.embeddings.Embedding.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        if not self.mask_zero:\n            return None\n        output_mask = K.not_equal(inputs, 0)\n        return output_mask",
        "begin_line": 109,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.embeddings.Embedding.compute_output_shape#115",
        "src_path": "keras/layers/embeddings.py",
        "class_name": "keras.layers.embeddings.Embedding",
        "signature": "keras.layers.embeddings.Embedding.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.input_length is None:\n            return input_shape + (self.output_dim,)\n        else:\n            # input_length can be tuple if input is 3D or higher\n            if isinstance(self.input_length, (list, tuple)):\n                in_lens = list(self.input_length)\n            else:\n                in_lens = [self.input_length]\n            if len(in_lens) != len(input_shape) - 1:\n                raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n                                 (str(self.input_length), str(input_shape)))\n            else:\n                for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n                    if s1 is not None and s2 is not None and s1 != s2:\n                        raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n                                         (str(self.input_length), str(input_shape)))\n                    elif s1 is None:\n                        in_lens[i] = s2\n            return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)",
        "begin_line": 115,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.embeddings.Embedding.call#136",
        "src_path": "keras/layers/embeddings.py",
        "class_name": "keras.layers.embeddings.Embedding",
        "signature": "keras.layers.embeddings.Embedding.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if K.dtype(inputs) != 'int32':\n            inputs = K.cast(inputs, 'int32')\n        out = K.gather(self.embeddings, inputs)\n        return out",
        "begin_line": 136,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010481081647626035,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.layers.embeddings.Embedding.get_config#142",
        "src_path": "keras/layers/embeddings.py",
        "class_name": "keras.layers.embeddings.Embedding",
        "signature": "keras.layers.embeddings.Embedding.get_config(self)",
        "snippet": "    def get_config(self):\n        config = {'input_dim': self.input_dim,\n                  'output_dim': self.output_dim,\n                  'embeddings_initializer': initializers.serialize(self.embeddings_initializer),\n                  'embeddings_regularizer': regularizers.serialize(self.embeddings_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'embeddings_constraint': constraints.serialize(self.embeddings_constraint),\n                  'mask_zero': self.mask_zero,\n                  'input_length': self.input_length}\n        base_config = super(Embedding, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 142,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.binary_accuracy#26",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.binary_accuracy(y_true, y_pred)",
        "snippet": "def binary_accuracy(y_true, y_pred):\n    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)",
        "begin_line": 26,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00010787486515641856,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.categorical_accuracy#30",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.categorical_accuracy(y_true, y_pred)",
        "snippet": "def categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n                          K.argmax(y_pred, axis=-1)),\n                  K.floatx())",
        "begin_line": 30,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946969696969697,
            "pseudo_dstar_susp": 0.0008361204013377926,
            "pseudo_tarantula_susp": 0.0007204610951008645,
            "pseudo_op2_susp": 0.0008361204013377926,
            "pseudo_barinel_susp": 0.0007380073800738007
        }
    },
    {
        "name": "keras.metrics.sparse_categorical_accuracy#36",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.sparse_categorical_accuracy(y_true, y_pred)",
        "snippet": "def sparse_categorical_accuracy(y_true, y_pred):\n    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)\n    return K.cast(K.equal(K.flatten(y_true),\n                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                  K.floatx())",
        "begin_line": 36,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.top_k_categorical_accuracy#43",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)",
        "snippet": "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.sparse_top_k_categorical_accuracy#47",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=5)",
        "snippet": "def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k), axis=-1)",
        "begin_line": 47,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.serialize#60",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.serialize(metric)",
        "snippet": "def serialize(metric):\n    return serialize_keras_object(metric)",
        "begin_line": 60,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.deserialize#64",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    return deserialize_keras_object(config,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='metric function')",
        "begin_line": 64,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 9.699321047526673e-05,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.metrics.get#71",
        "src_path": "keras/metrics.py",
        "class_name": "keras.metrics",
        "signature": "keras.metrics.get(identifier)",
        "snippet": "def get(identifier):\n    if isinstance(identifier, dict):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif isinstance(identifier, six.string_types):\n        return deserialize(str(identifier))\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret '\n                         'metric function identifier:', identifier)",
        "begin_line": 71,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.datasets.mnist.load_data#11",
        "src_path": "keras/datasets/mnist.py",
        "class_name": "keras.datasets.mnist",
        "signature": "keras.datasets.mnist.load_data(path='mnist.npz')",
        "snippet": "def load_data(path='mnist.npz'):\n    \"\"\"Loads the MNIST dataset.\n\n    # Arguments\n        path: path where to cache the dataset locally\n            (relative to ~/.keras/datasets).\n\n    # Returns\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n    \"\"\"\n    path = get_file(path,\n                    origin='https://s3.amazonaws.com/img-datasets/mnist.npz',\n                    file_hash='8a61469f7ea1b51cbae51d4f78837e45')\n    f = np.load(path)\n    x_train, y_train = f['x_train'], f['y_train']\n    x_test, y_test = f['x_test'], f['y_test']\n    f.close()\n    return (x_train, y_train), (x_test, y_test)",
        "begin_line": 11,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CallbackList.__init__#38",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.__init__(self, callbacks=None, queue_length=10)",
        "snippet": "    def __init__(self, callbacks=None, queue_length=10):\n        callbacks = callbacks or []\n        self.callbacks = [c for c in callbacks]\n        self.queue_length = queue_length",
        "begin_line": 38,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00032175032175032174,
            "pseudo_dstar_susp": 0.0005455537370430987,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0005455537370430987,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.set_params#46",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.set_params(self, params)",
        "snippet": "    def set_params(self, params):\n        for callback in self.callbacks:\n            callback.set_params(params)",
        "begin_line": 46,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.set_model#50",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.set_model(self, model)",
        "snippet": "    def set_model(self, model):\n        for callback in self.callbacks:\n            callback.set_model(model)",
        "begin_line": 50,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_epoch_begin#54",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        \"\"\"Called at the start of an epoch.\n\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_begin(epoch, logs)\n        self._delta_t_batch = 0.\n        self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n        self._delta_ts_batch_end = deque([], maxlen=self.queue_length)",
        "begin_line": 54,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_epoch_end#68",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of an epoch.\n\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_end(epoch, logs)",
        "begin_line": 68,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_batch_begin#79",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_batch_begin(self, batch, logs=None)",
        "snippet": "    def on_batch_begin(self, batch, logs=None):\n        \"\"\"Called right before processing a batch.\n\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        t_before_callbacks = time.time()\n        for callback in self.callbacks:\n            callback.on_batch_begin(batch, logs)\n        self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n        delta_t_median = np.median(self._delta_ts_batch_begin)\n        if (self._delta_t_batch > 0. and\n           delta_t_median > 0.95 * self._delta_t_batch and\n           delta_t_median > 0.1):\n            warnings.warn('Method on_batch_begin() is slow compared '\n                          'to the batch update (%f). Check your callbacks.'\n                          % delta_t_median)\n        self._t_enter_batch = time.time()",
        "begin_line": 79,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002978850163836759,
            "pseudo_dstar_susp": 0.0002929973630237328,
            "pseudo_tarantula_susp": 0.00032289312237649337,
            "pseudo_op2_susp": 0.0002929973630237328,
            "pseudo_barinel_susp": 0.00032289312237649337
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_batch_end#100",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a batch.\n\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        if not hasattr(self, '_t_enter_batch'):\n            self._t_enter_batch = time.time()\n        self._delta_t_batch = time.time() - self._t_enter_batch\n        t_before_callbacks = time.time()\n        for callback in self.callbacks:\n            callback.on_batch_end(batch, logs)\n        self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n        delta_t_median = np.median(self._delta_ts_batch_end)\n        if (self._delta_t_batch > 0. and\n           (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n            warnings.warn('Method on_batch_end() is slow compared '\n                          'to the batch update (%f). Check your callbacks.'\n                          % delta_t_median)",
        "begin_line": 100,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_train_begin#122",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        \"\"\"Called at the beginning of training.\n\n        # Arguments\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)",
        "begin_line": 122,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_train_end#132",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_train_end(self, logs=None)",
        "snippet": "    def on_train_end(self, logs=None):\n        \"\"\"Called at the end of training.\n\n        # Arguments\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_end(logs)",
        "begin_line": 132,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00028376844494892167,
            "pseudo_dstar_susp": 0.0002835270768358378,
            "pseudo_tarantula_susp": 0.00029717682020802375,
            "pseudo_op2_susp": 0.0002835270768358378,
            "pseudo_barinel_susp": 0.00029717682020802375
        }
    },
    {
        "name": "keras.callbacks.CallbackList.__iter__#142",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self.callbacks)",
        "begin_line": 142,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002845759817871372,
            "pseudo_dstar_susp": 0.0002840909090909091,
            "pseudo_tarantula_susp": 0.0002977963073257892,
            "pseudo_op2_susp": 0.0002840909090909091,
            "pseudo_barinel_susp": 0.0002977963073257892
        }
    },
    {
        "name": "keras.callbacks.Callback.__init__#173",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.__init__(self)",
        "snippet": "    def __init__(self):\n        self.validation_data = None\n        self.model = None",
        "begin_line": 173,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00046446818392940084,
            "pseudo_dstar_susp": 0.0008058017727639,
            "pseudo_tarantula_susp": 0.0005599104143337066,
            "pseudo_op2_susp": 0.0008058017727639,
            "pseudo_barinel_susp": 0.0005599104143337066
        }
    },
    {
        "name": "keras.callbacks.Callback.set_params#177",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.set_params(self, params)",
        "snippet": "    def set_params(self, params):\n        self.params = params",
        "begin_line": 177,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.Callback.set_model#180",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.set_model(self, model)",
        "snippet": "    def set_model(self, model):\n        self.model = model",
        "begin_line": 180,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.Callback.on_epoch_begin#183",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        pass",
        "begin_line": 183,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.Callback.on_epoch_end#186",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        pass",
        "begin_line": 186,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.Callback.on_batch_begin#189",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_batch_begin(self, batch, logs=None)",
        "snippet": "    def on_batch_begin(self, batch, logs=None):\n        pass",
        "begin_line": 189,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.Callback.on_batch_end#192",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        pass",
        "begin_line": 192,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.Callback.on_train_begin#195",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        pass",
        "begin_line": 195,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.Callback.on_train_end#198",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_train_end(self, logs=None)",
        "snippet": "    def on_train_end(self, logs=None):\n        pass",
        "begin_line": 198,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00028376844494892167,
            "pseudo_dstar_susp": 0.0002835270768358378,
            "pseudo_tarantula_susp": 0.00029717682020802375,
            "pseudo_op2_susp": 0.0002835270768358378,
            "pseudo_barinel_susp": 0.00029717682020802375
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.__init__#214",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.__init__(self, stateful_metrics=None)",
        "snippet": "    def __init__(self, stateful_metrics=None):\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()",
        "begin_line": 214,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002848191398461977,
            "pseudo_dstar_susp": 0.0002843332385555871,
            "pseudo_tarantula_susp": 0.00029806259314456036,
            "pseudo_op2_susp": 0.0002843332385555871,
            "pseudo_barinel_susp": 0.00029806259314456036
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.on_epoch_begin#220",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        self.seen = 0\n        self.totals = {}",
        "begin_line": 220,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.on_batch_end#224",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        batch_size = logs.get('size', 0)\n        self.seen += batch_size\n\n        for k, v in logs.items():\n            if k in self.stateful_metrics:\n                self.totals[k] = v\n            else:\n                if k in self.totals:\n                    self.totals[k] += v * batch_size\n                else:\n                    self.totals[k] = v * batch_size",
        "begin_line": 224,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002978850163836759,
            "pseudo_dstar_susp": 0.0002929973630237328,
            "pseudo_tarantula_susp": 0.00032289312237649337,
            "pseudo_op2_susp": 0.0002929973630237328,
            "pseudo_barinel_susp": 0.00032289312237649337
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.on_epoch_end#238",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        if logs is not None:\n            for k in self.params['metrics']:\n                if k in self.totals:\n                    # Make value available to next callbacks.\n                    if k in self.stateful_metrics:\n                        logs[k] = self.totals[k]\n                    else:\n                        logs[k] = self.totals[k] / self.seen",
        "begin_line": 238,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.TerminateOnNaN.on_batch_end#253",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.TerminateOnNaN",
        "signature": "keras.callbacks.TerminateOnNaN.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        loss = logs.get('loss')\n        if loss is not None:\n            if np.isnan(loss) or np.isinf(loss):\n                print('Batch %d: Invalid loss, terminating training' % (batch))\n                self.model.stop_training = True",
        "begin_line": 253,
        "end_line": 259,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ProgbarLogger.__init__#278",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ProgbarLogger",
        "signature": "keras.callbacks.ProgbarLogger.__init__(self, count_mode='samples', stateful_metrics=None)",
        "snippet": "    def __init__(self, count_mode='samples',\n                 stateful_metrics=None):\n        super(ProgbarLogger, self).__init__()\n        if count_mode == 'samples':\n            self.use_steps = False\n        elif count_mode == 'steps':\n            self.use_steps = True\n        else:\n            raise ValueError('Unknown `count_mode`: ' + str(count_mode))\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()",
        "begin_line": 278,
        "end_line": 290,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031240237425804435,
            "pseudo_dstar_susp": 0.00030553009471432935,
            "pseudo_tarantula_susp": 0.0003718854592785422,
            "pseudo_op2_susp": 0.00030553009471432935,
            "pseudo_barinel_susp": 0.0003718854592785422
        }
    },
    {
        "name": "keras.callbacks.ProgbarLogger.on_train_begin#292",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ProgbarLogger",
        "signature": "keras.callbacks.ProgbarLogger.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        self.verbose = self.params['verbose']\n        self.epochs = self.params['epochs']",
        "begin_line": 292,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030441400304414006,
            "pseudo_dstar_susp": 0.00029895366218236175,
            "pseudo_tarantula_susp": 0.0003439972480220158,
            "pseudo_op2_susp": 0.00029895366218236175,
            "pseudo_barinel_susp": 0.0003439972480220158
        }
    },
    {
        "name": "keras.callbacks.ProgbarLogger.on_epoch_begin#296",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ProgbarLogger",
        "signature": "keras.callbacks.ProgbarLogger.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        if self.verbose:\n            print('Epoch %d/%d' % (epoch + 1, self.epochs))\n            if self.use_steps:\n                target = self.params['steps']\n            else:\n                target = self.params['samples']\n            self.target = target\n            self.progbar = Progbar(target=self.target,\n                                   verbose=self.verbose,\n                                   stateful_metrics=self.stateful_metrics)\n        self.seen = 0",
        "begin_line": 296,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00031240237425804435,
            "pseudo_dstar_susp": 0.00030553009471432935,
            "pseudo_tarantula_susp": 0.0003718854592785422,
            "pseudo_op2_susp": 0.00030553009471432935,
            "pseudo_barinel_susp": 0.0003718854592785422
        }
    },
    {
        "name": "keras.callbacks.ProgbarLogger.on_batch_begin#309",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ProgbarLogger",
        "signature": "keras.callbacks.ProgbarLogger.on_batch_begin(self, batch, logs=None)",
        "snippet": "    def on_batch_begin(self, batch, logs=None):\n        if self.seen < self.target:\n            self.log_values = []",
        "begin_line": 309,
        "end_line": 311,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030441400304414006,
            "pseudo_dstar_susp": 0.00029895366218236175,
            "pseudo_tarantula_susp": 0.0003439972480220158,
            "pseudo_op2_susp": 0.00029895366218236175,
            "pseudo_barinel_susp": 0.0003439972480220158
        }
    },
    {
        "name": "keras.callbacks.ProgbarLogger.on_batch_end#313",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ProgbarLogger",
        "signature": "keras.callbacks.ProgbarLogger.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        batch_size = logs.get('size', 0)\n        if self.use_steps:\n            self.seen += 1\n        else:\n            self.seen += batch_size\n\n        for k in self.params['metrics']:\n            if k in logs:\n                self.log_values.append((k, logs[k]))\n\n        # Skip progbar update for the last batch;\n        # will be handled by on_epoch_end.\n        if self.verbose and self.seen < self.target:\n            self.progbar.update(self.seen, self.log_values)",
        "begin_line": 313,
        "end_line": 328,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034698126301179735,
            "pseudo_dstar_susp": 0.0003154574132492114,
            "pseudo_tarantula_susp": 0.0005313496280552603,
            "pseudo_op2_susp": 0.0003154574132492114,
            "pseudo_barinel_susp": 0.0005313496280552603
        }
    },
    {
        "name": "keras.callbacks.ProgbarLogger.on_epoch_end#330",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ProgbarLogger",
        "signature": "keras.callbacks.ProgbarLogger.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        for k in self.params['metrics']:\n            if k in logs:\n                self.log_values.append((k, logs[k]))\n        if self.verbose:\n            self.progbar.update(self.seen, self.log_values)",
        "begin_line": 330,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030441400304414006,
            "pseudo_dstar_susp": 0.00029895366218236175,
            "pseudo_tarantula_susp": 0.0003439972480220158,
            "pseudo_op2_susp": 0.00029895366218236175,
            "pseudo_barinel_susp": 0.0003439972480220158
        }
    },
    {
        "name": "keras.callbacks.History.on_train_begin#347",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.History",
        "signature": "keras.callbacks.History.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        self.epoch = []\n        self.history = {}",
        "begin_line": 347,
        "end_line": 349,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002801905295601009,
            "pseudo_dstar_susp": 0.0002799552071668533,
            "pseudo_tarantula_susp": 0.0002931691586045148,
            "pseudo_op2_susp": 0.0002799552071668533,
            "pseudo_barinel_susp": 0.0002931691586045148
        }
    },
    {
        "name": "keras.callbacks.History.on_epoch_end#351",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.History",
        "signature": "keras.callbacks.History.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epoch.append(epoch)\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)",
        "begin_line": 351,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00028376844494892167,
            "pseudo_dstar_susp": 0.0002835270768358378,
            "pseudo_tarantula_susp": 0.00029717682020802375,
            "pseudo_op2_susp": 0.0002835270768358378,
            "pseudo_barinel_susp": 0.00029717682020802375
        }
    },
    {
        "name": "keras.callbacks.ModelCheckpoint.__init__#390",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ModelCheckpoint",
        "signature": "keras.callbacks.ModelCheckpoint.__init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)",
        "snippet": "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n                 save_best_only=False, save_weights_only=False,\n                 mode='auto', period=1):\n        super(ModelCheckpoint, self).__init__()\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.period = period\n        self.epochs_since_last_save = 0\n\n        if mode not in ['auto', 'min', 'max']:\n            warnings.warn('ModelCheckpoint mode %s is unknown, '\n                          'fallback to auto mode.' % (mode),\n                          RuntimeWarning)\n            mode = 'auto'\n\n        if mode == 'min':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == 'max':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n                self.best = np.Inf",
        "begin_line": 390,
        "end_line": 420,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ModelCheckpoint.on_epoch_end#422",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ModelCheckpoint",
        "signature": "keras.callbacks.ModelCheckpoint.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n            if self.save_best_only:\n                current = logs.get(self.monitor)\n                if current is None:\n                    warnings.warn('Can save best model only with %s available, '\n                                  'skipping.' % (self.monitor), RuntimeWarning)\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n                                  ' saving model to %s'\n                                  % (epoch + 1, self.monitor, self.best,\n                                     current, filepath))\n                        self.best = current\n                        if self.save_weights_only:\n                            self.model.save_weights(filepath, overwrite=True)\n                        else:\n                            self.model.save(filepath, overwrite=True)\n                    else:\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\n                                  (epoch + 1, self.monitor, self.best))\n            else:\n                if self.verbose > 0:\n                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n                if self.save_weights_only:\n                    self.model.save_weights(filepath, overwrite=True)\n                else:\n                    self.model.save(filepath, overwrite=True)",
        "begin_line": 422,
        "end_line": 455,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.EarlyStopping.__init__#486",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.EarlyStopping",
        "signature": "keras.callbacks.EarlyStopping.__init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)",
        "snippet": "    def __init__(self,\n                 monitor='val_loss',\n                 min_delta=0,\n                 patience=0,\n                 verbose=0,\n                 mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n        super(EarlyStopping, self).__init__()\n\n        self.monitor = monitor\n        self.baseline = baseline\n        self.patience = patience\n        self.verbose = verbose\n        self.min_delta = min_delta\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n\n        if mode not in ['auto', 'min', 'max']:\n            warnings.warn('EarlyStopping mode %s is unknown, '\n                          'fallback to auto mode.' % mode,\n                          RuntimeWarning)\n            mode = 'auto'\n\n        if mode == 'min':\n            self.monitor_op = np.less\n        elif mode == 'max':\n            self.monitor_op = np.greater\n        else:\n            if 'acc' in self.monitor:\n                self.monitor_op = np.greater\n            else:\n                self.monitor_op = np.less\n\n        if self.monitor_op == np.greater:\n            self.min_delta *= 1\n        else:\n            self.min_delta *= -1",
        "begin_line": 486,
        "end_line": 525,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.EarlyStopping.on_train_begin#527",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.EarlyStopping",
        "signature": "keras.callbacks.EarlyStopping.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        # Allow instances to be re-used\n        self.wait = 0\n        self.stopped_epoch = 0\n        if self.baseline is not None:\n            self.best = self.baseline\n        else:\n            self.best = np.Inf if self.monitor_op == np.less else -np.Inf",
        "begin_line": 527,
        "end_line": 534,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.EarlyStopping.on_epoch_end#536",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.EarlyStopping",
        "signature": "keras.callbacks.EarlyStopping.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        current = self.get_monitor_value(logs)\n        if current is None:\n            return\n\n        if self.monitor_op(current - self.min_delta, self.best):\n            self.best = current\n            self.wait = 0\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)",
        "begin_line": 536,
        "end_line": 554,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.EarlyStopping.on_train_end#556",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.EarlyStopping",
        "signature": "keras.callbacks.EarlyStopping.on_train_end(self, logs=None)",
        "snippet": "    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0 and self.verbose > 0:\n            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))",
        "begin_line": 556,
        "end_line": 558,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.EarlyStopping.get_monitor_value#560",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.EarlyStopping",
        "signature": "keras.callbacks.EarlyStopping.get_monitor_value(self, logs)",
        "snippet": "    def get_monitor_value(self, logs):\n        monitor_value = logs.get(self.monitor)\n        if monitor_value is None:\n            warnings.warn(\n                'Early stopping conditioned on metric `%s` '\n                'which is not available. Available metrics are: %s' %\n                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n            )\n        return monitor_value",
        "begin_line": 560,
        "end_line": 568,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.RemoteMonitor.__init__#590",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.RemoteMonitor",
        "signature": "keras.callbacks.RemoteMonitor.__init__(self, root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None, send_as_json=False)",
        "snippet": "    def __init__(self,\n                 root='http://localhost:9000',\n                 path='/publish/epoch/end/',\n                 field='data',\n                 headers=None,\n                 send_as_json=False):\n        super(RemoteMonitor, self).__init__()\n\n        self.root = root\n        self.path = path\n        self.field = field\n        self.headers = headers\n        self.send_as_json = send_as_json",
        "begin_line": 590,
        "end_line": 602,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.25,
            "pseudo_dstar_susp": 0.0007485029940119761,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.0007485029940119761,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "keras.callbacks.LearningRateScheduler.__init__#638",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.LearningRateScheduler",
        "signature": "keras.callbacks.LearningRateScheduler.__init__(self, schedule, verbose=0)",
        "snippet": "    def __init__(self, schedule, verbose=0):\n        super(LearningRateScheduler, self).__init__()\n        self.schedule = schedule\n        self.verbose = verbose",
        "begin_line": 638,
        "end_line": 641,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.LearningRateScheduler.on_epoch_begin#643",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.LearningRateScheduler",
        "signature": "keras.callbacks.LearningRateScheduler.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = float(K.get_value(self.model.optimizer.lr))\n        try:  # new API\n            lr = self.schedule(epoch, lr)\n        except TypeError:  # old API for backward compatibility\n            lr = self.schedule(epoch)\n        if not isinstance(lr, (float, np.float32, np.float64)):\n            raise ValueError('The output of the \"schedule\" function '\n                             'should be float.')\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: LearningRateScheduler setting learning '\n                  'rate to %s.' % (epoch + 1, lr))",
        "begin_line": 643,
        "end_line": 657,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.LearningRateScheduler.on_epoch_end#659",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.LearningRateScheduler",
        "signature": "keras.callbacks.LearningRateScheduler.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)",
        "begin_line": 659,
        "end_line": 661,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.TensorBoard.__init__#718",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.TensorBoard",
        "signature": "keras.callbacks.TensorBoard.__init__(self, log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)",
        "snippet": "    def __init__(self, log_dir='./logs',\n                 histogram_freq=0,\n                 batch_size=32,\n                 write_graph=True,\n                 write_grads=False,\n                 write_images=False,\n                 embeddings_freq=0,\n                 embeddings_layer_names=None,\n                 embeddings_metadata=None,\n                 embeddings_data=None):\n        super(TensorBoard, self).__init__()\n        global tf, projector\n        try:\n            import tensorflow as tf\n            from tensorflow.contrib.tensorboard.plugins import projector\n        except ImportError:\n            raise ImportError('You need the TensorFlow module installed to use TensorBoard.')\n\n        if K.backend() != 'tensorflow':\n            if histogram_freq != 0:\n                warnings.warn('You are not using the TensorFlow backend. '\n                              'histogram_freq was set to 0')\n                histogram_freq = 0\n            if write_graph:\n                warnings.warn('You are not using the TensorFlow backend. '\n                              'write_graph was set to False')\n                write_graph = False\n            if write_images:\n                warnings.warn('You are not using the TensorFlow backend. '\n                              'write_images was set to False')\n                write_images = False\n            if embeddings_freq != 0:\n                warnings.warn('You are not using the TensorFlow backend. '\n                              'embeddings_freq was set to 0')\n                embeddings_freq = 0\n\n        self.log_dir = log_dir\n        self.histogram_freq = histogram_freq\n        self.merged = None\n        self.write_graph = write_graph\n        self.write_grads = write_grads\n        self.write_images = write_images\n        self.embeddings_freq = embeddings_freq\n        self.embeddings_layer_names = embeddings_layer_names\n        self.embeddings_metadata = embeddings_metadata or {}\n        self.batch_size = batch_size\n        self.embeddings_data = embeddings_data",
        "begin_line": 718,
        "end_line": 764,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00011746740279572419,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.TensorBoard.set_model#766",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.TensorBoard",
        "signature": "keras.callbacks.TensorBoard.set_model(self, model)",
        "snippet": "    def set_model(self, model):\n        self.model = model\n        if K.backend() == 'tensorflow':\n            self.sess = K.get_session()\n        if self.histogram_freq and self.merged is None:\n            for layer in self.model.layers:\n\n                for weight in layer.weights:\n                    mapped_weight_name = weight.name.replace(':', '_')\n                    tf.summary.histogram(mapped_weight_name, weight)\n                    if self.write_grads:\n                        grads = model.optimizer.get_gradients(model.total_loss,\n                                                              weight)\n\n                        def is_indexed_slices(grad):\n                            return type(grad).__name__ == 'IndexedSlices'\n                        grads = [\n                            grad.values if is_indexed_slices(grad) else grad\n                            for grad in grads]\n                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n                    if self.write_images:\n                        w_img = tf.squeeze(weight)\n                        shape = K.int_shape(w_img)\n                        if len(shape) == 2:  # dense layer kernel case\n                            if shape[0] > shape[1]:\n                                w_img = tf.transpose(w_img)\n                                shape = K.int_shape(w_img)\n                            w_img = tf.reshape(w_img, [1,\n                                                       shape[0],\n                                                       shape[1],\n                                                       1])\n                        elif len(shape) == 3:  # convnet case\n                            if K.image_data_format() == 'channels_last':\n                                # switch to channels_first to display\n                                # every kernel as a separate image\n                                w_img = tf.transpose(w_img, perm=[2, 0, 1])\n                                shape = K.int_shape(w_img)\n                            w_img = tf.reshape(w_img, [shape[0],\n                                                       shape[1],\n                                                       shape[2],\n                                                       1])\n                        elif len(shape) == 1:  # bias case\n                            w_img = tf.reshape(w_img, [1,\n                                                       shape[0],\n                                                       1,\n                                                       1])\n                        else:\n                            # not possible to handle 3D convnets etc.\n                            continue\n\n                        shape = K.int_shape(w_img)\n                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n                        tf.summary.image(mapped_weight_name, w_img)\n\n                if hasattr(layer, 'output'):\n                    if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)\n        self.merged = tf.summary.merge_all()\n\n        if self.write_graph:\n            self.writer = tf.summary.FileWriter(self.log_dir,\n                                                self.sess.graph)\n        else:\n            self.writer = tf.summary.FileWriter(self.log_dir)\n\n        if self.embeddings_freq and self.embeddings_data is not None:\n            self.embeddings_data = standardize_input_data(self.embeddings_data, model.input_names)\n\n            embeddings_layer_names = self.embeddings_layer_names\n\n            if not embeddings_layer_names:\n                embeddings_layer_names = [layer.name for layer in self.model.layers\n                                          if type(layer).__name__ == 'Embedding']\n            self.assign_embeddings = []\n            embeddings_vars = {}\n\n            self.batch_id = batch_id = tf.placeholder(tf.int32)\n            self.step = step = tf.placeholder(tf.int32)\n\n            for layer in self.model.layers:\n                if layer.name in embeddings_layer_names:\n                    embedding_input = self.model.get_layer(layer.name).output\n                    embedding_size = np.prod(embedding_input.shape[1:])\n                    embedding_input = tf.reshape(embedding_input,\n                                                 (step, int(embedding_size)))\n                    shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n                    embedding = tf.Variable(tf.zeros(shape),\n                                            name=layer.name + '_embedding')\n                    embeddings_vars[layer.name] = embedding\n                    batch = tf.assign(embedding[batch_id:batch_id + step],\n                                      embedding_input)\n                    self.assign_embeddings.append(batch)\n\n            self.saver = tf.train.Saver(list(embeddings_vars.values()))\n\n            embeddings_metadata = {}\n\n            if not isinstance(self.embeddings_metadata, str):\n                embeddings_metadata = self.embeddings_metadata\n            else:\n                embeddings_metadata = {layer_name: self.embeddings_metadata\n                                       for layer_name in embeddings_vars.keys()}\n\n            config = projector.ProjectorConfig()\n\n            for layer_name, tensor in embeddings_vars.items():\n                embedding = config.embeddings.add()\n                embedding.tensor_name = tensor.name\n\n                if layer_name in embeddings_metadata:\n                    embedding.metadata_path = embeddings_metadata[layer_name]\n\n            projector.visualize_embeddings(self.writer, config)",
        "begin_line": 766,
        "end_line": 882,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.TensorBoard.is_indexed_slices#780",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.TensorBoard",
        "signature": "keras.callbacks.TensorBoard.is_indexed_slices(grad)",
        "snippet": "                        def is_indexed_slices(grad):\n                            return type(grad).__name__ == 'IndexedSlices'",
        "begin_line": 780,
        "end_line": 781,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.TensorBoard.on_epoch_end#884",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.TensorBoard",
        "signature": "keras.callbacks.TensorBoard.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        if not self.validation_data and self.histogram_freq:\n            raise ValueError(\"If printing histograms, validation_data must be \"\n                             \"provided, and cannot be a generator.\")\n        if self.embeddings_data is None and self.embeddings_freq:\n            raise ValueError(\"To visualize embeddings, embeddings_data must \"\n                             \"be provided.\")\n        if self.validation_data and self.histogram_freq:\n            if epoch % self.histogram_freq == 0:\n\n                val_data = self.validation_data\n                tensors = (self.model.inputs +\n                           self.model.targets +\n                           self.model.sample_weights)\n\n                if self.model.uses_learning_phase:\n                    tensors += [K.learning_phase()]\n\n                assert len(val_data) == len(tensors)\n                val_size = val_data[0].shape[0]\n                i = 0\n                while i < val_size:\n                    step = min(self.batch_size, val_size - i)\n                    if self.model.uses_learning_phase:\n                        # do not slice the learning phase\n                        batch_val = [x[i:i + step] for x in val_data[:-1]]\n                        batch_val.append(val_data[-1])\n                    else:\n                        batch_val = [x[i:i + step] for x in val_data]\n                    assert len(batch_val) == len(tensors)\n                    feed_dict = dict(zip(tensors, batch_val))\n                    result = self.sess.run([self.merged], feed_dict=feed_dict)\n                    summary_str = result[0]\n                    self.writer.add_summary(summary_str, epoch)\n                    i += self.batch_size\n\n        if self.embeddings_freq and self.embeddings_data is not None:\n            if epoch % self.embeddings_freq == 0:\n                # We need a second forward-pass here because we're passing\n                # the `embeddings_data` explicitly. This design allows to pass\n                # arbitrary data as `embeddings_data` and results from the fact\n                # that we need to know the size of the `tf.Variable`s which\n                # hold the embeddings in `set_model`. At this point, however,\n                # the `validation_data` is not yet set.\n\n                # More details in this discussion:\n                # https://github.com/keras-team/keras/pull/7766#issuecomment-329195622\n\n                embeddings_data = self.embeddings_data\n                n_samples = embeddings_data[0].shape[0]\n\n                i = 0\n                while i < n_samples:\n                    step = min(self.batch_size, n_samples - i)\n                    batch = slice(i, i + step)\n\n                    if type(self.model.input) == list:\n                        feed_dict = {model_input: embeddings_data[idx][batch]\n                                     for idx, model_input in enumerate(self.model.input)}\n                    else:\n                        feed_dict = {self.model.input: embeddings_data[0][batch]}\n\n                    feed_dict.update({self.batch_id: i, self.step: step})\n\n                    if self.model.uses_learning_phase:\n                        feed_dict[K.learning_phase()] = False\n\n                    self.sess.run(self.assign_embeddings, feed_dict=feed_dict)\n                    self.saver.save(self.sess,\n                                    os.path.join(self.log_dir, 'keras_embedding.ckpt'),\n                                    epoch)\n\n                    i += self.batch_size\n\n        for name, value in logs.items():\n            if name in ['batch', 'size']:\n                continue\n            summary = tf.Summary()\n            summary_value = summary.value.add()\n            if isinstance(value, np.ndarray):\n                summary_value.simple_value = value.item()\n            else:\n                summary_value.simple_value = value\n            summary_value.tag = name\n            self.writer.add_summary(summary, epoch)\n        self.writer.flush()",
        "begin_line": 884,
        "end_line": 971,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.TensorBoard.on_train_end#973",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.TensorBoard",
        "signature": "keras.callbacks.TensorBoard.on_train_end(self, _)",
        "snippet": "    def on_train_end(self, _):\n        self.writer.close()",
        "begin_line": 973,
        "end_line": 974,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012150668286755772,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ReduceLROnPlateau.__init__#1014",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ReduceLROnPlateau",
        "signature": "keras.callbacks.ReduceLROnPlateau.__init__(self, monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0, **kwargs)",
        "snippet": "    def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n        super(ReduceLROnPlateau, self).__init__()\n\n        self.monitor = monitor\n        if factor >= 1.0:\n            raise ValueError('ReduceLROnPlateau '\n                             'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` instead.')\n        self.factor = factor\n        self.min_lr = min_lr\n        self.min_delta = min_delta\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0  # Cooldown counter.\n        self.wait = 0\n        self.best = 0\n        self.mode = mode\n        self.monitor_op = None\n        self._reset()",
        "begin_line": 1014,
        "end_line": 1038,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ReduceLROnPlateau._reset#1040",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ReduceLROnPlateau",
        "signature": "keras.callbacks.ReduceLROnPlateau._reset(self)",
        "snippet": "    def _reset(self):\n        \"\"\"Resets wait counter and cooldown counter.\n        \"\"\"\n        if self.mode not in ['auto', 'min', 'max']:\n            warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n                          'fallback to auto mode.' % (self.mode),\n                          RuntimeWarning)\n            self.mode = 'auto'\n        if (self.mode == 'min' or\n           (self.mode == 'auto' and 'acc' not in self.monitor)):\n            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n            self.best = np.Inf\n        else:\n            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n            self.best = -np.Inf\n        self.cooldown_counter = 0\n        self.wait = 0",
        "begin_line": 1040,
        "end_line": 1056,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00012610340479192938,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ReduceLROnPlateau.on_train_begin#1058",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ReduceLROnPlateau",
        "signature": "keras.callbacks.ReduceLROnPlateau.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        self._reset()",
        "begin_line": 1058,
        "end_line": 1059,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ReduceLROnPlateau.on_epoch_end#1061",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ReduceLROnPlateau",
        "signature": "keras.callbacks.ReduceLROnPlateau.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\n                'Reduce LR on plateau conditioned on metric `%s` '\n                'which is not available. Available metrics are: %s' %\n                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n            )\n\n        else:\n            if self.in_cooldown():\n                self.cooldown_counter -= 1\n                self.wait = 0\n\n            if self.monitor_op(current, self.best):\n                self.best = current\n                self.wait = 0\n            elif not self.in_cooldown():\n                self.wait += 1\n                if self.wait >= self.patience:\n                    old_lr = float(K.get_value(self.model.optimizer.lr))\n                    if old_lr > self.min_lr:\n                        new_lr = old_lr * self.factor\n                        new_lr = max(new_lr, self.min_lr)\n                        K.set_value(self.model.optimizer.lr, new_lr)\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n                                  'rate to %s.' % (epoch + 1, new_lr))\n                        self.cooldown_counter = self.cooldown\n                        self.wait = 0",
        "begin_line": 1061,
        "end_line": 1092,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.ReduceLROnPlateau.in_cooldown#1094",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.ReduceLROnPlateau",
        "signature": "keras.callbacks.ReduceLROnPlateau.in_cooldown(self)",
        "snippet": "    def in_cooldown(self):\n        return self.cooldown_counter > 0",
        "begin_line": 1094,
        "end_line": 1095,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.000136986301369863,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CSVLogger.__init__#1118",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CSVLogger",
        "signature": "keras.callbacks.CSVLogger.__init__(self, filename, separator=',', append=False)",
        "snippet": "    def __init__(self, filename, separator=',', append=False):\n        self.sep = separator\n        self.filename = filename\n        self.append = append\n        self.writer = None\n        self.keys = None\n        self.append_header = True\n        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n        super(CSVLogger, self).__init__()",
        "begin_line": 1118,
        "end_line": 1126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CSVLogger.on_train_begin#1128",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CSVLogger",
        "signature": "keras.callbacks.CSVLogger.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        if self.append:\n            if os.path.exists(self.filename):\n                with open(self.filename, 'r' + self.file_flags) as f:\n                    self.append_header = not bool(len(f.readline()))\n            self.csv_file = open(self.filename, 'a' + self.file_flags)\n        else:\n            self.csv_file = open(self.filename, 'w' + self.file_flags)",
        "begin_line": 1128,
        "end_line": 1135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CSVLogger.on_epoch_end#1137",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CSVLogger",
        "signature": "keras.callbacks.CSVLogger.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        def handle_value(k):\n            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n            if isinstance(k, six.string_types):\n                return k\n            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n                return '\"[%s]\"' % (', '.join(map(str, k)))\n            else:\n                return k\n\n        if self.keys is None:\n            self.keys = sorted(logs.keys())\n\n        if self.model.stop_training:\n            # We set NA so that csv parsers do not fail for this last epoch.\n            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n\n        if not self.writer:\n            class CustomDialect(csv.excel):\n                delimiter = self.sep\n\n            self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)\n            if self.append_header:\n                self.writer.writeheader()\n\n        row_dict = OrderedDict({'epoch': epoch})\n        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()",
        "begin_line": 1137,
        "end_line": 1168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CSVLogger.handle_value#1140",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CSVLogger",
        "signature": "keras.callbacks.CSVLogger.handle_value(k)",
        "snippet": "        def handle_value(k):\n            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n            if isinstance(k, six.string_types):\n                return k\n            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n                return '\"[%s]\"' % (', '.join(map(str, k)))\n            else:\n                return k",
        "begin_line": 1140,
        "end_line": 1147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CustomDialect.on_epoch_end#1137",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CustomDialect",
        "signature": "keras.callbacks.CustomDialect.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        def handle_value(k):\n            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n            if isinstance(k, six.string_types):\n                return k\n            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n                return '\"[%s]\"' % (', '.join(map(str, k)))\n            else:\n                return k\n\n        if self.keys is None:\n            self.keys = sorted(logs.keys())\n\n        if self.model.stop_training:\n            # We set NA so that csv parsers do not fail for this last epoch.\n            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n\n        if not self.writer:\n            class CustomDialect(csv.excel):\n                delimiter = self.sep\n\n            self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)\n            if self.append_header:\n                self.writer.writeheader()\n\n        row_dict = OrderedDict({'epoch': epoch})\n        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()",
        "begin_line": 1137,
        "end_line": 1168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.CSVLogger.on_train_end#1170",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CSVLogger",
        "signature": "keras.callbacks.CSVLogger.on_train_end(self, logs=None)",
        "snippet": "    def on_train_end(self, logs=None):\n        self.csv_file.close()\n        self.writer = None",
        "begin_line": 1170,
        "end_line": 1172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.00015542430836182778,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    },
    {
        "name": "keras.callbacks.LambdaCallback.__init__#1227",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.LambdaCallback",
        "signature": "keras.callbacks.LambdaCallback.__init__(self, on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None, **kwargs)",
        "snippet": "    def __init__(self,\n                 on_epoch_begin=None,\n                 on_epoch_end=None,\n                 on_batch_begin=None,\n                 on_batch_end=None,\n                 on_train_begin=None,\n                 on_train_end=None,\n                 **kwargs):\n        super(LambdaCallback, self).__init__()\n        self.__dict__.update(kwargs)\n        if on_epoch_begin is not None:\n            self.on_epoch_begin = on_epoch_begin\n        else:\n            self.on_epoch_begin = lambda epoch, logs: None\n        if on_epoch_end is not None:\n            self.on_epoch_end = on_epoch_end\n        else:\n            self.on_epoch_end = lambda epoch, logs: None\n        if on_batch_begin is not None:\n            self.on_batch_begin = on_batch_begin\n        else:\n            self.on_batch_begin = lambda batch, logs: None\n        if on_batch_end is not None:\n            self.on_batch_end = on_batch_end\n        else:\n            self.on_batch_end = lambda batch, logs: None\n        if on_train_begin is not None:\n            self.on_train_begin = on_train_begin\n        else:\n            self.on_train_begin = lambda logs: None\n        if on_train_end is not None:\n            self.on_train_end = on_train_end\n        else:\n            self.on_train_end = lambda logs: None",
        "begin_line": 1227,
        "end_line": 1260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00013130252100840336,
            "pseudo_dstar_susp": 0.00013130252100840336,
            "pseudo_tarantula_susp": 0.00013130252100840336,
            "pseudo_op2_susp": 0.0002028809089064719,
            "pseudo_barinel_susp": 0.00013130252100840336
        }
    }
]